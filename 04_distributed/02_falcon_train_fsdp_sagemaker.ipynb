{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Falcon with near-linear scaling using Sharded Data Parallelism technique in SageMaker Model Parallelism Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll learn how to train the Hugging Face Transformers [Falcon](https://huggingface.co/docs/transformers/main/model_doc/falcon) model with the [Sharded Data Parallelism](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-sharded-data-parallelism.html) technique supported by [SageMaker's Model Parallelism library (SMP)](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html) with PyTorch 2.0 and [GLUE/SST2 dataset](https://huggingface.co/datasets/glue/viewer/sst2/train) on SageMaker. \n",
    "\n",
    "Sharded data parallelism is a distributed training technique that splits the model parameters, gradients, and optimizer states across GPUs in a data parallel group. It is purpose-built for extreme-scale models and leverages Amazon in-house [MiCS](https://arxiv.org/pdf/2205.00119.pdf) technology which achieves a near-linear scaling efficiency. For large models that cannot fit into a single GPU, we also recommend using the sharded data parallelism technique with [Activation Checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-checkpointing.html) and [Activation Offloading](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-offloading.html) in SMP first, before leveraging other techniques such as tensor parallelism or pipeline parallelism.\n",
    "\n",
    "This feature is also compatible with [Tensor Parallelism](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-tensor-parallelism.html). \n",
    "\n",
    "This notebook is accompanied by the following files:\n",
    "\n",
    "- `train.py`: The entry point script that'll be passed to the SageMaker PyTorch estimator later in this notebook when launching the training job. This script is prepared to run an end-to-end training of the Falcon model with SMP, settings for sharded data parallelism applied, and implemented with code lines to save, load, and fine-tune the model. You can follow the comments throughout the script to learn where the SMP APIs and code modifications are implemented.\n",
    "- `data_pipeline.py`: This has data pipeline functions to prepare the training dataset.\n",
    "- `learining_rate.py`: This has functions for learning rate schedule.\n",
    "- `requirements.txt`: This installs the dependencies, including huggingface transformers.\n",
    "- `memory_tracker.py`: This has functions to track memory usage.\n",
    "- `model_config.py`: This has functions to get model configuration information.\n",
    "- `sdp_utils.py`: This has util functions for sharded data parallelism.\n",
    "\n",
    "### Additional resources\n",
    "- To learn more about the SageMaker model parallelism library, see [Model Parallel Distributed Training with SageMaker Distributed](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html).\n",
    "\n",
    "- To learn more about using the SageMaker Python SDK with PyTorch, see [Using PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html).\n",
    "\n",
    "- To learn more about launching a training job in Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html).\n",
    "\n",
    "- To learn more about sharded data parallelism, check [Sharded Data Parallelism](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-sharded-data-parallelism.html) or the blog [Near-linear scaling of gigantic-model training on AWS](https://www.amazon.science/blog/near-linear-scaling-of-gigantic-model-training-on-aws).\n",
    "\n",
    "### Prerequisites\n",
    "You must create an S3 bucket to store the input data for training. This bucket must be located in the same AWS Region that you choose to launch your training job. To learn how to create a S3 bucket, see [Create your first S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html) in the *Amazon S3 documentation*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker initialization\n",
    "\n",
    "Run the following cell to import SageMaker modules and retrieve information of your current SageMaker work environment, such as your AWS account ID, the AWS Region, and the ARN of your Amazon SageMaker execution role. Upgrade SageMaker SDK to the latest version. \n",
    "\n",
    "**NOTE:** This step might require a kernel restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.173.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.181.0.tar.gz (868 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.5/868.5 kB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.28.14)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.24.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.23.4)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.4.4)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.1)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.16.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.5.2)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.14 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.31.14)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2022.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.7)\n",
      "Requirement already satisfied: dill>=0.3.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.7)\n",
      "Requirement already satisfied: pox>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.15 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.15)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Collecting urllib3<1.27,>=1.25.4 (from botocore<1.32.0,>=1.31.14->boto3<2.0,>=1.26.131->sagemaker)\n",
      "  Obtaining dependency information for urllib3<1.27,>=1.25.4 from https://files.pythonhosted.org/packages/c5/05/c214b32d21c0b465506f95c4f28ccbcba15022e000b043b72b3df7728471/urllib3-1.26.16-py2.py3-none-any.whl.metadata\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m145.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.181.0-py2.py3-none-any.whl size=1184831 sha256=89ea8fce0e8ca4fc6947fe1fa48e8e9441bdd615b0985638d353664f1b061c90\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4q_6zjyn/wheels/41/66/35/1a05956e14154d227e34d17ac03ac361ced0c081333af98499\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: urllib3, sagemaker\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.4\n",
      "    Uninstalling urllib3-2.0.4:\n",
      "      Successfully uninstalled urllib3-2.0.4\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.173.0\n",
      "    Uninstalling sagemaker-2.173.0:\n",
      "      Successfully uninstalled sagemaker-2.173.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed sagemaker-2.181.0 urllib3-1.26.16\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting sagemaker-experiments\n",
      "  Obtaining dependency information for sagemaker-experiments from https://files.pythonhosted.org/packages/2b/2b/47d105bbcc328c58b1a23948c3fd9b86930d10b33d220d20c9819e75c41b/sagemaker_experiments-0.1.45-py3-none-any.whl.metadata\n",
      "  Downloading sagemaker_experiments-0.1.45-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: boto3>=1.16.27 in /opt/conda/lib/python3.10/site-packages (from sagemaker-experiments) (1.28.14)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.14 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.31.14)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.14->boto3>=1.16.27->sagemaker-experiments) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.14->boto3>=1.16.27->sagemaker-experiments) (1.26.16)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.14->boto3>=1.16.27->sagemaker-experiments) (1.16.0)\n",
      "Downloading sagemaker_experiments-0.1.45-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sagemaker-experiments\n",
      "Successfully installed sagemaker-experiments-0.1.45\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade sagemaker\n",
    "%pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Execution Role: arn:aws:iam::079002598131:role/service-role/AmazonSageMaker-ExecutionRole-20220804T150518\n",
      "AWS account: 079002598131\n",
      "AWS region: us-east-1\n",
      "\n",
      "Default bucket for this session:  sagemaker-us-east-1-079002598131\n",
      "CPU times: user 1.77 s, sys: 598 ms, total: 2.37 s\n",
      "Wall time: 5.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account: {account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region: {region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print()\n",
    "print(\"Default bucket for this session: \", default_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare GLUE/SST2 data\n",
    "Here you will download, prepare the GLUE/SST2 dataset and then copy the files to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Hugging Face Transformers and Datasets libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -q datasets transformers==4.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, load_from_disk, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import transformers\n",
    "import logging\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from transformers.testing_utils import CaptureLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "This section loads the [GLUE/SST2](https://huggingface.co/datasets/glue/viewer/sst2/train) dataset and splits it to training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"dataset_name\": \"glue\",\n",
    "    \"dataset_config_name\": \"sst2\",\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"cache_dir\": \"tmp\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\n",
    "    hyperparameters[\"dataset_name\"],\n",
    "    hyperparameters[\"dataset_config_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"validation\" not in raw_datasets.keys():\n",
    "    raw_datasets[\"validation\"] = load_dataset(\n",
    "        hyperparameters[\"dataset_name\"],\n",
    "        hyperparameters[\"dataset_config_name\"],\n",
    "        split=\"train[:5%]\",\n",
    "        cache_dir=hyperparameters[\"cache_dir\"],\n",
    "    )\n",
    "\n",
    "    raw_datasets[\"train\"] = load_dataset(\n",
    "        hyperparameters[\"dataset_name\"],\n",
    "        hyperparameters[\"dataset_config_name\"],\n",
    "        split=\"train[5%:]\",\n",
    "        cache_dir=hyperparameters[\"cache_dir\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer\n",
    "Nearly every NLP task begins with a tokenizer. A tokenizer converts your text data into a format (token) that can be processed by the NLP model.\n",
    "The following cell loads a tokenizer for Falcon using [AutoTokenizer.from_pretrained()](https://huggingface.co/docs/transformers/v4.19.4/en/autoclass_tutorial#autotokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887c2e63ee1c44c69ce5cb8407583950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/175 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d89941c02543d48b4f5e1f196f6f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.61M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2051646598324d52b195e505cedc3a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": hyperparameters[\"cache_dir\"],\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"tiiuae/falcon-40b\", trust_remote_code=True, **tokenizer_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "\n",
    "The following two cells set up a function to run the tokenizer and group texts into chunks smaller than the block size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
    "\n",
    "    with CaptureLogger(tok_logger) as cl:\n",
    "        output = tokenizer(examples[text_column_name])\n",
    "        # clm input could be much much longer than block_size\n",
    "        if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "            tok_logger.warning(\n",
    "                \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\"\n",
    "            )\n",
    "    return output\n",
    "\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b827e2529e94398968ac2f4e0f81fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa17e33573ff4fbe99d0ccf4fd0e9b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca0e641922049bb8a5d496893cc521f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer picked seems to have a very large `model_max_length` (2048). Picking 1024 instead. You can change that default value by passing --block_size xxx.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76188d4e7674c779e8cc86042fde80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23e6eca4b7f4eb399b3044a78aaa8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a854e9e769b54f869f8bb3466c2ebbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "# since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n",
    "tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "\n",
    "block_size = tokenizer.model_max_length\n",
    "if block_size > 1024:\n",
    "    logger.warning(\n",
    "        f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "        \"Picking 1024 instead. You can change that default value by passing --block_size xxx.\"\n",
    "    )\n",
    "    block_size = 1024\n",
    "else:\n",
    "    if block_size > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The block_size passed ({block_size}) is larger than the maximum length for the model\"\n",
    "            f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    block_size = min(block_size, tokenizer.model_max_length)\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    #     num_proc=args.preprocessing_num_workers,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set additional hyperparameters and S3 paths for mapping the train and validation datasets properly depending on the phase (training or validation) of the training job in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparameters[\"do_train\"]:\n",
    "    if \"train\" not in tokenized_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = lm_datasets[\"train\"]\n",
    "\n",
    "\n",
    "if hyperparameters[\"do_eval\"]:\n",
    "    if \"validation\" not in tokenized_datasets:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    eval_dataset = lm_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0702db9d9ef4b80a8b724ef21bd7382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acc9ba3ee574bfdb11455c4588a9fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_dataset_location = None\n",
    "validation_dataset_location = None\n",
    "\n",
    "\n",
    "if hyperparameters[\"do_train\"]:\n",
    "    train_dataset.to_json(\"./training.json\")\n",
    "    training_dataset_location = \"s3://{}/dataset/train/\".format(default_bucket)\n",
    "\n",
    "if hyperparameters[\"do_eval\"]:\n",
    "    eval_dataset.to_json(\"./validation.json\")\n",
    "    validation_dataset_location = \"s3://{}/dataset/validation/\".format(default_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./training.json to s3://sagemaker-us-east-1-079002598131/dataset/train/training.json\n",
      "upload: ./validation.json to s3://sagemaker-us-east-1-079002598131/dataset/validation/validation.json\n"
     ]
    }
   ],
   "source": [
    "if training_dataset_location is not None:\n",
    "    command = \"aws s3 cp ./training.json {}\".format(training_dataset_location)\n",
    "    os.system(command)\n",
    "\n",
    "if validation_dataset_location is not None:\n",
    "    command = \"aws s3 cp ./validation.json {}\".format(validation_dataset_location)\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparameters[\"do_train\"]:\n",
    "    command = \"rm ./training.json\"\n",
    "    os.system(command)\n",
    "\n",
    "if hyperparameters[\"do_eval\"]:\n",
    "    command = \"rm ./validation.json\"\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'training_dataset_location' (str)\n",
      "Stored 'validation_dataset_location' (str)\n"
     ]
    }
   ],
   "source": [
    "%store training_dataset_location\n",
    "%store validation_dataset_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "augmented_ai_flow_definition_arn                                        -> 'arn:aws:sagemaker:us-east-1:079002598131:flow-def\n",
      "augmented_ai_workteam_arn                                               -> 'arn:aws:sagemaker:us-east-1:079002598131:workteam\n",
      "checkpoint_s3_path                                                      -> 's3://sagemaker-us-east-1-079002598131/flant5-chec\n",
      "checkpoint_s3_uri                                                       -> 's3://sagemaker-us-east-1-079002598131/flan-t5-che\n",
      "fine_tuned_with_public_qanda                                            -> './fine_tuned_with_public_qanda'\n",
      "fine_tuned_with_public_qanda_checkpoint                                 -> './fine_tuned_with_public_qanda'\n",
      "human_feedback_dataset                                                  -> Dataset({\n",
      "    features: ['prompt', 'response', 're\n",
      "human_loops_started                                                     -> ['d4ecca4d-d62d-4be9-9ce6-5b53904c714c', '5b336746\n",
      "local_data_processed_path                                               -> './data-summarization-processed/'\n",
      "model_checkpoint                                                        -> 'google/flan-t5-base'\n",
      "peft_fine_tuned_with_detoxification_rewards_checkpoint                  -> './peft_fine_tuned_with_detoxification_rewards'\n",
      "peft_fine_tuned_with_public_qanda                                       -> './peft_fine_tuned_with_public_qanda'\n",
      "peft_fine_tuned_with_public_qanda_checkpoint                            -> './peft_fine_tuned_with_public_qanda'\n",
      "peft_ranking_reward_public_qanda_checkpoint                             -> './peft_ranking_reward_public_qanda/'\n",
      "peft_rl_ranking_reward_custom_dataset_model_checkpoint                  -> './rl-ranking-reward-model-custom-dataset/'\n",
      "peft_rl_ranking_reward_public_dataset_model_checkpoint_name             -> './peft_rl_ranking_reward_public_dataset_model/'\n",
      "pipeline_endpoint_name                                                  -> 'model-from-registry-ep-1682528395'\n",
      "pipeline_experiment_name                                                -> 'dialogue-summary-pipeline-1682662744'\n",
      "pipeline_name                                                           -> 'dialogue-summary-pipeline-1682662744'\n",
      "pipeline_trial_name                                                     -> 'trial-1682662744'\n",
      "ranking_reward_model_custom_checkpoint                                  -> './ranking_reward_model_custom/'\n",
      "raw_input_data_s3_uri                                                   -> 's3://sagemaker-us-east-1-079002598131/data-summar\n",
      "rl_ranking_reward_custom_dataset_model_checkpoint                       -> './rl-ranking-reward-model-custom-dataset/'\n",
      "role                                                                    -> 'arn:aws:iam::079002598131:role/service-role/Amazo\n",
      "s3_private_path_tsv                                                     -> 's3://sagemaker-us-east-1-079002598131/amazon-revi\n",
      "s3_public_path_tsv                                                      -> 's3://dsoaws/tsv'\n",
      "setup_dependencies_passed                                               -> True\n",
      "supervised_fine_tuned_model_path                                        -> './flan-dialogue-summary-checkpoint'\n",
      "training_dataset_location                                               -> 's3://sagemaker-us-east-1-079002598131/dataset/tra\n",
      "validation_dataset_location                                             -> 's3://sagemaker-us-east-1-079002598131/dataset/val\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Amazon S3 bucket paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you need to specify the paths for training data to be used by your job. The bucket used must be in the same region as where training will run. In the cells above you downloaded the GLUE/SST2 training and validation split datasets and uploaded the json files in an S3 bucket in your account. This example will train on those json files.\n",
    "\n",
    "After you successfully run this example tensor parallel training job, you can modify the S3 bucket to where your own dataset is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r training_dataset_location\n",
    "%store -r validation_dataset_location\n",
    "\n",
    "# if you're bringing your own data, uncomment the following lines and specify the locations there\n",
    "# training_dataset_location = YOUR_S3_BUCKET/training\n",
    "# validation_dataset_location = YOUR_S3_BUCKET/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_bucket = training_dataset_location\n",
    "s3_test_bucket = validation_dataset_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following S3 bucket will store the output artifacts of the training job. You can modify this as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_bucket = f\"s3://sagemaker-{region}-{account}/smp-tensorparallel-outputdir/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data channels for SageMaker Training using Amazon S3\n",
    "\n",
    "In this step, define SageMaker training data channels to the S3 buckets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <sagemaker.inputs.TrainingInput object at 0x7f01dad6bb50>, 'test': <sagemaker.inputs.TrainingInput object at 0x7f01dad6bac0>}\n"
     ]
    }
   ],
   "source": [
    "# Set use_fsx to False by default\n",
    "# Set below var to True if you want to use fsx (see next cell)\n",
    "use_fsx = False\n",
    "if not use_fsx:\n",
    "    if s3_train_bucket != None:\n",
    "        train = sagemaker.inputs.TrainingInput(\n",
    "            s3_train_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "        )\n",
    "        data_channels = {\"train\": train}\n",
    "    else:\n",
    "        data_channels = {\"train\": mock_data}\n",
    "    if s3_test_bucket != None:\n",
    "        test = sagemaker.inputs.TrainingInput(\n",
    "            s3_test_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "        )\n",
    "        data_channels[\"test\"] = test\n",
    "    else:\n",
    "        data_channels[\"test\"] = mock_data\n",
    "    print(data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Set up and use Amazon FSx for data channels and checkpoints\n",
    "\n",
    "While the previous option of using Amazon S3 is easier to set up, using an FSx can be beneficial for performance when dealing with large input sizes and large model sizes. If you are using models above 13B, checkpointing should be done using FSx. \n",
    "\n",
    "Please see the instructions from [Distributed Training of Mask-RCNN in Amazon SageMaker Using FSx](https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/distributed_tensorflow_mask_rcnn/mask-rcnn-scriptmode-fsx.ipynb) to create an FSx Lustre file system and import the dataset from the S3 bucket to your FSx file system. Note that the FSx file system must be created in a private subnet with internet gateway to ensure that training job has access to the internet. For general guidance on setting an FSx Lustre file system as data input channel, see [Configure Data Input Channel to Use Amazon FSx for Lustre](https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html#model-access-training-data-fsx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions obtained from:\n",
    "# https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/distributed_tensorflow_mask_rcnn/mask-rcnn-scriptmode-fsx.ipynb\n",
    "\n",
    "if use_fsx:\n",
    "    from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "    # Specify FSx Lustre file system id.\n",
    "    file_system_id = \"<your-file-system-id>\"\n",
    "\n",
    "    # Specify the SG and subnet used by the FSX, these are passed to SM Estimator so jobs use this as well\n",
    "    fsx_security_group_id = \"<your-security-group-id>\"\n",
    "    fsx_subnet = \"<your-subnet>\"\n",
    "\n",
    "    # Specify directory path for input data on the file system.\n",
    "    # You need to provide normalized and absolute path below.\n",
    "    # Your mount name can be provided by you when creating fsx, or generated automatically.\n",
    "    # You can find this mount_name on the FSX page in console.\n",
    "    # Example of fsx generated mount_name: \"3x5lhbmv\"\n",
    "    base_path = \"<your-mount-name>\"\n",
    "\n",
    "    # Specify your file system type.\n",
    "    file_system_type = \"FSxLustre\"\n",
    "\n",
    "    train = FileSystemInput(\n",
    "        file_system_id=file_system_id,\n",
    "        file_system_type=file_system_type,\n",
    "        directory_path=base_path,\n",
    "        file_system_access_mode=\"rw\",\n",
    "    )\n",
    "\n",
    "    data_channels = {\"train\": train, \"test\": train}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters, metric definitions, and MPI options\n",
    "The following `hyperparameters` dictionary passes arguments to the training script (`train.py`) and set the model parallel configuration when creating the training job.\n",
    "\n",
    "You can also add custom `mpi` flags. By default, we have `--mca btl_vader_single_copy_mechanism none` to remove unnecessary logs.\n",
    "\n",
    "Next, we add a base metric definitions to enable the metric upload in SageMaker. You can add any further metric definitions.\n",
    "\n",
    "Note that we add the `sharded_data_parallel_degree` parameter to the `hyperparameter` dictionary. This will be parsed and used when we configure a SageMaker PyTorch estimator to activate sharded data parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_steps\": 100,\n",
    "    \"seed\": 12345,\n",
    "    \"fp16\": 0,\n",
    "    \"bf16\": 1,\n",
    "    \"lr\": 2.0e-4,\n",
    "    \"lr_decay_iters\": 125000,\n",
    "    \"min_lr\": 0.00001,\n",
    "    \"lr-decay-style\": \"linear\",\n",
    "    \"warmup\": 0.01,\n",
    "    \"num_kept_checkpoints\": 5,\n",
    "    \"checkpoint_freq\": 200,\n",
    "    \"logging_freq\": 1,\n",
    "    \"save_final_full_model\": 0,\n",
    "    \"delayed_param\": 1,\n",
    "    \"offload_activations\": 0,\n",
    "    \"activation_loading_horizon\": 4,\n",
    "    \"gradient_accumulation\": 1,\n",
    "    \"validation_freq\": 200,\n",
    "    \"train_batch_size\": 4,\n",
    "    \"val_batch_size\": 4,\n",
    "    \"zipped_data\": 0,\n",
    "    \"epochs\": 100,\n",
    "    \"use_distributed_transformer\": 0,\n",
    "    \"model_type\": \"falcon\",\n",
    "    # parameters for sharded data parallelism\n",
    "    \"sharded_data_parallel_degree\": 16,\n",
    "}\n",
    "\n",
    "if use_fsx:\n",
    "    # make sure to update paths for training-dir and test-dir based on the paths of datasets in fsx\n",
    "    # If you want to resume training, set checkpoint-dir to the same path as a previous job.\n",
    "    SM_TRAIN_DIR = \"/opt/ml/input/data/train\"\n",
    "    hyperparameters[\"checkpoint-dir\"] = f\"{SM_TRAIN_DIR}/checkpointdir-job2\"\n",
    "    hyperparameters[\"model-dir\"] = f\"{SM_TRAIN_DIR}/modeldir-job2\"\n",
    "    hyperparameters[\"training-dir\"] = f\"{SM_TRAIN_DIR}/datasets/pytorch_gpt/train_synthetic\"\n",
    "    hyperparameters[\"test-dir\"] = f\"{SM_TRAIN_DIR}/datasets/pytorch_gpt/val_synthetic\"\n",
    "\n",
    "# The checkpoint path (hyperparameters['checkpoint-dir'] or checkpoint_s3_uri) is not unique per job.\n",
    "# You need to modify as needed for different runs.\n",
    "# If same path is used for unrelated runs, this may increase time when downloading unnecessary checkpoints,\n",
    "# and cause conflicts when loading checkpoints.\n",
    "\n",
    "mpioptions = \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR \"\n",
    "mpioptions += (\n",
    "    \"-x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 \"\n",
    ")\n",
    "mpioptions += \"-x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\"\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"base_metric\", \"Regex\": \"<><><><><><>\"}\n",
    "]  # Add your custom metric definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = \"falcon-7b\"\n",
    "\n",
    "if model_config == \"falcon-7b\":\n",
    "    model_params = {\n",
    "        \"max_context_width\": 2048,\n",
    "        \"hidden_width\": 4544,\n",
    "        \"num_layers\": 32,\n",
    "        \"num_heads\": 71,\n",
    "        \"num_heads_kv\": 71,\n",
    "    }\n",
    "else:\n",
    "    raise RuntimeError(\"Unknown model config\")\n",
    "\n",
    "for k, v in model_params.items():\n",
    "    hyperparameters[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify essential parameters for a SageMaker Training job\n",
    "\n",
    "Next, you use the [`SageMaker Estimator class`](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) to define a SageMaker Training Job, passing values through the following parameters for training job name, the number of EC2 instances, the instance type, and the size of the volume attached to the instances. \n",
    "\n",
    "* `instance_count`\n",
    "* `instance_type`\n",
    "* `volume_size`\n",
    "* `base_job_name`\n",
    "\n",
    "### Update the type and the number of EC2 instance to use\n",
    "\n",
    "The instance type and the number of instances you specify to the `instance_type` and `instance_count` parameters, respectively, determine the total number of GPUs (world size).\n",
    "\n",
    "$$ \\text{(world size) = (the number of GPUs on a single instance)}\\times\\text{(the number of instances)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "\n",
    "instance_count = 2\n",
    "\n",
    "# set to the number of GPUs on that instance\n",
    "processes_per_host = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look up the number of GPUs of different instance types, see [Amazon EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/). Use the section **Accelerated Computing** to see general purpose GPU instances. Note that, for example, a given instance type `p4d.24xlarge` has a corresponding instance type `ml.p4d.24xlarge` in SageMaker.\n",
    "For SageMaker supported `ml` instances and cost information, see [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify a base job name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "sharding_degree = hyperparameters[\"sharded_data_parallel_degree\"]\n",
    "base_job_name = (\n",
    "    f'smp-{model_config}-{machine_str}-sdp{sharding_degree}-bs{hyperparameters[\"train_batch_size\"]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_fsx:\n",
    "    # If you want to resume training, set checkpoint_s3_uri to the same path as a previous job.\n",
    "    # Previous checkpoint to load must have same model config.\n",
    "    checkpoint_bucket = f\"s3://sagemaker-{region}-{account}/\"\n",
    "    checkpoint_s3_uri = (\n",
    "        f\"{checkpoint_bucket}/experiments/gpt_synthetic_simpletrainer_checkpoints/{base_job_name}/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_job_name: smp-falcon-7b-p4d24x-sdp16-bs4 checkpoint_s3_uri: s3://sagemaker-us-east-1-079002598131//experiments/gpt_synthetic_simpletrainer_checkpoints/smp-falcon-7b-p4d24x-sdp16-bs4/\n"
     ]
    }
   ],
   "source": [
    "print(f\"base_job_name: {base_job_name} checkpoint_s3_uri: {checkpoint_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker PyTorch estimator\n",
    "\n",
    "The following cell constructs a PyTorch estimator using the parameters defined above. To see how the SageMaker APIs and functions are applied to the script, see the `train.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "if use_fsx:\n",
    "    # Use the security group and subnet that was used to create the fsx filesystem\n",
    "    kwargs[\"security_group_ids\"] = [fsx_security_group_id]\n",
    "    kwargs[\"subnets\"] = [fsx_subnet]\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=os.getcwd(),\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": processes_per_host,\n",
    "            \"custom_mpi_options\": mpioptions,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"ddp\": True,\n",
    "                    \"skip_tracing\": True,\n",
    "                    \"delayed_parameter_initialization\": hyperparameters[\"delayed_param\"] > 0,\n",
    "                    \"offload_activations\": hyperparameters[\"offload_activations\"] > 0,\n",
    "                    \"activation_loading_horizon\": hyperparameters[\"activation_loading_horizon\"],\n",
    "                    \"sharded_data_parallel_degree\": hyperparameters[\"sharded_data_parallel_degree\"],\n",
    "                    \"fp16\": hyperparameters[\"fp16\"] > 0,\n",
    "                    \"bf16\": hyperparameters[\"bf16\"] > 0,\n",
    "                    # partitions is a required param in the current SM SDK so it needs to be passed,\n",
    "                    \"partitions\": 1,\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    framework_version=\"2.0\",\n",
    "    py_version=\"py310\",\n",
    "    output_path=s3_output_bucket,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri if not use_fsx else None,\n",
    "    checkpoint_local_path=hyperparameters[\"checkpoint-dir\"] if use_fsx else None,\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameters=hyperparameters,\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,\n",
    "    base_job_name=base_job_name,\n",
    "    **kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the `estimator.fit` method to launch the SageMaker training job of the Falcon model with sharded data parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: smp-falcon-7b-p4d24x-sdp16-bs4-2023-08-29-08-25-43-710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-29 08:25:45 Starting - Starting the training job...\n",
      "2023-08-29 08:25:51 Pending - Training job waiting for capacity...\n",
      "2023-08-29 08:26:25 Pending - Preparing the instances for training..............................................................................\n",
      "2023-08-29 08:39:27 Pending - Launched instance was unhealthy, replacing it!...\n",
      "2023-08-29 08:40:01 Pending - Preparing the instances for training........................\n",
      "2023-08-29 08:44:00 Downloading - Downloading input data...\n",
      "2023-08-29 08:44:30 Training - Downloading the training image.....................\n",
      "2023-08-29 08:47:56 Training - Training image download completed. Training in progress........\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:17,437 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:17,492 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:17,500 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:17,502 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:18,881 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mCollecting datasets (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for datasets from https://files.pythonhosted.org/packages/66/f8/38298237d18d4b6a8ee5dfe390e97bed5adb8e01ec6f9680c0ddf3066728/datasets-2.14.4-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.14.4-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.24.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.180.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.1.45)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.11.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torchnet in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.0.4)\u001b[0m\n",
      "\u001b[35mCollecting transformers==4.21.0 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 54.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: smdebug in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.0.34)\u001b[0m\n",
      "\u001b[35mCollecting humanize (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for humanize from https://files.pythonhosted.org/packages/4a/52/cccfc7a0d3bcf52cca6f6e1792786075df979346d638bf4cf5bc8bf2be3c/humanize-4.8.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading humanize-4.8.0-py3-none-any.whl.metadata (7.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.21.0->-r requirements.txt (line 7)) (3.12.2)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.1.0 (from transformers==4.21.0->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for huggingface-hub<1.0,>=0.1.0 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.21.0->-r requirements.txt (line 7)) (23.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.21.0->-r requirements.txt (line 7)) (6.0.1)\u001b[0m\n",
      "\u001b[35mCollecting regex!=2019.12.17 (from transformers==4.21.0->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 7.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.21.0->-r requirements.txt (line 7)) (2.31.0)\u001b[0m\n",
      "\u001b[35mCollecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.21.0->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 114.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.21.0->-r requirements.txt (line 7)) (4.65.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (13.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.0.3)\u001b[0m\n",
      "\u001b[35mCollecting xxhash (from datasets->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/13/c3/e942893f4864a424514c81640f114980cfd5aff7e7414d1e0255f4571111/xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2023.6.0)\u001b[0m\n",
      "\u001b[35mCollecting aiohttp (from datasets->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (23.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: boto3<2.0,>=1.26.131 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.28.34)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (2.2.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (3.20.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (6.8.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.7.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (4.19.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (3.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.7.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchnet->-r requirements.txt (line 6)) (2.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from torchnet->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: visdom in /opt/conda/lib/python3.10/site-packages (from torchnet->-r requirements.txt (line 6)) (0.2.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyinstrument==3.4.2 in /opt/conda/lib/python3.10/site-packages (from smdebug->-r requirements.txt (line 8)) (3.4.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyinstrument-cext>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from pyinstrument==3.4.2->smdebug->-r requirements.txt (line 8)) (0.2.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: botocore<1.32.0,>=1.31.34 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker->-r requirements.txt (line 3)) (1.31.34)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker->-r requirements.txt (line 3)) (0.6.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[35mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 35.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 45.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.0->-r requirements.txt (line 7)) (4.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker->-r requirements.txt (line 3)) (3.16.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.21.0->-r requirements.txt (line 7)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.21.0->-r requirements.txt (line 7)) (1.26.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.21.0->-r requirements.txt (line 7)) (2023.7.22)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 3)) (2023.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 3)) (0.30.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 3)) (0.9.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ppft>=1.7.6.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (1.7.6.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pox>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (0.3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker->-r requirements.txt (line 3)) (21.6.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchnet->-r requirements.txt (line 6)) (1.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchnet->-r requirements.txt (line 6)) (3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchnet->-r requirements.txt (line 6)) (3.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tornado in /opt/conda/lib/python3.10/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (6.3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jsonpatch in /opt/conda/lib/python3.10/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.33)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.6.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (10.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchnet->-r requirements.txt (line 6)) (2.1.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch->visdom->torchnet->-r requirements.txt (line 6)) (2.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchnet->-r requirements.txt (line 6)) (1.3.0)\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.14.4-py3-none-any.whl (519 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.3/519.3 kB 72.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading humanize-4.8.0-py3-none-any.whl (117 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.1/117.1 kB 34.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 90.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 68.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 771.9/771.9 kB 79.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 52.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.7/225.7 kB 42.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mInstalling collected packages: tokenizers, xxhash, regex, multidict, humanize, frozenlist, async-timeout, yarl, huggingface-hub, aiosignal, transformers, aiohttp, datasets\u001b[0m\n",
      "\u001b[35mSuccessfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.4 frozenlist-1.4.0 huggingface-hub-0.16.4 humanize-4.8.0 multidict-6.0.4 regex-2023.8.8 tokenizers-0.12.1 transformers-4.21.0 xxhash-3.3.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:23,158 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:23,213 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:23,221 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:23,223 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:24,610 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for datasets from https://files.pythonhosted.org/packages/66/f8/38298237d18d4b6a8ee5dfe390e97bed5adb8e01ec6f9680c0ddf3066728/datasets-2.14.4-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.4-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.180.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.1.45)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.11.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchnet in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.0.4)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.21.0 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 73.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.0.34)\u001b[0m\n",
      "\u001b[34mCollecting humanize (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for humanize from https://files.pythonhosted.org/packages/4a/52/cccfc7a0d3bcf52cca6f6e1792786075df979346d638bf4cf5bc8bf2be3c/humanize-4.8.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading humanize-4.8.0-py3-none-any.whl.metadata (7.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.21.0->-r requirements.txt (line 7)) (3.12.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.1.0 (from transformers==4.21.0->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub<1.0,>=0.1.0 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.21.0->-r requirements.txt (line 7)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.21.0->-r requirements.txt (line 7)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.21.0->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 13.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.21.0->-r requirements.txt (line 7)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.21.0->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 123.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.21.0->-r requirements.txt (line 7)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (13.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.0.3)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/13/c3/e942893f4864a424514c81640f114980cfd5aff7e7414d1e0255f4571111/xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2023.6.0)\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:26,865 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:26,865 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:26,946 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:27,009 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:27,017 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:27,017 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:27,017 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:27,023 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:27,024 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.85.50.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3<2.0,>=1.26.131 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.28.34)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (2.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (6.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (4.19.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (3.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchnet->-r requirements.txt (line 6)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from torchnet->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: visdom in /opt/conda/lib/python3.10/site-packages (from torchnet->-r requirements.txt (line 6)) (0.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument==3.4.2 in /opt/conda/lib/python3.10/site-packages (from smdebug->-r requirements.txt (line 8)) (3.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument-cext>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from pyinstrument==3.4.2->smdebug->-r requirements.txt (line 8)) (0.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.32.0,>=1.31.34 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker->-r requirements.txt (line 3)) (1.31.34)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker->-r requirements.txt (line 3)) (0.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 38.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 66.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.0->-r requirements.txt (line 7)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker->-r requirements.txt (line 3)) (3.16.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.21.0->-r requirements.txt (line 7)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.21.0->-r requirements.txt (line 7)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.21.0->-r requirements.txt (line 7)) (2023.7.22)\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:28,039 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:28,039 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.85.50.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 3)) (2023.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 3)) (0.30.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 3)) (0.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.7.6.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (1.7.6.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker->-r requirements.txt (line 3)) (21.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchnet->-r requirements.txt (line 6)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchnet->-r requirements.txt (line 6)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchnet->-r requirements.txt (line 6)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado in /opt/conda/lib/python3.10/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (6.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonpatch in /opt/conda/lib/python3.10/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.33)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchnet->-r requirements.txt (line 6)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch->visdom->torchnet->-r requirements.txt (line 6)) (2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchnet->-r requirements.txt (line 6)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.4-py3-none-any.whl (519 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.3/519.3 kB 72.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading humanize-4.8.0-py3-none-any.whl (117 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.1/117.1 kB 39.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 98.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 63.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 771.9/771.9 kB 86.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 42.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.7/225.7 kB 56.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:29,067 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:29,067 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.85.50.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, xxhash, regex, multidict, humanize, frozenlist, async-timeout, yarl, huggingface-hub, aiosignal, transformers, aiohttp, datasets\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:30,069 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:30,069 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.85.50.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:31,071 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:31,071 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.85.50.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:32,072 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:32,072 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.85.50.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.4 frozenlist-1.4.0 huggingface-hub-0.16.4 humanize-4.8.0 multidict-6.0.4 regex-2023.8.8 tokenizers-0.12.1 transformers-4.21.0 xxhash-3.3.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:32,428 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:32,428 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:32,506 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:32,569 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:32,577 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:32,577 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:32,602 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:33,096 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:33,221 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:33,221 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:33,221 sagemaker-training-toolkit INFO     Worker algo-1 available for communication\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:33,221 sagemaker-training-toolkit INFO     Env Hosts: ['algo-2', 'algo-1'] Hosts: ['algo-2:8', 'algo-1:8'] process_per_hosts: 8 num_processes: 16\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:33,222 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:33,298 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:33,306 sagemaker-training-toolkit INFO     PyTorch version is 2.0.1\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:33,362 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:33,371 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"activation_loading_horizon\": 4,\n",
      "        \"bf16\": 1,\n",
      "        \"checkpoint_freq\": 200,\n",
      "        \"delayed_param\": 1,\n",
      "        \"epochs\": 100,\n",
      "        \"fp16\": 0,\n",
      "        \"gradient_accumulation\": 1,\n",
      "        \"hidden_width\": 4544,\n",
      "        \"logging_freq\": 1,\n",
      "        \"lr\": 0.0002,\n",
      "        \"lr-decay-style\": \"linear\",\n",
      "        \"lr_decay_iters\": 125000,\n",
      "        \"max_context_width\": 2048,\n",
      "        \"max_steps\": 100,\n",
      "        \"min_lr\": 1e-05,\n",
      "        \"model_type\": \"falcon\",\n",
      "        \"mp_parameters\": {\n",
      "            \"ddp\": true,\n",
      "            \"skip_tracing\": true,\n",
      "            \"delayed_parameter_initialization\": true,\n",
      "            \"offload_activations\": false,\n",
      "            \"activation_loading_horizon\": 4,\n",
      "            \"sharded_data_parallel_degree\": 16,\n",
      "            \"fp16\": false,\n",
      "            \"bf16\": true,\n",
      "            \"partitions\": 1\n",
      "        },\n",
      "        \"num_heads\": 71,\n",
      "        \"num_heads_kv\": 71,\n",
      "        \"num_kept_checkpoints\": 5,\n",
      "        \"num_layers\": 32,\n",
      "        \"offload_activations\": 0,\n",
      "        \"save_final_full_model\": 0,\n",
      "        \"seed\": 12345,\n",
      "        \"sharded_data_parallel_degree\": 16,\n",
      "        \"train_batch_size\": 4,\n",
      "        \"use_distributed_transformer\": 0,\n",
      "        \"val_batch_size\": 4,\n",
      "        \"validation_freq\": 200,\n",
      "        \"warmup\": 0.01,\n",
      "        \"zipped_data\": 0\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": true,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"smp-falcon-7b-p4d24x-sdp16-bs4-2023-08-29-08-25-43-710\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-2\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-079002598131/smp-falcon-7b-p4d24x-sdp16-bs4-2023-08-29-08-25-43-710/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"activation_loading_horizon\":4,\"bf16\":1,\"checkpoint_freq\":200,\"delayed_param\":1,\"epochs\":100,\"fp16\":0,\"gradient_accumulation\":1,\"hidden_width\":4544,\"logging_freq\":1,\"lr\":0.0002,\"lr-decay-style\":\"linear\",\"lr_decay_iters\":125000,\"max_context_width\":2048,\"max_steps\":100,\"min_lr\":1e-05,\"model_type\":\"falcon\",\"mp_parameters\":{\"activation_loading_horizon\":4,\"bf16\":true,\"ddp\":true,\"delayed_parameter_initialization\":true,\"fp16\":false,\"offload_activations\":false,\"partitions\":1,\"sharded_data_parallel_degree\":16,\"skip_tracing\":true},\"num_heads\":71,\"num_heads_kv\":71,\"num_kept_checkpoints\":5,\"num_layers\":32,\"offload_activations\":0,\"save_final_full_model\":0,\"seed\":12345,\"sharded_data_parallel_degree\":16,\"train_batch_size\":4,\"use_distributed_transformer\":0,\"val_batch_size\":4,\"validation_freq\":200,\"warmup\":0.01,\"zipped_data\":0}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-079002598131/smp-falcon-7b-p4d24x-sdp16-bs4-2023-08-29-08-25-43-710/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-2\",\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"activation_loading_horizon\":4,\"bf16\":1,\"checkpoint_freq\":200,\"delayed_param\":1,\"epochs\":100,\"fp16\":0,\"gradient_accumulation\":1,\"hidden_width\":4544,\"logging_freq\":1,\"lr\":0.0002,\"lr-decay-style\":\"linear\",\"lr_decay_iters\":125000,\"max_context_width\":2048,\"max_steps\":100,\"min_lr\":1e-05,\"model_type\":\"falcon\",\"mp_parameters\":{\"activation_loading_horizon\":4,\"bf16\":true,\"ddp\":true,\"delayed_parameter_initialization\":true,\"fp16\":false,\"offload_activations\":false,\"partitions\":1,\"sharded_data_parallel_degree\":16,\"skip_tracing\":true},\"num_heads\":71,\"num_heads_kv\":71,\"num_kept_checkpoints\":5,\"num_layers\":32,\"offload_activations\":0,\"save_final_full_model\":0,\"seed\":12345,\"sharded_data_parallel_degree\":16,\"train_batch_size\":4,\"use_distributed_transformer\":0,\"val_batch_size\":4,\"validation_freq\":200,\"warmup\":0.01,\"zipped_data\":0},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"smp-falcon-7b-p4d24x-sdp16-bs4-2023-08-29-08-25-43-710\",\"log_level\":20,\"master_hostname\":\"algo-2\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-079002598131/smp-falcon-7b-p4d24x-sdp16-bs4-2023-08-29-08-25-43-710/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--activation_loading_horizon\",\"4\",\"--bf16\",\"1\",\"--checkpoint_freq\",\"200\",\"--delayed_param\",\"1\",\"--epochs\",\"100\",\"--fp16\",\"0\",\"--gradient_accumulation\",\"1\",\"--hidden_width\",\"4544\",\"--logging_freq\",\"1\",\"--lr\",\"0.0002\",\"--lr-decay-style\",\"linear\",\"--lr_decay_iters\",\"125000\",\"--max_context_width\",\"2048\",\"--max_steps\",\"100\",\"--min_lr\",\"1e-05\",\"--model_type\",\"falcon\",\"--mp_parameters\",\"activation_loading_horizon=4,bf16=True,ddp=True,delayed_parameter_initialization=True,fp16=False,offload_activations=False,partitions=1,sharded_data_parallel_degree=16,skip_tracing=True\",\"--num_heads\",\"71\",\"--num_heads_kv\",\"71\",\"--num_kept_checkpoints\",\"5\",\"--num_layers\",\"32\",\"--offload_activations\",\"0\",\"--save_final_full_model\",\"0\",\"--seed\",\"12345\",\"--sharded_data_parallel_degree\",\"16\",\"--train_batch_size\",\"4\",\"--use_distributed_transformer\",\"0\",\"--val_batch_size\",\"4\",\"--validation_freq\",\"200\",\"--warmup\",\"0.01\",\"--zipped_data\",\"0\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_HP_ACTIVATION_LOADING_HORIZON=4\u001b[0m\n",
      "\u001b[35mSM_HP_BF16=1\u001b[0m\n",
      "\u001b[35mSM_HP_CHECKPOINT_FREQ=200\u001b[0m\n",
      "\u001b[35mSM_HP_DELAYED_PARAM=1\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=100\u001b[0m\n",
      "\u001b[35mSM_HP_FP16=0\u001b[0m\n",
      "\u001b[35mSM_HP_GRADIENT_ACCUMULATION=1\u001b[0m\n",
      "\u001b[35mSM_HP_HIDDEN_WIDTH=4544\u001b[0m\n",
      "\u001b[35mSM_HP_LOGGING_FREQ=1\u001b[0m\n",
      "\u001b[35mSM_HP_LR=0.0002\u001b[0m\n",
      "\u001b[35mSM_HP_LR-DECAY-STYLE=linear\u001b[0m\n",
      "\u001b[35mSM_HP_LR_DECAY_ITERS=125000\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_CONTEXT_WIDTH=2048\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_STEPS=100\u001b[0m\n",
      "\u001b[35mSM_HP_MIN_LR=1e-05\u001b[0m\n",
      "\u001b[35mSM_HP_MODEL_TYPE=falcon\u001b[0m\n",
      "\u001b[35mSM_HP_MP_PARAMETERS={\"activation_loading_horizon\":4,\"bf16\":true,\"ddp\":true,\"delayed_parameter_initialization\":true,\"fp16\":false,\"offload_activations\":false,\"partitions\":1,\"sharded_data_parallel_degree\":16,\"skip_tracing\":true}\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_HEADS=71\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_HEADS_KV=71\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_KEPT_CHECKPOINTS=5\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_LAYERS=32\u001b[0m\n",
      "\u001b[35mSM_HP_OFFLOAD_ACTIVATIONS=0\u001b[0m\n",
      "\u001b[35mSM_HP_SAVE_FINAL_FULL_MODEL=0\u001b[0m\n",
      "\u001b[35mSM_HP_SEED=12345\u001b[0m\n",
      "\u001b[35mSM_HP_SHARDED_DATA_PARALLEL_DEGREE=16\u001b[0m\n",
      "\u001b[35mSM_HP_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[35mSM_HP_USE_DISTRIBUTED_TRANSFORMER=0\u001b[0m\n",
      "\u001b[35mSM_HP_VAL_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[35mSM_HP_VALIDATION_FREQ=200\u001b[0m\n",
      "\u001b[35mSM_HP_WARMUP=0.01\u001b[0m\n",
      "\u001b[35mSM_HP_ZIPPED_DATA=0\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35mmpirun --host algo-2:8,algo-1:8 -np 16 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.10/site-packages/gethostname.cpython-310-x86_64-linux-gnu.so -x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x FI_PROVIDER=efa -x NCCL_PROTO=simple -x FI_EFA_USE_DEVICE_RDMA=1 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_NUM_NEURONS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TEST -x SM_CHANNEL_TRAIN -x SM_HP_ACTIVATION_LOADING_HORIZON -x SM_HP_BF16 -x SM_HP_CHECKPOINT_FREQ -x SM_HP_DELAYED_PARAM -x SM_HP_EPOCHS -x SM_HP_FP16 -x SM_HP_GRADIENT_ACCUMULATION -x SM_HP_HIDDEN_WIDTH -x SM_HP_LOGGING_FREQ -x SM_HP_LR -x SM_HP_LR-DECAY-STYLE -x SM_HP_LR_DECAY_ITERS -x SM_HP_MAX_CONTEXT_WIDTH -x SM_HP_MAX_STEPS -x SM_HP_MIN_LR -x SM_HP_MODEL_TYPE -x SM_HP_MP_PARAMETERS -x SM_HP_NUM_HEADS -x SM_HP_NUM_HEADS_KV -x SM_HP_NUM_KEPT_CHECKPOINTS -x SM_HP_NUM_LAYERS -x SM_HP_OFFLOAD_ACTIVATIONS -x SM_HP_SAVE_FINAL_FULL_MODEL -x SM_HP_SEED -x SM_HP_SHARDED_DATA_PARALLEL_DEGREE -x SM_HP_TRAIN_BATCH_SIZE -x SM_HP_USE_DISTRIBUTED_TRANSFORMER -x SM_HP_VAL_BATCH_SIZE -x SM_HP_VALIDATION_FREQ -x SM_HP_WARMUP -x SM_HP_ZIPPED_DATA -x PYTHONPATH -x NCCL_PROTO=simple -x NCCL_ALGO=ring smddpmprun -i ml.p4d.24xlarge --allow-bypass /opt/conda/bin/python3.10 -m mpi4py train.py --activation_loading_horizon 4 --bf16 1 --checkpoint_freq 200 --delayed_param 1 --epochs 100 --fp16 0 --gradient_accumulation 1 --hidden_width 4544 --logging_freq 1 --lr 0.0002 --lr-decay-style linear --lr_decay_iters 125000 --max_context_width 2048 --max_steps 100 --min_lr 1e-05 --model_type falcon --mp_parameters activation_loading_horizon=4,bf16=True,ddp=True,delayed_parameter_initialization=True,fp16=False,offload_activations=False,partitions=1,sharded_data_parallel_degree=16,skip_tracing=True --num_heads 71 --num_heads_kv 71 --num_kept_checkpoints 5 --num_layers 32 --offload_activations 0 --save_final_full_model 0 --seed 12345 --sharded_data_parallel_degree 16 --train_batch_size 4 --use_distributed_transformer 0 --val_batch_size 4 --validation_freq 200 --warmup 0.01 --zipped_data 0\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:33,426 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:32,727 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:32,727 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:32,727 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:32,727 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:32,729 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:34,141 torch.distributed.nn.jit.instantiator INFO     Created a temporary directory at /tmp/tmpy_8_x9to\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:34,141 torch.distributed.nn.jit.instantiator INFO     Writing /tmp/tmpy_8_x9to/_remote_module_non_scriptable.py\u001b[0m\n",
      "\u001b[35m2023-08-29 08:49:34,405 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35m[algo-2:00080] Warning: could not find environment variable \"SM_HP_LR-DECAY-STYLE\"\u001b[0m\n",
      "\u001b[35mWarning: Permanently added 'algo-1,10.0.85.50' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[35mData for JOB [22900,1] offset 0 Total slots allocated 16\u001b[0m\n",
      "\u001b[35m========================   JOB MAP   ========================\n",
      " Data for node: algo-2#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 7 Bound: N/A\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 8 Bound: N/A\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 9 Bound: N/A\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 10 Bound: N/A\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 11 Bound: N/A\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 12 Bound: N/A\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 13 Bound: N/A\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 14 Bound: N/A\n",
      " #011Process OMPI jobid: [22900,1] App: 0 Process rank: 15 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:34,733 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=81, name='orted', status='sleeping', started='08:49:34')]\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:34,733 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=81, name='orted', status='sleeping', started='08:49:34')]\u001b[0m\n",
      "\u001b[34m2023-08-29 08:49:34,733 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=81, name='orted', status='sleeping', started='08:49:34')]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:44.650: I smdistributed/modelparallel/torch/state_mod.py:100] [0] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-08-29 08:49:44.652: I smdistributed/modelparallel/torch/state_mod.py:100] [8] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-08-29 08:49:44.658: I smdistributed/modelparallel/torch/state_mod.py:100] [6] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-08-29 08:49:44.658: I smdistributed/modelparallel/torch/state_mod.py:100] [5] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-08-29 08:49:44.659: I smdistributed/modelparallel/torch/state_mod.py:100] [1] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-08-29 08:49:44.665: I smdistributed/modelparallel/torch/state_mod.py:100] [2] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-08-29 08:49:44.668: I smdistributed/modelparallel/torch/state_mod.py:100] [11] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-08-29 08:49:44.669: I smdistributed/modelparallel/torch/state_mod.py:100] [7] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-08-29 08:49:44.671: I smdistributed/modelparallel/torch/state_mod.py:100] [10] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-08-29 08:49:44.672: I smdistributed/modelparallel/torch/state_mod.py:100] [13] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-08-29 08:49:44.672: I smdistributed/modelparallel/torch/state_mod.py:100] [3] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-08-29 08:49:44.673: I smdistributed/modelparallel/torch/state_mod.py:100] [4] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-08-29 08:49:44.674: I smdistributed/modelparallel/torch/state_mod.py:100] [9] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-08-29 08:49:44.678: I smdistributed/modelparallel/torch/state_mod.py:100] [14] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-08-29 08:49:44.680: I smdistributed/modelparallel/torch/state_mod.py:100] [15] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-08-29 08:49:44.683: I smdistributed/modelparallel/torch/state_mod.py:100] [12] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:Successfully opened device rdmap16s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:Successfully opened device rdmap32s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:Successfully opened device rdmap16s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:Successfully opened device rdmap32s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:Successfully opened device rdmap16s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:Successfully opened device rdmap32s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:Successfully opened device rdmap144s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:Successfully opened device rdmap160s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:Successfully opened device rdmap144s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:Successfully opened device rdmap16s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:Successfully opened device rdmap32s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:Successfully opened device rdmap144s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:Successfully opened device rdmap160s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:Successfully opened device rdmap160s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:Successfully opened device rdmap144s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:Successfully opened device rdmap160s27\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= [1,mpirank:4,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= [1,mpirank:2,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size=\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:Running smdistributed.dataparallel v1.8.1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-08-29 08:49:45.377: I smdistributed/modelparallel/torch/state_mod.py:163] [7] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 7, rdp_rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-08-29 08:49:45.377: I smdistributed/modelparallel/torch/state_mod.py:163] [3] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 3, rdp_rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-08-29 08:49:45.377: I smdistributed/modelparallel/torch/state_mod.py:163] [4] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 4, rdp_rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-08-29 08:49:45.377: I smdistributed/modelparallel/torch/state_mod.py:163] [13] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 13, rdp_rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-08-29 08:49:45.378: I smdistributed/modelparallel/torch/state_mod.py:163] [14] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 14, rdp_rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-08-29 08:49:45.378: I smdistributed/modelparallel/torch/state_mod.py:163] [9] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 9, rdp_rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-08-29 08:49:45.378: I smdistributed/modelparallel/torch/state_mod.py:163] [12] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 12, rdp_rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-08-29 08:49:45.378: I smdistributed/modelparallel/torch/state_mod.py:163] [10] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 10, rdp_rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-08-29 08:49:45.378: I smdistributed/modelparallel/torch/state_mod.py:163] [8] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 8, rdp_rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-08-29 08:49:45.378: I smdistributed/modelparallel/torch/state_mod.py:163] [15] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 15, rdp_rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-08-29 08:49:45.378: I smdistributed/modelparallel/torch/state_mod.py:163] [11] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 11, rdp_rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-08-29 08:49:45.387: I smdistributed/modelparallel/torch/state_mod.py:163] [6] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 6, rdp_rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.387: I smdistributed/modelparallel/torch/state_mod.py:163] [0] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-08-29 08:49:45.387: I smdistributed/modelparallel/torch/state_mod.py:163] [2] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 2, rdp_rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-08-29 08:49:45.387: I smdistributed/modelparallel/torch/state_mod.py:163] [5] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 5, rdp_rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-08-29 08:49:45.387: I smdistributed/modelparallel/torch/state_mod.py:163] [1] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.387: I smdistributed/modelparallel/torch/throttler.py:37] Using NCCL throttle limit of 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.388: I smdistributed/modelparallel/backend/config.py:314] Configuration parameters:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.388: I smdistributed/modelparallel/backend/config.py:317]   activation_loading_horizon: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.388: I smdistributed/modelparallel/backend/config.py:317]   active_microbatches: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.388: I smdistributed/modelparallel/backend/config.py:317]   auto_partition: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.388: I smdistributed/modelparallel/backend/config.py:317]   bf16: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.388: I smdistributed/modelparallel/backend/config.py:317]   contiguous: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.388: I smdistributed/modelparallel/backend/config.py:317]   ddp: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.388: I smdistributed/modelparallel/backend/config.py:317]   ddp_dist_backend: auto\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.388: I smdistributed/modelparallel/backend/config.py:317]   ddp_port: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.388: I smdistributed/modelparallel/backend/config.py:317]   default_partition: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.388: I smdistributed/modelparallel/backend/config.py:317]   delayed_parameter_initialization: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.388: I smdistributed/modelparallel/backend/config.py:317]   fp16: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   fp16_params: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   horovod: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   memory_weight: 0.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   microbatches: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   offload_activations: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   optimize: speed\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   pipeline: interleaved\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   pipeline_parallel_degree: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   placement_strategy: cluster\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   predefined_hooks: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   prescaled_batch: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   sdp_gradient_clipping: 1.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   sdp_hierarchical_allgather: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   sdp_max_live_parameters: 1000000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   sdp_param_persistence_threshold: 1000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.389: I smdistributed/modelparallel/backend/config.py:317]   sdp_reduce_bucket_size: 500000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.390: I smdistributed/modelparallel/backend/config.py:317]   shard_optimizer_state: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.390: I smdistributed/modelparallel/backend/config.py:317]   sharded_data_parallel_degree: 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.390: I smdistributed/modelparallel/backend/config.py:317]   skip_tracing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.390: I smdistributed/modelparallel/backend/config.py:317]   tensor_parallel_degree: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.390: I smdistributed/modelparallel/backend/config.py:317]   tensor_parallel_seed: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.390: W smdistributed/modelparallel/backend/config.py:323] WARNING: \"fp16_params\" is a deprecated config key, please use \"fp16\" instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-08-29 08:49:45.436: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-08-29 08:49:45.436: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-08-29 08:49:45.436: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-08-29 08:49:45.436: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-08-29 08:49:45.437: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-08-29 08:49:45.439: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-08-29 08:49:45.439: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-08-29 08:49:45.445: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-08-29 08:49:45.446: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-08-29 08:49:45.446: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-08-29 08:49:45.446: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45.452: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Env variables (len = 195):\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [000/195] AWS_CONTAINER_CREDENTIALS_RELATIVE_URI: `/v2/credentials/proxy-c48a1614f272bbd3a409b735b27dfe81fb88eb12a4fd287bc69e626a79adad2b-customer`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [001/195] AWS_REGION          : `us-east-1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [002/195] BRANCH_OFI          : `1.5.0-aws`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [003/195] CMAKE_PREFIX_PATH   : `$(dirname $(which conda))/../`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [004/195] CUDA_HOME           : `/opt/conda/`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [005/195] CUDA_MODULE_LOADING : `LAZY`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [006/195] CUDA_VERSION        : `11.8.0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [007/195] CUDNN_VERSION       : `8.7.0.84`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [008/195] CURRENT_HOST        : `algo-2`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [009/195] DEBIAN_FRONTEND     : `noninteractive`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [010/195] DGLBACKEND          : `pytorch`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [011/195] DLC_CONTAINER_TYPE  : `training`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [012/195] DMLC_INTERFACE      : `eth0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [013/195] EFA_VERSION         : `1.21.0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [014/195] FI_EFA_USE_DEVICE_RDMA: `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [015/195] FI_PROVIDER         : `efa`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [016/195] GDRCOPY_VERSION     : `2.3.1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [017/195] HFI_NO_BACKTRACE    : `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [018/195] HOME                : `/root`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [019/195] HOROVOD_VERSION     : `0.26.1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [020/195] HOSTNAME            : `ip-10-0-91-141.ec2.internal`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [021/195] IPATH_NO_BACKTRACE  : `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [022/195] LANG                : `C.UTF-8`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [023/195] LC_ALL              : `C.UTF-8`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [024/195] LD_LIBRARY_PATH     : `/opt/amazon/openmpi/lib:/opt/conda/lib/python3.10/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [025/195] LD_PRELOAD          : `/opt/conda/lib/python3.10/site-packages/gethostname.cpython-310-x86_64-linux-gnu.so`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [026/195] LOCAL_RANK          : `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [027/195] MANUAL_BUILD        : `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [028/195] MASTER_ADDR         : `algo-2`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [029/195] MASTER_PORT         : `7777`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [030/195] NCCL_ALGO           : `ring`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [031/195] NCCL_DEBUG          : `WARN`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [032/195] NCCL_IB_DISABLE     : `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [033/195] NCCL_MIN_NRINGS     : `4`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [034/195] NCCL_PROTO          : `simple`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [035/195] NCCL_SOCKET_IFNAME  : `eth0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [036/195] NCCL_VERSION        : `2.16.2`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [037/195] NVARCH              : `x86_64`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [038/195] NVIDIA_DRIVER_CAPABILITIES: `compute,utility,compat32,graphics,video`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [039/195] NVIDIA_REQUIRE_CUDA : `cuda>=11.8 brand=tesla,driver>=450,driver<451 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=510,driver<511 brand=unknown,driver>=510,driver<511 brand=nvidia,driver>=510,driver<511 brand=nvidiartx,driver>=510,driver<511 brand=geforce,driver>=510,driver<511 brand=geforcertx,driver>=510,driver<511 brand=quadro,driver>=510,driver<511 brand=quadrortx,driver>=510,driver<511 brand=titan,driver>=510,driver<511 brand=titanrtx,driver>=510,driver<511 brand=tesla,driver>=515,driver<516 brand=unknown,driver>=515,driver<516 brand=nvidia,driver>=515,driver<516 brand=nvidiartx,driver>=515,driver<516 brand=geforce,driver>=515,driver<516 brand=geforcertx,driver>=515,driver<516 brand=quadro,driver>=515,driver<516 brand=quadrortx,driver>=515,driver<516 brand=titan,driver>=515,driver<516 brand=titanrtx,driver>=515,driver<516`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [040/195] NVIDIA_VISIBLE_DEVICES: `all`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [041/195] NV_CUDA_COMPAT_PACKAGE: `cuda-compat-11-8`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [042/195] NV_CUDA_CUDART_VERSION: `11.8.89-1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [043/195] OMPI_APP_CTX_NUM_PROCS: `16`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [044/195] OMPI_ARGV           : `-i ml.p4d.24xlarge --allow-bypass /opt/conda/bin/python3.10 -m mpi4py train.py --activation_loading_horizon 4 --bf16 1 --checkpoint_freq 200 --delayed_param 1 --epochs 100 --fp16 0 --gradient_accumulation 1 --hidden_width 4544 --logging_freq 1 --lr 0.0002 --lr-decay-style linear --lr_decay_iters 125000 --max_context_width 2048 --max_steps 100 --min_lr 1e-05 --model_type falcon --mp_parameters activation_loading_horizon=4,bf16=True,ddp=True,delayed_parameter_initialization=True,fp16=False,offload_activations=False,partitions=1,sharded_data_parallel_degree=16,skip_tracing=True --num_heads 71 --num_heads_kv 71 --num_kept_checkpoints 5 --num_layers 32 --offload_activations 0 --save_final_full_model 0 --seed 12345 --sharded_data_parallel_degree 16 --train_batch_size 4 --use_distributed_transformer 0 --val_batch_size 4 --validation_freq 200 --warmup 0.01 --zipped_data 0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [045/195] OMPI_COMMAND        : `smddpmprun`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [046/195] OMPI_COMM_WORLD_LOCAL_RANK: `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [047/195] OMPI_COMM_WORLD_LOCAL_SIZE: `8`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [048/195] OMPI_COMM_WORLD_NODE_RANK: `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [049/195] OMPI_COMM_WORLD_RANK: `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [050/195] OMPI_COMM_WORLD_SIZE: `16`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [051/195] OMPI_FILE_LOCATION  : `/tmp/ompi.algo-2.0/pid.80/0/0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [052/195] OMPI_FIRST_RANKS    : `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [053/195] OMPI_MCA_btl        : `^openib`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [054/195] OMPI_MCA_btl_tcp_if_include: `eth0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [055/195] OMPI_MCA_btl_vader_single_copy_mechanism: `none`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [056/195] OMPI_MCA_ess        : `^singleton`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [057/195] OMPI_MCA_ess_base_jobid: `1500774401`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [058/195] OMPI_MCA_ess_base_vpid: `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [059/195] OMPI_MCA_hwloc_base_binding_policy: `none`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [060/195] OMPI_MCA_initial_wdir: `/opt/ml/code`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [061/195] OMPI_MCA_mpi_oversubscribe: `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [062/195] OMPI_MCA_oob_tcp_if_include: `eth0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [063/195] OMPI_MCA_orte_abort_on_non_zero_status: `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [064/195] OMPI_MCA_orte_app_num: `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [065/195] OMPI_MCA_orte_ess_node_rank: `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [066/195] OMPI_MCA_orte_ess_num_procs: `16`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [067/195] OMPI_MCA_orte_hnp_uri: `1500774400.0;tcp://10.0.91.141:53109`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [068/195] OMPI_MCA_orte_jobfam_session_dir: `/tmp/ompi.algo-2.0/pid.80`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [069/195] OMPI_MCA_orte_launch: `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [070/195] OMPI_MCA_orte_local_daemon_uri: `1500774400.0;tcp://10.0.91.141:53109`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [071/195] OMPI_MCA_orte_num_nodes: `2`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [072/195] OMPI_MCA_orte_precondition_transports: `dc4d2a997dba8fd1-2fa4b80738c02f5a`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [073/195] OMPI_MCA_orte_tag_output: `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [074/195] OMPI_MCA_orte_tmpdir_base: `/tmp`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [075/195] OMPI_MCA_orte_top_session_dir: `/tmp/ompi.algo-2.0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [076/195] OMPI_MCA_plm_rsh_no_tree_spawn: `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [077/195] OMPI_MCA_pmix       : `^s1,s2,cray,isolated`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [078/195] OMPI_MCA_pml        : `ob1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [079/195] OMPI_MCA_rmaps_base_display_map: `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [080/195] OMPI_MCA_rmaps_base_mapping_policy: `slot`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [081/195] OMPI_MCA_shmem_RUNTIME_QUERY_hint: `mmap`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [082/195] OMPI_NUM_APP_CTX    : `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [083/195] OMPI_UNIVERSE_SIZE  : `16`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [084/195] OMPI_VERSION        : `4.1.5`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [085/195] OPEN_MPI_PATH       : `/opt/amazon/openmpi`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [086/195] PATH                : `/opt/amazon/openmpi/bin:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [087/195] PMIX_BFROP_BUFFER_TYPE: `PMIX_BFROP_BUFFER_NON_DESC`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [088/195] PMIX_DSTORE_21_BASE_PATH: `/tmp/ompi.algo-2.0/pid.80/pmix_dstor_ds21_80`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [089/195] PMIX_DSTORE_ESH_BASE_PATH: `/tmp/ompi.algo-2.0/pid.80/pmix_dstor_ds12_80`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [090/195] PMIX_GDS_MODULE     : `ds21,ds12,hash`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [091/195] PMIX_HOSTNAME       : `algo-2`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [092/195] PMIX_ID             : `1500774401.0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [093/195] PMIX_MCA_mca_base_component_show_load_errors: `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [094/195] PMIX_NAMESPACE      : `1500774401`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [095/195] PMIX_PTL_MODULE     : `tcp,usock`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [096/195] PMIX_RANK           : `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [097/195] PMIX_SECURITY_MODE  : `native`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [098/195] PMIX_SERVER_TMPDIR  : `/tmp/ompi.algo-2.0/pid.80`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [099/195] PMIX_SERVER_URI2    : `1500774400.0;tcp4://127.0.0.1:36991`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [100/195] PMIX_SERVER_URI21   : `1500774400.0;tcp4://127.0.0.1:36991`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [101/195] PMIX_SERVER_URI3    : `1500774400.0;tcp4://127.0.0.1:36991`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [102/195] PMIX_SYSTEM_TMPDIR  : `/tmp`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [103/195] PMIX_VERSION        : `3.2.4rc1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [104/195] PWD                 : `/opt/ml/code`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [105/195] PYTHONDONTWRITEBYTECODE: `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [106/195] PYTHONIOENCODING    : `UTF-8`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [107/195] PYTHONPATH          : `/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [108/195] PYTHONUNBUFFERED    : `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [109/195] RANK                : `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [110/195] RDMAV_FORK_SAFE     : `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [111/195] REQUESTS_CA_BUNDLE  : `/etc/ssl/certs/ca-certificates.crt`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [112/195] SAGEMAKER_INSTANCE_TYPE: `ml.p4d.24xlarge`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [113/195] SAGEMAKER_JOB_NAME  : `smp-falcon-7b-p4d24x-sdp16-bs4-2023-08-29-08-25-43-710`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [114/195] SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY: `/opt/ml/sagemaker/warmpoolcache`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [115/195] SAGEMAKER_METRICS_DIRECTORY: `/opt/ml/output/metrics/sagemaker`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [116/195] SAGEMAKER_REGION    : `us-east-1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [117/195] SAGEMAKER_TRAINING_MODULE: `sagemaker_pytorch_container.training:main`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [118/195] SHLVL               : `3`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [119/195] SMDATAPARALLEL_DEVICE_NAME: `rdmap16s27`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [120/195] SMDATAPARALLEL_LMC_ENABLE: `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [121/195] SMDEBUG_LOG_LEVEL   : `ERROR`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [122/195] SMP_D2D_GPU_BUFFER_SIZE_BYTES: `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [123/195] SMP_DISABLE_D2D     : `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [124/195] SMP_NCCL_THROTTLE_LIMIT: `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [125/195] SM_CHANNELS         : `[\"test\",\"train\"]`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [126/195] SM_CHANNEL_TEST     : `/opt/ml/input/data/test`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [127/195] SM_CHANNEL_TRAIN    : `/opt/ml/input/data/train`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [128/195] SM_CURRENT_HOST     : `algo-2`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [129/195] SM_CURRENT_INSTANCE_GROUP: `homogeneousCluster`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [130/195] SM_CURRENT_INSTANCE_GROUP_HOSTS: `[\"algo-2\",\"algo-1\"]`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [131/195] SM_CURRENT_INSTANCE_TYPE: `ml.p4d.24xlarge`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [132/195] SM_DISTRIBUTION_INSTANCE_GROUPS: `[\"homogeneousCluster\"]`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [133/195] SM_DLC_TORCH_VERSION: `2.0.1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [134/195] SM_FRAMEWORK_MODULE : `sagemaker_pytorch_container.training:main`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [135/195] SM_FRAMEWORK_PARAMS : `{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [136/195] SM_HOSTS            : `[\"algo-1\",\"algo-2\"]`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [137/195] SM_HPS              : `{\"activation_loading_horizon\":4,\"bf16\":1,\"checkpoint_freq\":200,\"delayed_param\":1,\"epochs\":100,\"fp16\":0,\"gradient_accumulation\":1,\"hidden_width\":4544,\"logging_freq\":1,\"lr\":0.0002,\"lr-decay-style\":\"linear\",\"lr_decay_iters\":125000,\"max_context_width\":2048,\"max_steps\":100,\"min_lr\":1e-05,\"model_type\":\"falcon\",\"mp_parameters\":{\"activation_loading_horizon\":4,\"bf16\":true,\"ddp\":true,\"delayed_parameter_initialization\":true,\"fp16\":false,\"offload_activations\":false,\"partitions\":1,\"sharded_data_parallel_degree\":16,\"skip_tracing\":true},\"num_heads\":71,\"num_heads_kv\":71,\"num_kept_checkpoints\":5,\"num_layers\":32,\"offload_activations\":0,\"save_final_full_model\":0,\"seed\":12345,\"sharded_data_parallel_degree\":16,\"train_batch_size\":4,\"use_distributed_transformer\":0,\"val_batch_size\":4,\"validation_freq\":200,\"warmup\":0.01,\"zipped_data\":0}`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [138/195] SM_HP_ACTIVATION_LOADING_HORIZON: `4`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [139/195] SM_HP_BF16          : `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [140/195] SM_HP_CHECKPOINT_FREQ: `200`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [141/195] SM_HP_DELAYED_PARAM : `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [142/195] SM_HP_EPOCHS        : `100`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [143/195] SM_HP_FP16          : `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [144/195] SM_HP_GRADIENT_ACCUMULATION: `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [145/195] SM_HP_HIDDEN_WIDTH  : `4544`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [146/195] SM_HP_LOGGING_FREQ  : `1`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [147/195] SM_HP_LR            : `0.0002`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [148/195] SM_HP_LR_DECAY_ITERS: `125000`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [149/195] SM_HP_MAX_CONTEXT_WIDTH: `2048`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [150/195] SM_HP_MAX_STEPS     : `100`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [151/195] SM_HP_MIN_LR        : `1e-05`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [152/195] SM_HP_MODEL_TYPE    : `falcon`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [153/195] SM_HP_MP_PARAMETERS : `{\"activation_loading_horizon\":4,\"bf16\":true,\"ddp\":true,\"delayed_parameter_initialization\":true,\"fp16\":false,\"offload_activations\":false,\"partitions\":1,\"sharded_data_parallel_degree\":16,\"skip_tracing\":true}`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [154/195] SM_HP_NUM_HEADS     : `71`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [155/195] SM_HP_NUM_HEADS_KV  : `71`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [156/195] SM_HP_NUM_KEPT_CHECKPOINTS: `5`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [157/195] SM_HP_NUM_LAYERS    : `32`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [158/195] SM_HP_OFFLOAD_ACTIVATIONS: `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [159/195] SM_HP_SAVE_FINAL_FULL_MODEL: `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [160/195] SM_HP_SEED          : `12345`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [161/195] SM_HP_SHARDED_DATA_PARALLEL_DEGREE: `16`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [162/195] SM_HP_TRAIN_BATCH_SIZE: `4`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [163/195] SM_HP_USE_DISTRIBUTED_TRANSFORMER: `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [164/195] SM_HP_VALIDATION_FREQ: `200`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [165/195] SM_HP_VAL_BATCH_SIZE: `4`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [166/195] SM_HP_WARMUP        : `0.01`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [167/195] SM_HP_ZIPPED_DATA   : `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [168/195] SM_INPUT_CONFIG_DIR : `/opt/ml/input/config`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [169/195] SM_INPUT_DATA_CONFIG: `{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [170/195] SM_INPUT_DIR        : `/opt/ml/input`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [171/195] SM_INSTANCE_GROUPS  : `[\"homogeneousCluster\"]`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [172/195] SM_INSTANCE_GROUPS_DICT: `{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [173/195] SM_IS_HETERO        : `false`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [174/195] SM_LOG_LEVEL        : `20`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [175/195] SM_MODEL_DIR        : `/opt/ml/model`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [176/195] SM_MODULE_DIR       : `s3://sagemaker-us-east-1-079002598131/smp-falcon-7b-p4d24x-sdp16-bs4-2023-08-29-08-25-43-710/source/sourcedir.tar.gz`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [177/195] SM_MODULE_NAME      : `train`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [178/195] SM_NETWORK_INTERFACE_NAME: `eth0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [179/195] SM_NUM_CPUS         : `96`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [180/195] SM_NUM_GPUS         : `8`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [181/195] SM_NUM_NEURONS      : `0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [182/195] SM_OUTPUT_DATA_DIR  : `/opt/ml/output/data`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [183/195] SM_OUTPUT_DIR       : `/opt/ml/output`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [184/195] SM_OUTPUT_INTERMEDIATE_DIR: `/opt/ml/output/intermediate`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [185/195] SM_RESOURCE_CONFIG  : `{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [186/195] SM_TRAINING_ENV     : `{\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-2\",\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"activation_loading_horizon\":4,\"bf16\":1,\"checkpoint_freq\":200,\"delayed_param\":1,\"epochs\":100,\"fp16\":0,\"gradient_accumulation\":1,\"hidden_width\":4544,\"logging_freq\":1,\"lr\":0.0002,\"lr-decay-style\":\"linear\",\"lr_decay_iters\":125000,\"max_context_width\":2048,\"max_steps\":100,\"min_lr\":1e-05,\"model_type\":\"falcon\",\"mp_parameters\":{\"activation_loading_horizon\":4,\"bf16\":true,\"ddp\":true,\"delayed_parameter_initialization\":true,\"fp16\":false,\"offload_activations\":false,\"partitions\":1,\"sharded_data_parallel_degree\":16,\"skip_tracing\":true},\"num_heads\":71,\"num_heads_kv\":71,\"num_kept_checkpoints\":5,\"num_layers\":32,\"offload_activations\":0,\"save_final_full_model\":0,\"seed\":12345,\"sharded_data_parallel_degree\":16,\"train_batch_size\":4,\"use_distributed_transformer\":0,\"val_batch_size\":4,\"validation_freq\":200,\"warmup\":0.01,\"zipped_data\":0},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"smp-falcon-7b-p4d24x-sdp16-bs4-2023-08-29-08-25-43-710\",\"log_level\":20,\"master_hostname\":\"algo-2\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-079002598131/smp-falcon-7b-p4d24x-sdp16-bs4-2023-08-29-08-25-43-710/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [187/195] SM_USER_ARGS        : `[\"--activation_loading_horizon\",\"4\",\"--bf16\",\"1\",\"--checkpoint_freq\",\"200\",\"--delayed_param\",\"1\",\"--epochs\",\"100\",\"--fp16\",\"0\",\"--gradient_accumulation\",\"1\",\"--hidden_width\",\"4544\",\"--logging_freq\",\"1\",\"--lr\",\"0.0002\",\"--lr-decay-style\",\"linear\",\"--lr_decay_iters\",\"125000\",\"--max_context_width\",\"2048\",\"--max_steps\",\"100\",\"--min_lr\",\"1e-05\",\"--model_type\",\"falcon\",\"--mp_parameters\",\"activation_loading_horizon=4,bf16=True,ddp=True,delayed_parameter_initialization=True,fp16=False,offload_activations=False,partitions=1,sharded_data_parallel_degree=16,skip_tracing=True\",\"--num_heads\",\"71\",\"--num_heads_kv\",\"71\",\"--num_kept_checkpoints\",\"5\",\"--num_layers\",\"32\",\"--offload_activations\",\"0\",\"--save_final_full_model\",\"0\",\"--seed\",\"12345\",\"--sharded_data_parallel_degree\",\"16\",\"--train_batch_size\",\"4\",\"--use_distributed_transformer\",\"0\",\"--val_batch_size\",\"4\",\"--validation_freq\",\"200\",\"--warmup\",\"0.01\",\"--zipped_data\",\"0\"]`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [188/195] SM_USER_ENTRY_POINT : `train.py`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [189/195] TORCH_CUDA_ARCH_LIST: `3.7 5.0 7.0+PTX 8.0`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [190/195] TORCH_NVCC_FLAGS    : `-Xfatbin -compress-all`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [191/195] TRAINING_JOB_ARN    : `arn:aws:sagemaker:us-east-1:079002598131:training-job/smp-falcon-7b-p4d24x-sdp16-bs4-2023-08-29-08-25-43-710`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [192/195] TRAINING_JOB_NAME   : `smp-falcon-7b-p4d24x-sdp16-bs4-2023-08-29-08-25-43-710`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [193/195] WORLD_SIZE          : `16`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:  env [194/195] _                   : `/opt/conda/bin/python3.10`\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Arguments: {'train_batch_size': 4, 'val_batch_size': 4, 'max_steps': 100, 'seed': 12345, 'same_seed': 0, 'n_gpus': '8', 'fp16': 0, 'bf16': 1, 'sharded_data_parallel_degree': 16, 'ddp_dist_backend': 'auto', 'grad_clip': 1.0, 'weight_decay': 0.01, 'beta1': 0.9, 'beta2': 0.95, 'activation_checkpointing': 1, 'logging_freq': 1, 'log_param_norms': 0, 'log_reduced_training_loss': 0, 'use_bert_data': 0, 'zipped_data': 0, 'epochs': 100, 'output_data_dir': '/opt/ml/output/data', 'checkpoint_dir': '/opt/ml/checkpoints', 'model_dir': '/opt/ml/model', 'training_dir': '/opt/ml/input/data/train', 'test_dir': '/opt/ml/input/data/test', 'parallel_proc_data_processing': 0, 'save_final_full_model': 0, 'load_partial': 0, 'load_full': 0, 'logits_output': '', 'prescaled_batch': 1, 'fine_tune': 0, 'model_name': '', 'max_context_width': 2048, 'vocab_size': 50264, 'hidden_width': 4544, 'intermediate_size': 2048, 'num_layers': 32, 'num_heads': 71, 'num_heads_kv': 71, 'resid_pdrop': 0.1, 'embd_pdrop': 0.1, 'attn_pdrop': 0.1, 'alibi': 0, 'summary_first_pdrop': 0.1, 'use_adamw': 0, 'use_distributed_transformer': 0, 'checkpoint_sublayers': 0, 'initializer_range': 0.02, 'tensor_parallel_degree': 1, 'pipeline_parallel_degree': 1, 'microbatches': 1, 'active_microbatches': None, 'optimize': 'speed', 'activation_strategy': 'each', 'shard_optimizer_state': 0, 'offload_activations': 0, 'fast_mode': 0, 'static_mode': 0, 'delayed_param': 1, 'same_partition_load': 0, 'attention_in_fp32': 0, 'residual_addition_in_fp32': 0, 'placement_strategy': 'cluster', 'activation_loading_horizon': 4, 'skip_tracing': 0, 'query_key_layer_scaling': 0, 'fused_softmax': 1, 'flash_attention': 1, 'fused_dropout': 0, 'fused_bias_gelu': 1, 'gradient_accumulation': 1, 'model_type': 'falcon', 'rotary_pct': 0.25, 'rotary_emb_base': 10000, 'num_kept_checkpoints': 5, 'checkpoint_freq': 200, 'validation_freq': 200, 'validation_batches': 10, 'manual_partition': 0, 'partition_assignment': '', 'preserve_np_state': 0, 'fast_validation': 1, 'gather_if_shard': 1, 'clean_cache': 0, 'use_fsx': 0, 'enable_memory_profiling': 0, 'lr': 0.0002, 'lr_decay_style': 'linear', 'lr_decay_iters': 125000, 'min_lr': 1e-05, 'warmup': 0.01, 'plateau': 0.4, 'ci': False, 'time_to_train': None, 'throughput': None, 'loss': None}\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Transformers version: 4.21.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:smdistributed.modelparallel version: 1.15.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:smdistributed config: {'ddp': True, 'tensor_parallel_degree': 1, 'pipeline_parallel_degree': 1, 'microbatches': 1, 'shard_optimizer_state': False, 'prescaled_batch': True, 'fp16': False, 'bf16': True, 'offload_activations': False, 'delayed_parameter_initialization': True, 'optimize': 'speed', 'placement_strategy': 'cluster', 'activation_loading_horizon': 4, 'skip_tracing': False, 'auto_partition': True, 'default_partition': 0, 'static_mode': False, 'fast_mode': False, 'sharded_data_parallel_degree': 16, 'ddp_dist_backend': 'auto', 'sdp_hierarchical_allgather': False, 'sdp_gradient_clipping': 1.0}\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-08-29 08:49:45.465: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-08-29 08:49:45.466: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-08-29 08:49:45.468: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-08-29 08:49:45.468: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading config.json: 100%|██████████| 950/950 [00:00<00:00, 1.64MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading config.json: 100%|██████████| 950/950 [00:00<00:00, 1.60MB/s][1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading configuration_RW.py:   0%|          | 0.00/2.55k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Downloading configuration_RW.py: 100%|██████████| 2.55k/2.55k [00:00<00:00, 4.93MB/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading configuration_RW.py:   0%|          | 0.00/2.55k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Downloading configuration_RW.py: 100%|██████████| 2.55k/2.55k [00:00<00:00, 4.56MB/s][1,mpirank:7,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-08-29 08:49:45,838] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:rank 6 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-08-29 08:49:45,838] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:rank 5 enable local shard at partition parameters Init[1,mpirank:4,algo-1]<stdout>:[2023-08-29 08:49:45,838] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:rank 4 enable local shard at partition parameters Init[1,mpirank:1,algo-1]<stdout>:[2023-08-29 08:49:45,838] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:rank 1 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:45,838] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:rank 0 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-08-29 08:49:45,838] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:rank 2 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-08-29 08:49:45,838] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:rank 3 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:shard size 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:mp size 1[1,mpirank:7,algo-1]<stdout>:[2023-08-29 08:49:45,838] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:rank 7 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-08-29 08:49:45,838] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:rank 13 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-08-29 08:49:45,838] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:rank 11 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-08-29 08:49:45,838] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-08-29 08:49:45,838] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:rank 14 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:rank 8 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-08-29 08:49:45,838] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:rank 15 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-08-29 08:49:45,839] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:rank 9 enable local shard at partition parameters Init[1,mpirank:10,algo-2]<stdout>:[2023-08-29 08:49:45,839] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:rank 10 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-08-29 08:49:45,839] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:rank 12 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:rank 9, model_parallel_rank 0, shard group 9/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:rank 9 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:rank 12, model_parallel_rank 0, shard group 12/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:rank 12 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:rank 13, model_parallel_rank 0, shard group 13/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:rank 13 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:rank 11, model_parallel_rank 0, shard group 11/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:rank 11 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:rank 10, model_parallel_rank 0, shard group 10/16[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:rank 10 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:rank 14, model_parallel_rank 0, shard group 14/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:rank 8, model_parallel_rank 0, shard group 8/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:rank 8 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:rank 14 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:rank 15, model_parallel_rank 0, shard group 15/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:rank 15 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:rank 0, model_parallel_rank 0, shard group 0/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:rank 0 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:created shard groups and replicate groups based on shard size 16 and mp size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:rank 6, model_parallel_rank 0, shard group 6/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:rank 6 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:rank 5, model_parallel_rank 0, shard group 5/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:rank 5 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:rank 1, model_parallel_rank 0, shard group 1/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:rank 1 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:rank 7, model_parallel_rank 0, shard group 7/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:rank 2, model_parallel_rank 0, shard group 2/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:rank 2 replicate group 0/1[1,mpirank:3,algo-1]<stdout>:rank 3, model_parallel_rank 0, shard group 3/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:rank 7 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:rank 4, model_parallel_rank 0, shard group 4/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:rank 3 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:rank 4 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:nn.functional.linear has been overridden with a more memory efficient version. This will persist unless manually reset.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-08-29 08:49:53.717: I smdistributed/modelparallel/torch/model.py:153] [7] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-08-29 08:49:53,719] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:rank 7 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-08-29 08:49:53.719: I smdistributed/modelparallel/torch/model.py:153] [15] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-08-29 08:49:53.719: I smdistributed/modelparallel/torch/model.py:153] [14] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-08-29 08:49:53,721] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:rank 15 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-08-29 08:49:53,721] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:rank 14 enable local shard at partition parameters Init[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-08-29 08:49:53.725: I smdistributed/modelparallel/torch/model.py:153] [6] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-08-29 08:49:53,727] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:rank 6 enable local shard at partition parameters Init[1,mpirank:6,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-08-29 08:49:53.730: I smdistributed/modelparallel/torch/model.py:153] [5] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-08-29 08:49:53.731: I smdistributed/modelparallel/torch/model.py:153] [12] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-08-29 08:49:53,732] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:rank 5 enable local shard at partition parameters Init[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-08-29 08:49:53,733] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:rank 12 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-08-29 08:49:53.738: I smdistributed/modelparallel/torch/model.py:153] [4] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-08-29 08:49:53,740] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:rank 4 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-08-29 08:49:53.744: I smdistributed/modelparallel/torch/model.py:153] [11] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-08-29 08:49:53,746] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:rank 11 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-08-29 08:49:53.746: I smdistributed/modelparallel/torch/model.py:153] [13] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-08-29 08:49:53.747: I smdistributed/modelparallel/torch/model.py:153] [1] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-08-29 08:49:53,749] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:rank 13 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-08-29 08:49:53,749] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:rank 1 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-08-29 08:49:53.752: I smdistributed/modelparallel/torch/model.py:153] [10] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:53,753] [INFO] [partition_parameters.py:519:__exit__] finished initializing model with 8.52B parameters\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-08-29 08:49:53,754] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:rank 10 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-08-29 08:49:53.755: I smdistributed/modelparallel/torch/model.py:153] [8] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:53.756: I smdistributed/modelparallel/torch/model.py:153] [0] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-08-29 08:49:53,757] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:rank 8 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:53,757] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:rank 0 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:shard size 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:mp size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-08-29 08:49:53.760: I smdistributed/modelparallel/torch/model.py:153] [2] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-08-29 08:49:53.761: I smdistributed/modelparallel/torch/model.py:153] [3] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-08-29 08:49:53,763] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:rank 2 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-08-29 08:49:53,763] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:rank 3 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-08-29 08:49:53.784: I smdistributed/modelparallel/torch/model.py:153] [9] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-08-29 08:49:53,787] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"16\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:rank 9 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:rank 8, model_parallel_rank 0, shard group 8/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:rank 8 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:rank 9, model_parallel_rank 0, shard group 9/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:rank 9 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:rank 13, model_parallel_rank 0, shard group 13/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:rank 12, model_parallel_rank 0, shard group 12/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:rank 11, model_parallel_rank 0, shard group 11/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:rank 11 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:rank 13 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:rank 14, model_parallel_rank 0, shard group 14/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:rank 14 replicate group 0/1[1,mpirank:12,algo-2]<stdout>:rank 12 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:rank 10, model_parallel_rank 0, shard group 10/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:rank 10 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:rank 15, model_parallel_rank 0, shard group 15/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:rank 15 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:rank 0, model_parallel_rank 0, shard group 0/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:rank 0 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:rank 6, model_parallel_rank 0, shard group 6/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:rank 2, model_parallel_rank 0, shard group 2/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:created shard groups and replicate groups based on shard size 16 and mp size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:rank 2 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:rank 5, model_parallel_rank 0, shard group 5/16[1,mpirank:6,algo-1]<stdout>:rank 6 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:rank 1, model_parallel_rank 0, shard group 1/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:rank 5 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:rank 3, model_parallel_rank 0, shard group 3/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:rank 3 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:rank 1 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:rank 7, model_parallel_rank 0, shard group 7/16\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:rank 7 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:rank 4, model_parallel_rank 0, shard group 4/16[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:rank 4 replicate group 0/1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:nn.functional.linear has been overridden with a more memory efficient version. This will persist unless manually reset.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:53,947] [INFO] [partition_parameters.py:519:__exit__] finished initializing model with 8.52B parameters\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:# total parameters: 8224867200\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:53,964] [INFO] [stage3.py:661:__init__] Reduce bucket size 500000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:49:53,964] [INFO] [stage3.py:662:__init__] Allgather bucket size 50000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:Creating extension directory /root/.cache/torch_extensions/py310_cu118/utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:Creating extension directory /root/.cache/torch_extensions/py310_cu118/utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:Creating extension directory /root/.cache/torch_extensions/py310_cu118/utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/utils/build.ninja...\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:Building extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/utils/build.ninja...\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:Building extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:Time to load utils op: 14.402769327163696 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-08-29 08:50:08,681] [INFO] [stage3.py:1036:_zero2d_setups] rank 13, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-08-29 08:50:08,682] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 13 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-08-29 08:50:08,682] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f31b8a96db0>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:Time to load utils op: 14.419740438461304 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-08-29 08:50:08,698] [INFO] [stage3.py:1036:_zero2d_setups] rank 11, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-08-29 08:50:08,699] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 11 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-08-29 08:50:08,699] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f96eb9d7bb0>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:Time to load utils op: 14.418947458267212 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-08-29 08:50:08,706] [INFO] [stage3.py:1036:_zero2d_setups] rank 10, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-08-29 08:50:08,707] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 10 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-08-29 08:50:08,707] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7fc8ae2c5170>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:Time to load utils op: 14.42794680595398 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-08-29 08:50:08,718] [INFO] [stage3.py:1036:_zero2d_setups] rank 4, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-08-29 08:50:08,719] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 4 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-08-29 08:50:08,719] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7fadf01ab2f0>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:Time to load utils op: 14.419268608093262 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:Time to load utils op: 14.419529676437378 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-08-29 08:50:08,722] [INFO] [stage3.py:1036:_zero2d_setups] rank 1, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-08-29 08:50:08,723] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 1 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-08-29 08:50:08,723] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7fb97e1a0d30>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-08-29 08:50:08,723] [INFO] [stage3.py:1036:_zero2d_setups] rank 12, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-08-29 08:50:08,725] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 12 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-08-29 08:50:08,725] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f14dcb6fd30>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:Time to load utils op: 14.418789386749268 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-08-29 08:50:08,728] [INFO] [stage3.py:1036:_zero2d_setups] rank 9, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:Time to load utils op: 14.419134378433228 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-08-29 08:50:08,729] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 9 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-08-29 08:50:08,729] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7fb6168c15b0>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:Time to load utils op: 14.41837191581726 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:Time to load utils op: 14.419266939163208 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-08-29 08:50:08,730] [INFO] [stage3.py:1036:_zero2d_setups] rank 14, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-08-29 08:50:08,731] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 14 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-08-29 08:50:08,732] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7fe76c7db670>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-08-29 08:50:08,732] [INFO] [stage3.py:1036:_zero2d_setups] rank 15, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:Time to load utils op: 14.418798923492432 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-08-29 08:50:08,732] [INFO] [stage3.py:1036:_zero2d_setups] rank 6, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-08-29 08:50:08,733] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 15 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-08-29 08:50:08,733] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f2b690b4db0>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:Time to load utils op: 14.4192214012146 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-08-29 08:50:08,733] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 6 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-08-29 08:50:08,733] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f8a3542d4f0>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:Time to load utils op: 14.419678211212158 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-08-29 08:50:08,734] [INFO] [stage3.py:1036:_zero2d_setups] rank 8, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:Time to load utils op: 14.420030117034912 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-08-29 08:50:08,735] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 8 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-08-29 08:50:08,735] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f1b1555eab0>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-08-29 08:50:08,735] [INFO] [stage3.py:1036:_zero2d_setups] rank 3, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-08-29 08:50:08,736] [INFO] [stage3.py:1036:_zero2d_setups] rank 2, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-08-29 08:50:08,736] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 3 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-08-29 08:50:08,736] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f200d547d30>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:Time to load utils op: 14.419361352920532 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:50:08,737] [INFO] [stage3.py:1036:_zero2d_setups] rank 0, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-08-29 08:50:08,737] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 2 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-08-29 08:50:08,737] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7fc725857030>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:50:08,738] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 0 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:50:08,738] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7fc8df30b070>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-08-29 08:50:08,739] [INFO] [stage3.py:1036:_zero2d_setups] rank 7, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-08-29 08:50:08,740] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 7 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-08-29 08:50:08,740] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7fa1175cb330>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:Time to load utils op: 14.41832184791565 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-08-29 08:50:08,762] [INFO] [stage3.py:1036:_zero2d_setups] rank 5, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-08-29 08:50:08,763] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 5 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-08-29 08:50:08,763] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7fd78c0b2170>, ds_param_repli_group None ds_param_shard_size 16 ds_param_repli_size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:50:10,138] [INFO] [stage3.py:875:__init__] optimizer state initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:50:10,140] [INFO] [stage3.py:913:__init__] optimizer state initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:Learning rate decay style: linear\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Creating val dataloader\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Created val dataloader of size 0.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Reading data from training path ['/opt/ml/input/data/train/training.json'].\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:50:11.141: I smdistributed/modelparallel/torch/worker.py:300] Tracing on GPU. If the model parameters do not fit in a single GPU, you can set trace_device to `cpu`.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:50:11.159: I smdistributed/modelparallel/torch/model.py:665] Partition assignments:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:50:11.160: I smdistributed/modelparallel/torch/model.py:674] main: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:50:11.166: I smdistributed/modelparallel/torch/model.py:599] Number of parameters on partition 0 are 259. 259 require grads\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:50:11.168: I smdistributed/modelparallel/torch/model.py:628] Number of buffers on partition 0 are 32.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:50:11.182: I smdistributed/modelparallel/torch/model.py:725] Finished partitioning the model\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-08-29 08:50:12.743: I smdistributed/modelparallel/torch/model.py:734] Broadcasted parameters and buffers for partition 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Compute tflops: (11.886324712222, 8224867200, 16, 1024) ==> 50.05488243410134.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(5s), Batch 0 Loss: 12.125, Speed: 11.886324712222 samples/sec, TFLOPS/GPU: 50.05488243410134 , Grad norm: 23.077941073707958\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(7s), Batch 1 Loss: 12.125, Speed: 25.35059338429351 samples/sec, TFLOPS/GPU: 106.7546951818305 , Grad norm: 23.058765059631092\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(10s), Batch 2 Loss: 12.125, Speed: 25.667514009848176 samples/sec, TFLOPS/GPU: 108.08928977159206 , Grad norm: 22.847197236273285\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(12s), Batch 3 Loss: 12.0625, Speed: 25.776954509396543 samples/sec, TFLOPS/GPU: 108.5501581620366 , Grad norm: 22.945990261080162\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(15s), Batch 4 Loss: 12.0, Speed: 25.801919528444735 samples/sec, TFLOPS/GPU: 108.65528915279081 , Grad norm: 22.96244416255355\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(17s), Batch 5 Loss: 12.0625, Speed: 25.637324848283185 samples/sec, TFLOPS/GPU: 107.96215922707934 , Grad norm: 22.955475809683136\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(20s), Batch 6 Loss: 10.4375, Speed: 25.29028222073616 samples/sec, TFLOPS/GPU: 106.5007169106309 , Grad norm: 17.783085631236602\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(22s), Batch 7 Loss: 10.375, Speed: 25.73534805672495 samples/sec, TFLOPS/GPU: 108.37494789751847 , Grad norm: 16.585388345501816\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(25s), Batch 8 Loss: 10.3125, Speed: 25.75121757720148 samples/sec, TFLOPS/GPU: 108.44177654312342 , Grad norm: 14.02982529265304\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(27s), Batch 9 Loss: 10.1875, Speed: 25.686689048007096 samples/sec, TFLOPS/GPU: 108.17003838850702 , Grad norm: 11.000411374907054\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(30s), Batch 10 Loss: 10.0, Speed: 25.497063523280048 samples/sec, TFLOPS/GPU: 107.37150027209856 , Grad norm: 10.837884523191162\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(32s), Batch 11 Loss: 9.8125, Speed: 25.32696921604042 samples/sec, TFLOPS/GPU: 106.65521069077532 , Grad norm: 12.199285406811546\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Reading data from training path ['/opt/ml/input/data/train/training.json'].\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Compute tflops: (25.626022518900037, 8224867200, 16, 1024) ==> 107.91456362606709.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(35s), Batch 12 Loss: 9.75, Speed: 25.626022518900037 samples/sec, TFLOPS/GPU: 107.91456362606709 , Grad norm: 10.39694464260456\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(37s), Batch 13 Loss: 9.625, Speed: 25.688095083159485 samples/sec, TFLOPS/GPU: 108.17595938813938 , Grad norm: 10.369910289537824\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(40s), Batch 14 Loss: 10.25, Speed: 25.63272733245152 samples/sec, TFLOPS/GPU: 107.94279848101068 , Grad norm: 41.3866042317848\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(42s), Batch 15 Loss: 9.875, Speed: 26.04254463756457 samples/sec, TFLOPS/GPU: 109.66859325134885 , Grad norm: 32.29488272156611\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(45s), Batch 16 Loss: 10.0625, Speed: 25.62183745969556 samples/sec, TFLOPS/GPU: 107.89693978930244 , Grad norm: 63.81323124257777\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(47s), Batch 17 Loss: 9.0625, Speed: 25.668797673207074 samples/sec, TFLOPS/GPU: 108.09469544744816 , Grad norm: 32.93444712335801\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(50s), Batch 18 Loss: 9.0625, Speed: 25.459913851768704 samples/sec, TFLOPS/GPU: 107.21505810136989 , Grad norm: 25.095064439766166\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(52s), Batch 19 Loss: 9.0, Speed: 25.247580399254108 samples/sec, TFLOPS/GPU: 106.32089390345627 , Grad norm: 22.1926084246639\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(55s), Batch 20 Loss: 9.125, Speed: 25.22667942243797 samples/sec, TFLOPS/GPU: 106.23287713102081 , Grad norm: 22.757855250166212\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(57s), Batch 21 Loss: 9.0625, Speed: 25.504987732809163 samples/sec, TFLOPS/GPU: 107.40487016447203 , Grad norm: 19.61456369621798\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(60s), Batch 22 Loss: 8.9375, Speed: 25.688021336232264 samples/sec, TFLOPS/GPU: 108.17564883009378 , Grad norm: 16.363949407650384\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(62s), Batch 23 Loss: 8.75, Speed: 25.65572405962543 samples/sec, TFLOPS/GPU: 108.03964073485518 , Grad norm: 14.86433927647346\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Reading data from training path ['/opt/ml/input/data/train/training.json'].\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Compute tflops: (25.390027463765346, 8224867200, 16, 1024) ==> 106.92075729603727.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(65s), Batch 24 Loss: 8.625, Speed: 25.390027463765346 samples/sec, TFLOPS/GPU: 106.92075729603727 , Grad norm: 12.748137654582822\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(68s), Batch 25 Loss: 8.5625, Speed: 25.52222674263882 samples/sec, TFLOPS/GPU: 107.47746591052436 , Grad norm: 6.417666100082049\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(70s), Batch 26 Loss: 8.75, Speed: 25.689420141768878 samples/sec, TFLOPS/GPU: 108.18153938485975 , Grad norm: 21.409063170442426\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(73s), Batch 27 Loss: 8.75, Speed: 25.032623065858157 samples/sec, TFLOPS/GPU: 105.41568019678219 , Grad norm: 28.450691670931285\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(75s), Batch 28 Loss: 8.5625, Speed: 25.184985839069494 samples/sec, TFLOPS/GPU: 106.05729994763631 , Grad norm: 10.589476737186297\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(78s), Batch 29 Loss: 8.4375, Speed: 25.59315368689616 samples/sec, TFLOPS/GPU: 107.77614863560258 , Grad norm: 7.631958094595839\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(80s), Batch 30 Loss: 8.5625, Speed: 25.323170310070697 samples/sec, TFLOPS/GPU: 106.63921299625692 , Grad norm: 11.847355468180337\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 30, Running Avg Speed: 25.323170310070697 samples/sec, Running Avg TFLOPS/GPU: 106.63921299625692\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(83s), Batch 31 Loss: 8.5625, Speed: 25.91873156346455 samples/sec, TFLOPS/GPU: 109.14720005219549 , Grad norm: 12.826874360734143\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 31, Running Avg Speed: 25.620950936767624 samples/sec, Running Avg TFLOPS/GPU: 107.8932065242262\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(85s), Batch 32 Loss: 8.5, Speed: 26.138926402641903 samples/sec, TFLOPS/GPU: 110.07446958709933 , Grad norm: 12.275731152962917\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 32, Running Avg Speed: 25.793609425392386 samples/sec, Running Avg TFLOPS/GPU: 108.62029421185059\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(88s), Batch 33 Loss: 8.3125, Speed: 25.769459083715297 samples/sec, TFLOPS/GPU: 108.5185939350487 , Grad norm: 9.340995301442925\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 33, Running Avg Speed: 25.78757183997311 samples/sec, Running Avg TFLOPS/GPU: 108.59486914265011\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(90s), Batch 34 Loss: 8.25, Speed: 26.174277184671116 samples/sec, TFLOPS/GPU: 110.22333639675381 , Grad norm: 4.470292723913165\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 34, Running Avg Speed: 25.864912908912714 samples/sec, Running Avg TFLOPS/GPU: 108.92056259347086\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(92s), Batch 35 Loss: 8.1875, Speed: 25.92127192901986 samples/sec, TFLOPS/GPU: 109.15789786689339 , Grad norm: 14.004944245986762\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 35, Running Avg Speed: 25.874306078930573 samples/sec, Running Avg TFLOPS/GPU: 108.96011847237462\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Reading data from training path ['/opt/ml/input/data/train/training.json'].\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Compute tflops: (25.95791601711952, 8224867200, 16, 1024) ==> 109.31221095913521.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(95s), Batch 36 Loss: 8.1875, Speed: 25.95791601711952 samples/sec, TFLOPS/GPU: 109.31221095913521 , Grad norm: 15.972621782486183\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 36, Running Avg Speed: 25.886250355814706 samples/sec, Running Avg TFLOPS/GPU: 109.0104173990547\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(97s), Batch 37 Loss: 8.0625, Speed: 25.715994394960255 samples/sec, TFLOPS/GPU: 108.29344707302012 , Grad norm: 4.932613689135249\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 37, Running Avg Speed: 25.8649683607079 samples/sec, Running Avg TFLOPS/GPU: 108.92079610830038\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(100s), Batch 38 Loss: 8.125, Speed: 25.619629297425796 samples/sec, TFLOPS/GPU: 107.8876409264929 , Grad norm: 8.76273011127568\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 38, Running Avg Speed: 25.837708464787667 samples/sec, Running Avg TFLOPS/GPU: 108.80600108809955\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(102s), Batch 39 Loss: 8.0625, Speed: 25.438211600149728 samples/sec, TFLOPS/GPU: 107.12366705496628 , Grad norm: 10.348142155990605\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 39, Running Avg Speed: 25.797758778323875 samples/sec, Running Avg TFLOPS/GPU: 108.63776768478623\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(105s), Batch 40 Loss: 7.96875, Speed: 25.61252810064585 samples/sec, TFLOPS/GPU: 107.85773684944914 , Grad norm: 8.932819108509307\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 40, Running Avg Speed: 25.78091962580769 samples/sec, Running Avg TFLOPS/GPU: 108.56685579066468\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(107s), Batch 41 Loss: 7.78125, Speed: 26.070503043746818 samples/sec, TFLOPS/GPU: 109.78632979047084 , Grad norm: 5.090028816595605\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 41, Running Avg Speed: 25.80505157730262 samples/sec, Running Avg TFLOPS/GPU: 108.66847862398187\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(110s), Batch 42 Loss: 8.0, Speed: 25.866343592069562 samples/sec, TFLOPS/GPU: 108.92658738910367 , Grad norm: 13.749798461758564\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 42, Running Avg Speed: 25.809766347669306 samples/sec, Running Avg TFLOPS/GPU: 108.68833314437585\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(115s), Batch 43 Loss: 7.875, Speed: 13.798221395397801 samples/sec, TFLOPS/GPU: 58.10613174955295 , Grad norm: 12.8826998888197\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 43, Running Avg Speed: 24.951798851078482 samples/sec, Running Avg TFLOPS/GPU: 105.07531875903135\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(117s), Batch 44 Loss: 7.8125, Speed: 25.56594136858199 samples/sec, TFLOPS/GPU: 107.66155397098143 , Grad norm: 5.823984223854621\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 44, Running Avg Speed: 24.992741685578718 samples/sec, Running Avg TFLOPS/GPU: 105.24773443982802\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(120s), Batch 45 Loss: 7.78125, Speed: 25.902927416151798 samples/sec, TFLOPS/GPU: 109.08064670161289 , Grad norm: 8.146314352464987\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 45, Running Avg Speed: 25.049628293739534 samples/sec, Running Avg TFLOPS/GPU: 105.48729145618958\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(122s), Batch 46 Loss: 7.71875, Speed: 25.872329401078783 samples/sec, TFLOPS/GPU: 108.95179442100661 , Grad norm: 8.1505702259395\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 46, Running Avg Speed: 25.098022476524196 samples/sec, Running Avg TFLOPS/GPU: 105.69108574823764\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(124s), Batch 47 Loss: 7.6875, Speed: 25.683326986702692 samples/sec, TFLOPS/GPU: 108.15588030454057 , Grad norm: 6.1329168883281495\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 47, Running Avg Speed: 25.130539393756337 samples/sec, Running Avg TFLOPS/GPU: 105.82801877914336\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Reading data from training path ['/opt/ml/input/data/train/training.json'].\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Compute tflops: (25.336437989896588, 8224867200, 16, 1024) ==> 106.6950849474224.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(127s), Batch 48 Loss: 7.65625, Speed: 25.336437989896588 samples/sec, TFLOPS/GPU: 106.6950849474224 , Grad norm: 6.980250128886861\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 48, Running Avg Speed: 25.141376161974243 samples/sec, Running Avg TFLOPS/GPU: 105.87365384063172\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(130s), Batch 49 Loss: 7.75, Speed: 25.295222501180493 samples/sec, TFLOPS/GPU: 106.52152108373063 , Grad norm: 11.419392214157993\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 49, Running Avg Speed: 25.149068478934556 samples/sec, Running Avg TFLOPS/GPU: 105.90604720278667\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(132s), Batch 50 Loss: 7.75, Speed: 25.473673342342206 samples/sec, TFLOPS/GPU: 107.27300113251573 , Grad norm: 3.5595384666278966\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 50, Running Avg Speed: 25.164525853382536 samples/sec, Running Avg TFLOPS/GPU: 105.97114024705947\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(135s), Batch 51 Loss: 7.6875, Speed: 25.22788854768489 samples/sec, TFLOPS/GPU: 106.23796891704785 , Grad norm: 6.506469295771261\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 51, Running Avg Speed: 25.167405975850826 samples/sec, Running Avg TFLOPS/GPU: 105.98326882296804\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(137s), Batch 52 Loss: 7.65625, Speed: 25.583870032807646 samples/sec, TFLOPS/GPU: 107.7370539427341 , Grad norm: 7.07016381804758\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 52, Running Avg Speed: 25.18551310876199 samples/sec, Running Avg TFLOPS/GPU: 106.0595203499144\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(140s), Batch 53 Loss: 7.46875, Speed: 25.678425557946515 samples/sec, TFLOPS/GPU: 108.13523974102834 , Grad norm: 3.5440534415123515\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 53, Running Avg Speed: 25.206051127478016 samples/sec, Running Avg TFLOPS/GPU: 106.14600865787749\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(142s), Batch 54 Loss: 7.59375, Speed: 25.39929831878665 samples/sec, TFLOPS/GPU: 106.95979809034417 , Grad norm: 8.594258256316005\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 54, Running Avg Speed: 25.213781015130362 samples/sec, Running Avg TFLOPS/GPU: 106.17856023517615\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(145s), Batch 55 Loss: 7.46875, Speed: 25.31711830784775 samples/sec, TFLOPS/GPU: 106.61372721599307 , Grad norm: 3.2044698291536244\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 55, Running Avg Speed: 25.21775552638872 samples/sec, Running Avg TFLOPS/GPU: 106.19529742674602\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(147s), Batch 56 Loss: 7.5, Speed: 25.27395228636916 samples/sec, TFLOPS/GPU: 106.43194940135562 , Grad norm: 5.944060443979657\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 56, Running Avg Speed: 25.21983688786948 samples/sec, Running Avg TFLOPS/GPU: 106.20406231469453\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(150s), Batch 57 Loss: 7.46875, Speed: 25.311902151302938 samples/sec, TFLOPS/GPU: 106.59176129221682 , Grad norm: 4.573545967107395\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 57, Running Avg Speed: 25.223124932992103 samples/sec, Running Avg TFLOPS/GPU: 106.2179087067489\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(152s), Batch 58 Loss: 7.40625, Speed: 25.53299808737067 samples/sec, TFLOPS/GPU: 107.52282546507662 , Grad norm: 3.0291949120854955\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 58, Running Avg Speed: 25.23381021417757 samples/sec, Running Avg TFLOPS/GPU: 106.2629058363464\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(155s), Batch 59 Loss: 7.375, Speed: 25.461580145246526 samples/sec, TFLOPS/GPU: 107.2220750831664 , Grad norm: 4.9060242847905995\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 59, Running Avg Speed: 25.2414025452132 samples/sec, Running Avg TFLOPS/GPU: 106.29487814457373\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Reading data from training path ['/opt/ml/input/data/train/training.json'].\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Compute tflops: (25.337978141612282, 8224867200, 16, 1024) ==> 106.70157072960707.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(157s), Batch 60 Loss: 7.34375, Speed: 25.337978141612282 samples/sec, TFLOPS/GPU: 106.70157072960707 , Grad norm: 3.4727051877488218\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 60, Running Avg Speed: 25.24451788703253 samples/sec, Running Avg TFLOPS/GPU: 106.30799726021998\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(160s), Batch 61 Loss: 7.375, Speed: 25.33078599426528 samples/sec, TFLOPS/GPU: 106.67128364771938 , Grad norm: 4.088112754621883\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 61, Running Avg Speed: 25.247213765383552 samples/sec, Running Avg TFLOPS/GPU: 106.31934995982934\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(162s), Batch 62 Loss: 7.46875, Speed: 25.372220672112466 samples/sec, TFLOPS/GPU: 106.84577054577653 , Grad norm: 1.8249597287388082\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 62, Running Avg Speed: 25.251001853466246 samples/sec, Running Avg TFLOPS/GPU: 106.33530209879744\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(165s), Batch 63 Loss: 7.40625, Speed: 25.612320379638184 samples/sec, TFLOPS/GPU: 107.85686210886536 , Grad norm: 4.2557393364208815\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 63, Running Avg Speed: 25.26162886894189 samples/sec, Running Avg TFLOPS/GPU: 106.38005386379942\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(167s), Batch 64 Loss: 7.34375, Speed: 25.48755177305934 samples/sec, TFLOPS/GPU: 107.33144503630726 , Grad norm: 2.4026453110681008\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 64, Running Avg Speed: 25.268083809059533 samples/sec, Running Avg TFLOPS/GPU: 106.40723646872821\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(170s), Batch 65 Loss: 7.21875, Speed: 25.32120918272537 samples/sec, TFLOPS/GPU: 106.63095443012439 , Grad norm: 2.9610062186248243\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 65, Running Avg Speed: 25.269559513883582 samples/sec, Running Avg TFLOPS/GPU: 106.41345085654478\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(172s), Batch 66 Loss: 7.3125, Speed: 25.818328187822573 samples/sec, TFLOPS/GPU: 108.72438818347895 , Grad norm: 1.9764261309470592\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 66, Running Avg Speed: 25.28439109966572 samples/sec, Running Avg TFLOPS/GPU: 106.4759086221376\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(175s), Batch 67 Loss: 7.21875, Speed: 25.7086722554254 samples/sec, TFLOPS/GPU: 108.26261257686957 , Grad norm: 2.630641485883212\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 67, Running Avg Speed: 25.295556393238343 samples/sec, Running Avg TFLOPS/GPU: 106.52292714726212\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(177s), Batch 68 Loss: 7.21875, Speed: 25.39716438943924 samples/sec, TFLOPS/GPU: 106.95081183216989 , Grad norm: 2.085308218780989\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 68, Running Avg Speed: 25.298161726474262 samples/sec, Running Avg TFLOPS/GPU: 106.53389854943924\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(180s), Batch 69 Loss: 7.1875, Speed: 25.52787953345308 samples/sec, TFLOPS/GPU: 107.50127055884776 , Grad norm: 1.8679769046169934\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 69, Running Avg Speed: 25.30390467164873 samples/sec, Running Avg TFLOPS/GPU: 106.55808284967445\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(182s), Batch 70 Loss: 7.15625, Speed: 25.277529351819652 samples/sec, TFLOPS/GPU: 106.44701289616319 , Grad norm: 2.828372422408848\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 70, Running Avg Speed: 25.303261371165096 samples/sec, Running Avg TFLOPS/GPU: 106.55537382641808\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(185s), Batch 71 Loss: 7.15625, Speed: 25.144079871178903 samples/sec, TFLOPS/GPU: 105.88503951699947 , Grad norm: 2.3900253972971615\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 71, Running Avg Speed: 25.29947133545114 samples/sec, Running Avg TFLOPS/GPU: 106.53941348571766\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Reading data from training path ['/opt/ml/input/data/train/training.json'].\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Compute tflops: (24.900347390029488, 8224867200, 16, 1024) ==> 104.85864986463187.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(188s), Batch 72 Loss: 7.0625, Speed: 24.900347390029488 samples/sec, TFLOPS/GPU: 104.85864986463187 , Grad norm: 1.8284267601200492\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 72, Running Avg Speed: 25.290189383232033 samples/sec, Running Avg TFLOPS/GPU: 106.50032595964589\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(190s), Batch 73 Loss: 7.15625, Speed: 25.38846897203996 samples/sec, TFLOPS/GPU: 106.91419428165078 , Grad norm: 5.089564882303588\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 73, Running Avg Speed: 25.292423010250392 samples/sec, Running Avg TFLOPS/GPU: 106.50973205787326\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(193s), Batch 74 Loss: 7.28125, Speed: 25.339975356258577 samples/sec, TFLOPS/GPU: 106.70998126412773 , Grad norm: 13.301580389226782\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 74, Running Avg Speed: 25.293479729050574 samples/sec, Running Avg TFLOPS/GPU: 106.51418204023447\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(195s), Batch 75 Loss: 7.25, Speed: 25.587884154812944 samples/sec, TFLOPS/GPU: 107.7539579403882 , Grad norm: 11.002508937168262\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 75, Running Avg Speed: 25.299879825262796 samples/sec, Running Avg TFLOPS/GPU: 106.5411336902378\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(198s), Batch 76 Loss: 7.125, Speed: 25.553789619563467 samples/sec, TFLOPS/GPU: 107.61038135175579 , Grad norm: 7.621041538617858\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 76, Running Avg Speed: 25.305282161311748 samples/sec, Running Avg TFLOPS/GPU: 106.56388364048287\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(200s), Batch 77 Loss: 7.15625, Speed: 25.65451770955295 samples/sec, TFLOPS/GPU: 108.03456063545404 , Grad norm: 28.119558595995212\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 77, Running Avg Speed: 25.312557901900107 samples/sec, Running Avg TFLOPS/GPU: 106.59452274454476\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(203s), Batch 78 Loss: 7.375, Speed: 24.964836779991188 samples/sec, TFLOPS/GPU: 105.13022319877281 , Grad norm: 14.26683807091229\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 78, Running Avg Speed: 25.305461552473393 samples/sec, Running Avg TFLOPS/GPU: 106.56463908034534\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(205s), Batch 79 Loss: 7.375, Speed: 25.48363922477313 samples/sec, TFLOPS/GPU: 107.31496877899261 , Grad norm: 14.099102227417703\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 79, Running Avg Speed: 25.30902510591939 samples/sec, Running Avg TFLOPS/GPU: 106.57964567431829\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(208s), Batch 80 Loss: 7.0625, Speed: 25.736685397443605 samples/sec, TFLOPS/GPU: 108.38057961582227 , Grad norm: 8.924152122216922\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 80, Running Avg Speed: 25.31741060183163 samples/sec, Running Avg TFLOPS/GPU: 106.61495810454386\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(210s), Batch 81 Loss: 7.8125, Speed: 25.410052721734957 samples/sec, TFLOPS/GPU: 107.00508630080951 , Grad norm: 50.42435537606943\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 81, Running Avg Speed: 25.31919218106054 samples/sec, Running Avg TFLOPS/GPU: 106.62246056985666\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(213s), Batch 82 Loss: 7.1875, Speed: 25.448687754939133 samples/sec, TFLOPS/GPU: 107.16778352570394 , Grad norm: 12.429890108871493\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 82, Running Avg Speed: 25.32163549377523 samples/sec, Running Avg TFLOPS/GPU: 106.63274968223114\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(215s), Batch 83 Loss: 7.625, Speed: 25.383002556309442 samples/sec, TFLOPS/GPU: 106.89117448340771 , Grad norm: 13.8795139004208\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 83, Running Avg Speed: 25.322771920859196 samples/sec, Running Avg TFLOPS/GPU: 106.63753532669737\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Reading data from training path ['/opt/ml/input/data/train/training.json'].\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Compute tflops: (25.424716739860415, 8224867200, 16, 1024) ==> 107.06683841688006.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(218s), Batch 84 Loss: 7.5625, Speed: 25.424716739860415 samples/sec, TFLOPS/GPU: 107.06683841688006 , Grad norm: 13.368800988928605\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 84, Running Avg Speed: 25.32462546302286 samples/sec, Running Avg TFLOPS/GPU: 106.64534083742798\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(220s), Batch 85 Loss: 7.3125, Speed: 25.789658924930396 samples/sec, TFLOPS/GPU: 108.60365813291381 , Grad norm: 12.222932187087094\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 85, Running Avg Speed: 25.33292963198549 samples/sec, Running Avg TFLOPS/GPU: 106.68031078913306\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(223s), Batch 86 Loss: 7.03125, Speed: 25.724208131570954 samples/sec, TFLOPS/GPU: 108.3280362021536 , Grad norm: 4.55653053875202\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 86, Running Avg Speed: 25.33979416706594 samples/sec, Running Avg TFLOPS/GPU: 106.7092182525194\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(225s), Batch 87 Loss: 7.78125, Speed: 25.445780866636692 samples/sec, TFLOPS/GPU: 107.1555422353345 , Grad norm: 46.00974940897903\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 87, Running Avg Speed: 25.341621523955087 samples/sec, Running Avg TFLOPS/GPU: 106.7169134936024\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(228s), Batch 88 Loss: 6.9375, Speed: 25.767960026924154 samples/sec, TFLOPS/GPU: 108.51228120901611 , Grad norm: 4.131817373261585\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 88, Running Avg Speed: 25.348847600276596 samples/sec, Running Avg TFLOPS/GPU: 106.7473434548806\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(230s), Batch 89 Loss: 7.125, Speed: 25.557794307115362 samples/sec, TFLOPS/GPU: 107.62724561968122 , Grad norm: 11.387073181363833\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 89, Running Avg Speed: 25.352330045390577 samples/sec, Running Avg TFLOPS/GPU: 106.76200849096062\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(233s), Batch 90 Loss: 7.25, Speed: 25.732219913168105 samples/sec, TFLOPS/GPU: 108.36177487206564 , Grad norm: 11.368998208795555\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 90, Running Avg Speed: 25.358557748141028 samples/sec, Running Avg TFLOPS/GPU: 106.78823416933939\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(235s), Batch 91 Loss: 7.125, Speed: 25.635794608007032 samples/sec, TFLOPS/GPU: 107.95571518327495 , Grad norm: 10.93430194523412\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 91, Running Avg Speed: 25.36302931039693 samples/sec, Running Avg TFLOPS/GPU: 106.80706450827383\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(238s), Batch 92 Loss: 6.9375, Speed: 25.488940931959213 samples/sec, TFLOPS/GPU: 107.33729496781248 , Grad norm: 7.795855615004909\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 92, Running Avg Speed: 25.365027907564585 samples/sec, Running Avg TFLOPS/GPU: 106.81548086477443\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(240s), Batch 93 Loss: 7.0625, Speed: 25.57601370632286 samples/sec, TFLOPS/GPU: 107.7039699148213 , Grad norm: 22.089967773724045\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 93, Running Avg Speed: 25.368324560670185 samples/sec, Running Avg TFLOPS/GPU: 106.82936350618144\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(243s), Batch 94 Loss: 6.9375, Speed: 25.590884594539585 samples/sec, TFLOPS/GPU: 107.76659318815433 , Grad norm: 15.218684017428565\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 94, Running Avg Speed: 25.371748561191254 samples/sec, Running Avg TFLOPS/GPU: 106.84378242436564\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(245s), Batch 95 Loss: 6.90625, Speed: 25.598447359798353 samples/sec, TFLOPS/GPU: 107.79844099099242 , Grad norm: 8.588547929033897\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 95, Running Avg Speed: 25.37518339147318 samples/sec, Running Avg TFLOPS/GPU: 106.8582469481024\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Reading data from training path ['/opt/ml/input/data/train/training.json'].\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Compute tflops: (25.346134021226945, 8224867200, 16, 1024) ==> 106.73591621529272.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(248s), Batch 96 Loss: 6.90625, Speed: 25.346134021226945 samples/sec, TFLOPS/GPU: 106.73591621529272 , Grad norm: 10.070073837625504\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 96, Running Avg Speed: 25.37474981878294 samples/sec, Running Avg TFLOPS/GPU: 106.85642111626943\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(250s), Batch 97 Loss: 6.90625, Speed: 25.437625826886084 samples/sec, TFLOPS/GPU: 107.12120028642725 , Grad norm: 9.530969120066272\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 97, Running Avg Speed: 25.375674465960927 samples/sec, Running Avg TFLOPS/GPU: 106.86031492759528\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(253s), Batch 98 Loss: 6.90625, Speed: 25.361802574053012 samples/sec, TFLOPS/GPU: 106.80189855959254 , Grad norm: 5.6175912489475675\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 98, Running Avg Speed: 25.375473424049215 samples/sec, Running Avg TFLOPS/GPU: 106.85946831356625\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:(255s), Batch 99 Loss: 6.90625, Speed: 25.722191793213227 samples/sec, TFLOPS/GPU: 108.31954513515963 , Grad norm: 14.056315278243474\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:Batch 99, Running Avg Speed: 25.3804265436087 samples/sec, Running Avg TFLOPS/GPU: 106.88032655387471\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:root:SMP training finished successfully\u001b[0m\n",
      "\u001b[35m2023-08-29 08:54:33,786 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-08-29 08:54:33,786 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-08-29 08:54:33,786 sagemaker-training-toolkit INFO     Begin writing status file from leader node to worker nodes (if any)\u001b[0m\n",
      "\u001b[35m2023-08-29 08:54:33,786 sagemaker-training-toolkit INFO     Start writing mpirun finished status to algo-1\u001b[0m\n",
      "\u001b[34m2023-08-29 08:54:33,819 sagemaker-training-toolkit INFO     Invoked on_terminate from psutil.wait_for_procs\u001b[0m\n",
      "\u001b[34m2023-08-29 08:54:33,819 sagemaker-training-toolkit INFO     process psutil.Process(pid=81, name='orted', status='terminated', started='08:49:34') terminated with exit code None\u001b[0m\n",
      "\u001b[34m2023-08-29 08:54:33,819 sagemaker-training-toolkit INFO     Reporting status for ORTEd process. gone: [psutil.Process(pid=81, name='orted', status='terminated', started='08:49:34')] alive: []\u001b[0m\n",
      "\u001b[34m2023-08-29 08:54:33,819 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[35m2023-08-29 08:54:33,923 sagemaker-training-toolkit INFO     output from subprocess run CompletedProcess(args=['ssh', 'algo-1', 'touch', '/tmp/done.algo-2'], returncode=0, stdout='', stderr='')\u001b[0m\n",
      "\u001b[35m2023-08-29 08:54:33,923 sagemaker-training-toolkit INFO     Finished writing status file\u001b[0m\n",
      "\u001b[34m2023-08-29 08:55:03,831 sagemaker-training-toolkit INFO     Begin looking for status file on algo-1\u001b[0m\n",
      "\u001b[34m2023-08-29 08:55:03,831 sagemaker-training-toolkit INFO     MPI training job status file found. Exit gracefully\u001b[0m\n",
      "\u001b[34m2023-08-29 08:55:03,831 sagemaker-training-toolkit INFO     End looking for status file\u001b[0m\n",
      "\u001b[34m2023-08-29 08:55:03,831 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[34m2023-08-29 08:55:03,831 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m2023-08-29 08:55:03,928 sagemaker-training-toolkit INFO     Finished writing status file from leader node to worker nodes (if any)\u001b[0m\n",
      "\u001b[35m2023-08-29 08:55:03,928 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-08-29 08:55:29 Uploading - Uploading generated training model\n",
      "2023-08-29 08:55:29 Completed - Training job completed\n",
      "Training seconds: 1380\n",
      "Billable seconds: 1380\n"
     ]
    }
   ],
   "source": [
    "smp_estimator.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Training Logs\n",
    "\n",
    "You can access the training logs from [Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html). Make sure to look at the logs of **algo-1** because that is the main node whose output stream has the training job logs.\n",
    "\n",
    "You can use CloudWatch to track SageMaker GPU and memory utilization during training and inference. To view the metrics and logs that SageMaker writes to CloudWatch, see [SageMaker Jobs and Endpoint Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-jobs) in the Amazon SageMaker Developer Guide.\n",
    "\n",
    "If you are a new user of CloudWatch, see [Getting Started with Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/GettingStarted.html). \n",
    "\n",
    "For additional information on monitoring and analyzing Amazon SageMaker training jobs, see [Monitor and Analyze Training Jobs Using Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html).\n",
    "\n",
    "## Deploying Trained Model for Inference\n",
    "\n",
    "In most cases, a trained model can be deployed on a single device for inference because inference only requires a small amount of memory. You can use the SMP API to create a single, unified model after training: the [smp.DistributedModel.save_model()](https://sagemaker.readthedocs.io/en/stable/api/training/smp_versions/latest/smd_model_parallel_tensorflow.html#smp.DistributedModel.save_model) method for TensorFlow, and the [smp.save()](https://sagemaker.readthedocs.io/en/stable/api/training/smp_versions/latest/smd_model_parallel_pytorch.html#apis-for-saving-and-loading) function for PyTorch.\n",
    "\n",
    "After you build and train your models, you can deploy them to get predictions in one of two ways:\n",
    "\n",
    "* To set up a persistent endpoint to get predictions from your models, use SageMaker hosting services. For an overview on deploying a single model or multiple models with SageMaker hosting services, see [Deploy a Model on SageMaker Hosting Services](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html#how-it-works-hosting).\n",
    "* To get predictions for an entire dataset, use SageMaker batch transform. For an overview on deploying a model with SageMaker Batch Transform, see [Get Inferences for an Entire Dataset with Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html).\n",
    "\n",
    "To learn more about deploying models for inference using SageMaker, see [Deploy Models for Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "hide_input": false,
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
