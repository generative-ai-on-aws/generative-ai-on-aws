{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f9f42b-7575-4f3d-9ba2-ca2c5165c916",
   "metadata": {},
   "source": [
    "# Tested on ml.p4de.24xlarge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53443304-4173-4ac8-a57c-98e364f5df4a",
   "metadata": {},
   "source": [
    "Derived from this example: https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/rl_training.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c027e413-b079-4f80-9fa8-8e47801b7c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --disable-pip-version-check -q \\\n",
    "    torch==2.0.1 \\\n",
    "    transformers==4.34.1 \\\n",
    "    datasets==2.12.0 \\\n",
    "    accelerate==0.23.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    trl==0.7.2 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    typing_extensions==4.7.1 \\\n",
    "    bitsandbytes==0.41.1 \\\n",
    "    peft==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052c8fdf-e2fe-4191-a939-80de0f4000db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r peft_ranking_reward_public_qanda_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4854a87-583b-4907-b48f-26bb278af632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./peft_ranking_reward_public_qanda/\n"
     ]
    }
   ],
   "source": [
    "print(peft_ranking_reward_public_qanda_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "733ae426-3c17-45df-b3e5-6d1f41d2176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import Adafactor, AutoTokenizer, HfArgumentParser, pipeline\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9537b3de-a994-438e-bccf-c46debee4dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_fine_tuned_with_ranking_rewards_llama2_checkpoint = './peft_fine_tuned_with_ranking_rewards_llama2'\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    model_name: Optional[str] = field(default=\"NousResearch/Llama-2-7b-hf\", metadata={\"help\": \"the model name\"})\n",
    "    tokenizer_name: Optional[str] = field(default=\"NousResearch/Llama-2-7b-hf\", metadata={\"help\": \"the tokenizer name\"})\n",
    "    reward_model_name: Optional[str] = field(default=peft_ranking_reward_public_qanda_checkpoint, metadata={\"help\": \"the reward model name\"})\n",
    "    log_with: Optional[str] = field(default=None, metadata={\"help\": \"use 'wandb' to log with wandb\"})\n",
    "    learning_rate: Optional[float] = field(default=1.41e-5, metadata={\"help\": \"the learning rate\"})\n",
    "    output_max_length: Optional[int] = field(default=128, metadata={\"help\": \"maximum length for generation\"})\n",
    "    mini_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"the PPO minibatch size\"})\n",
    "    batch_size: Optional[int] = field(default=1, metadata={\"help\": \"the batch size\"})\n",
    "    ppo_epochs: Optional[int] = field(default=4, metadata={\"help\": \"the number of ppo epochs\"})\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=1, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
    "    )\n",
    "    adafactor: Optional[bool] = field(default=False, metadata={\"help\": \"whether to use the adafactor optimizer\"})\n",
    "    early_stopping: Optional[bool] = field(default=False, metadata={\"help\": \"whether to early stop\"})\n",
    "    target_kl: Optional[float] = field(default=0.1, metadata={\"help\": \"kl target for early stopping\"})\n",
    "    reward_baseline: Optional[float] = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"a baseline value that is subtracted from the reward\"},\n",
    "    )\n",
    "    batched_gen: Optional[bool] = field(default=False, metadata={\"help\": \"whether to use the batched text gen\"})\n",
    "    save_freq: Optional[int] = field(default=None, metadata={\"help\": \"n steps to save the model\"})\n",
    "#    output_dir: Optional[str] = field(default=\"runs/\", metadata={\"help\": \"n steps to save the model\"})\n",
    "    seed: Optional[int] = field(default=0, metadata={\"help\": \"the seed\"})\n",
    "    steps: Optional[int] = field(default=100, metadata={\"help\": \"number of epochs\"})\n",
    "    init_kl_coef: Optional[float] = field(\n",
    "        default=0.2,\n",
    "        metadata={\"help\": \"Initial KL penalty coefficient (used for adaptive and linear control)\"},\n",
    "    )\n",
    "\n",
    "    adap_kl_ctrl: Optional[bool] = field(default=True, metadata={\"help\": \"Use adaptive KL control, otherwise linear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ffcb28-f12b-4ffa-8936-85e58f0d3212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(ScriptArguments)\n",
    "script_args: ScriptArguments = parser.parse_args_into_dataclasses(return_remaining_strings=True)[0]\n",
    "\n",
    "dataset_name = \"lvwerra/stack-exchange-paired\"\n",
    "config = PPOConfig(\n",
    "    steps=script_args.steps,\n",
    "    model_name=script_args.model_name,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    log_with=script_args.log_with,\n",
    "    batch_size=script_args.batch_size,\n",
    "    mini_batch_size=script_args.mini_batch_size,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    optimize_cuda_cache=True,\n",
    "    early_stopping=script_args.early_stopping,\n",
    "    target_kl=script_args.target_kl,\n",
    "    ppo_epochs=script_args.ppo_epochs,\n",
    "    seed=script_args.seed,\n",
    "    init_kl_coef=script_args.init_kl_coef,\n",
    "    adap_kl_ctrl=script_args.adap_kl_ctrl,\n",
    ")\n",
    "\n",
    "# We then define the arguments to pass to the sentiment analysis pipeline.\n",
    "# We set `return_all_scores` to True to get the sentiment score for each token.\n",
    "sent_kwargs = {\n",
    "    \"return_all_scores\": True,\n",
    "    \"function_to_apply\": \"none\",\n",
    "    \"batch_size\": 16,\n",
    "    \"truncation\": True,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.tokenizer_name)\n",
    "\n",
    "if getattr(tokenizer, \"pad_token\", None) is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bdf44c7-7f48-436d-b37d-101211794e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Below is an example function to build the dataset. In our case, we use the IMDB dataset\n",
    "# from the `datasets` library. One should customize this function to train the model on\n",
    "# its own dataset.\n",
    "def build_dataset(\n",
    "    tokenizer,\n",
    "    dataset_name=\"lvwerra/stack-exchange-paired\",\n",
    "    data_dir=\"data/rl\",\n",
    "    split=\"train\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    ds = load_dataset(dataset_name, data_dir=data_dir, split=split)\n",
    "    original_columns = ds.column_names\n",
    "    num_proc = 24\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        new_examples = {\n",
    "            \"query\": [],\n",
    "            \"input_ids\": [],\n",
    "        }\n",
    "        for question in examples[\"question\"]:\n",
    "            query = \"Question: \" + question + \"\\n\\nAnswer: \"\n",
    "            tokenized_question = tokenizer(query, truncation=True)\n",
    "            new_examples[\"query\"].append(query)\n",
    "            new_examples[\"input_ids\"].append(tokenized_question[\"input_ids\"])\n",
    "\n",
    "        return new_examples\n",
    "\n",
    "    ds = ds.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=original_columns,\n",
    "    )\n",
    "    ds = ds.filter(lambda x: len(x[\"input_ids\"]) < 512, batched=False)\n",
    "\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cf63b9d-589b-4bd5-9f2d-4e385fd31c76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62cea29c-a5be-44e0-ad0f-bb1b56a67bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/lvwerra___parquet/lvwerra--stack-exchange-paired-e5ccc5f74f1da5b7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/7435908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7435908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We retrieve the dataloader by calling the `build_dataset` function.\n",
    "train_dataset = build_dataset(tokenizer, \"lvwerra/stack-exchange-paired\", data_dir=\"data/rl\", split=\"train\")\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a36d960-3d7f-4b84-8d53-1edf906d316e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set seed before initializing value head for deterministic eval\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Now let's build the model, the reference model, and the tokenizer.\n",
    "current_device = Accelerator().local_process_index\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd0c12ff-05ae-40d1-9e1d-03d8348fe245",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c107636acaac4e5483d8fd756091b1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 0\n",
      "all model parameters: 6746808321\n",
      "percentage of trainable model parameters: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from trl import create_reference_model\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    config.model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map={\"\": current_device},\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "\n",
    "ref_model = create_reference_model(model)\n",
    "print(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "530cb8dc-aadc-41cf-aa90-fbb6b7d0b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "optimizer = None\n",
    "if script_args.adafactor:\n",
    "    optimizer = Adafactor(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        scale_parameter=False,\n",
    "        relative_step=False,\n",
    "        warmup_init=False,\n",
    "        lr=config.learning_rate,\n",
    "    )\n",
    "    \n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=train_dataset,\n",
    "    data_collator=collator,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "\n",
    "# We then build the sentiment analysis pipeline using our reward model, passing the\n",
    "# model name and the sentiment analysis pipeline arguments. Let's also make sure to\n",
    "# set the device to the same device as the PPOTrainer.\n",
    "\n",
    "reward_model_tokenizer = AutoTokenizer.from_pretrained(script_args.reward_model_name)\n",
    "\n",
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a ` pipeline` bug\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=script_args.reward_model_name,\n",
    "    device_map={\"\": current_device},\n",
    "    model_kwargs={\"load_in_8bit\": True},\n",
    "    tokenizer=reward_model_tokenizer,\n",
    "    return_token_type_ids=False,\n",
    ")\n",
    "\n",
    "# We then define the arguments to pass to the `generate` function. These arguments\n",
    "# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
    "# the `generate` function of the trained model.\n",
    "generation_kwargs = {\n",
    "    # \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": 100_000,\n",
    "}\n",
    "output_min_length = 32\n",
    "output_max_length = script_args.output_max_length\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb3a5820-79fd-4388-b848-c1f5e3a32bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "1it [00:24, 24.47s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "6it [02:50, 27.44s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -2.13 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "10it [04:49, 29.62s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "16it [07:57, 30.24s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.57 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "19it [09:23, 29.34s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "20it [10:00, 31.64s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "32it [15:53, 31.17s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "50it [23:53, 27.66s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.08 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "60it [27:34, 21.62s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.56 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "68it [30:54, 24.92s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.98 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "71it [32:07, 23.59s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "88it [39:55, 28.02s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -2.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "97it [44:03, 28.99s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.15 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "100it [45:31, 27.31s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    if epoch >= config.total_ppo_epochs:\n",
    "        break\n",
    "\n",
    "    question_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        question_tensors,\n",
    "        return_prompt=False,\n",
    "        length_sampler=output_length_sampler,\n",
    "        **generation_kwargs,\n",
    "    )\n",
    "    batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "\n",
    "    # Compute reward score (using the sentiment analysis pipeline)\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[0][\"score\"] - script_args.reward_baseline) for output in pipe_outputs]\n",
    "\n",
    "    # Run PPO step\n",
    "    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    if script_args.save_freq and epoch and epoch % script_args.save_freq == 0:\n",
    "        #ppo_trainer.save_pretrained(script_args.output_dir + f\"step_{epoch}\")\n",
    "        ppo_trainer.tokenizer.save_pretrained(peft_fine_tuned_with_ranking_rewards_llama2_checkpoint)\n",
    "        ppo_trainer.accelerator.unwrap_model(ppo_trainer.model).save_pretrained(peft_fine_tuned_with_ranking_rewards_llama2_checkpoint) # merge\n",
    "        #ppo_trainer.model.save_pretrained(peft_fine_tuned_with_detoxification_rewards_checkpoint)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19af42bc-f1b4-4079-94ae-43e2369d4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer.tokenizer.save_pretrained(peft_fine_tuned_with_ranking_rewards_llama2_checkpoint)\n",
    "ppo_trainer.accelerator.unwrap_model(ppo_trainer.model).save_pretrained(peft_fine_tuned_with_ranking_rewards_llama2_checkpoint) # merge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef44c530-a091-4eb9-ac05-1f58a74a8f63",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44d9784f-2b92-466a-abcb-de36c3cd4aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType, AutoPeftModelForCausalLM\n",
    "\n",
    "import torch\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ed096ec-fa04-4a89-a729-d6765886f0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34c79b14dba4fe8997c4df813afc9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    peft_fine_tuned_with_ranking_rewards_llama2_checkpoint,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46f46c0e-85fb-4860-9114-10fd906d913f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984df35b5218415e88d57be5300026df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23a69baa-2b18-465b-9de3-8aa601f3eec4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'nothate', 1: 'hate'}\n"
     ]
    }
   ],
   "source": [
    "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    toxicity_model_name, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    toxicity_model_name, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(toxicity_model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd8c0b-21be-47cb-b529-27f67fec9b5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Take some non-toxic text, tokenize it, and pass it to the model. Print the output logits, probabilities, and the corresponding reward that will be used for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b9619ff-6d77-41f5-8be4-57d938a0fd20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate, hate]: [4.6532111167907715, -4.178227424621582]\n",
      "probabilities [not hate, hate]: [0.9998539686203003, 0.0001460467028664425]\n",
      "reward (value of \"not hate\" logit): [4.6532111167907715]\n"
     ]
    }
   ],
   "source": [
    "non_toxic_text = \"You are a great person and i like you.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# get the logits for \"not hate\" - this is the reward!\n",
    "not_hate_index = 0\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
    "print(f'reward (value of \"not hate\" logit): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f515b-b314-4eaf-8d2a-31895e0d167a",
   "metadata": {},
   "source": [
    "Let's show a toxic comment.  This will have a low reward because it is more toxic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5038a63b-8129-482b-acab-72ec34245afb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate, hate]: [-2.064443349838257, 1.6650441884994507]\n",
      "probabilities [not hate, hate]: [0.023442398756742477, 0.9765575528144836]\n",
      "reward (value of \"not hate\" logit): [-2.064443349838257]\n"
     ]
    }
   ],
   "source": [
    "toxic_text = \"You are a terrible person and i hate you.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "logits = toxicity_model(toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# Get the logits for \"not hate\" - this is the reward!\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist() \n",
    "print(f'reward (value of \"not hate\" logit): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229e83a-cddb-4e44-9953-a3f57e1a1221",
   "metadata": {},
   "source": [
    "Setup Hugging Face inference pipeline to simplify the code for the toxicity reward model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf46e7ad-011b-4fab-ad2b-be01c5831219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model output for non-toxic text:\n",
      "[{'label': 'nothate', 'score': 4.6532111167907715}, {'label': 'hate', 'score': -4.178227424621582}]\n",
      "[{'label': 'nothate', 'score': 0.9998539686203003}, {'label': 'hate', 'score': 0.0001460467028664425}]\n",
      "\n",
      "Reward model output for toxic text:\n",
      "[{'label': 'hate', 'score': 1.6650441884994507}, {'label': 'nothate', 'score': -2.064443349838257}]\n",
      "[{'label': 'hate', 'score': 0.9765575528144836}, {'label': 'nothate', 'score': 0.023442398756742477}]\n"
     ]
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", \n",
    "                          model=toxicity_model_name, \n",
    "                          device=device)\n",
    "reward_logits_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "reward_probabilities_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"softmax\", # Set to \"softmax\" to apply softmax and retrieve probabilities.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "print(\"Reward model output for non-toxic text:\")\n",
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\n",
    "print(\"\\nReward model output for toxic text:\")\n",
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98528a1-3f1d-45d8-8511-20dff9e86ae8",
   "metadata": {},
   "source": [
    "The outputs are the logits for both `nothate` (positive) and `hate` (negative) classes. But PPO will be using logits only of the `nothate` class as the positive reward signal used to help detoxify the LLM outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fcfb315-775f-40f2-8a2d-3dfc2ce28f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'nothate', 'score': 4.6532111167907715}, {'label': 'hate', 'score': -4.178227424621582}]\n",
      "[{'label': 'nothate', 'score': 0.9998539686203003}, {'label': 'hate', 'score': 0.0001460467028664425}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98eccf2b-9804-49a9-b5f2-d58f1c51d662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'hate', 'score': 1.6650441884994507}, {'label': 'nothate', 'score': -2.064443349838257}]\n",
      "[{'label': 'hate', 'score': 0.9765575528144836}, {'label': 'nothate', 'score': 0.023442398756742477}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdfc6dc-edb4-448f-88ff-6bba3fe9180f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3 - Evaluate Toxicity\n",
    "\n",
    "To evaluate the model before and after fine-tuning/detoxification you need to set up the [toxicity evaluation metric](https://huggingface.co/spaces/evaluate-measurement/toxicity). The **toxicity score** is a decimal value between 0 and 1 where 1 is the highest toxicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b75b272b-0758-4e4c-8a3b-213d4971b668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toxicity_evaluator = evaluate.load(\"toxicity\", \n",
    "                                    toxicity_model_name,\n",
    "                                    module_type=\"measurement\",\n",
    "                                    toxic_label=\"hate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2b36bb-77d7-4103-a0d3-e0235c0feedb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Try to calculate toxicity for the same sentences as in section [2.2](#2.2). It's no surprise that the toxicity scores are the probabilities of `hate` class returned directly from the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "967655d2-ac62-437d-a056-c9bd8a02bdd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity score for non-toxic text:\n",
      "[0.00014604683383367956]\n",
      "\n",
      "Toxicity score for toxic text:\n",
      "[0.9765576720237732]\n"
     ]
    }
   ],
   "source": [
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    non_toxic_text\n",
    "])\n",
    "\n",
    "print(\"Toxicity score for non-toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])\n",
    "\n",
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    toxic_text\n",
    "])\n",
    "\n",
    "print(\"\\nToxicity score for toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf2bd0-9622-4db8-8e2e-9c37ae7ac4a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "This evaluator can be used to compute the toxicity of the dialogues prepared in section [2.1](#2.1). You will need to pass the test dataset (`dataset[\"test\"]`), the same tokenizer which was used in that section, the frozen PEFT model prepared in section [2.2](#2.2), and the toxicity evaluator. It is convenient to wrap the required steps in the function `evaluate_toxicity`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3de48105-df1e-4bb7-8e04-8a37c67b3aff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/lvwerra___parquet/lvwerra--stack-exchange-paired-6fbcbcc16115b7c8/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/4483004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4483004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = build_dataset(tokenizer, \"lvwerra/stack-exchange-paired\", data_dir=\"data/evaluation\", split=\"train\")\n",
    "test_dataset = test_dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d90b06ca-af87-4a14-9e02-3f72f22446b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_toxicity(model, \n",
    "                      toxicity_evaluator, \n",
    "                      tokenizer, \n",
    "                      dataset, \n",
    "                      num_samples):\n",
    "\n",
    "    max_new_tokens=100\n",
    "\n",
    "    toxicities = []\n",
    "    input_texts = []\n",
    "    for i, sample in tqdm(enumerate(dataset)):\n",
    "        input_text = sample[\"query\"]\n",
    "\n",
    "        if i > num_samples:\n",
    "            break\n",
    "            \n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "        \n",
    "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n",
    "                                             tok_k=0.0,\n",
    "                                             top_p=1.0,\n",
    "                                             do_sample=True)\n",
    "\n",
    "        response_token_ids = model.generate(input_ids=input_ids,\n",
    "                                            generation_config=generation_config)\n",
    "        \n",
    "        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n",
    "\n",
    "        toxicities.extend(toxicity_score[\"toxicity\"])\n",
    "\n",
    "    # Compute mean & std using np.\n",
    "    mean = np.mean(toxicities)\n",
    "    std = np.std(toxicities)\n",
    "        \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6110249-18ba-4c32-97f6-347b4ad3f9af",
   "metadata": {
    "tags": []
   },
   "source": [
    "And now perform the calculation of the model toxicity before fine-tuning/detoxification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a90da8d8-e953-48ae-88cb-74b942d4a856",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [07:13,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity [mean, std] before detox: [0.01928018110269441, 0.01788940742928178]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(script_args.model_name, device_map=\"auto\")\n",
    "\n",
    "mean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=ref_model, \n",
    "                                                                          toxicity_evaluator=toxicity_evaluator, \n",
    "                                                                          tokenizer=tokenizer, \n",
    "                                                                          dataset=test_dataset, \n",
    "                                                                          num_samples=100)\n",
    "\n",
    "print(f'toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5efbec56-5339-4588-ad72-2a6b81bf24a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [08:40,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity [mean, std] after detox: [0.018666810890899437, 0.019985045280750742]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_after_detoxification, std_after_detoxification = evaluate_toxicity(model=ppo_model, \n",
    "                                                                        toxicity_evaluator=toxicity_evaluator, \n",
    "                                                                        tokenizer=tokenizer, \n",
    "                                                                        dataset=test_dataset, \n",
    "                                                                        num_samples=100)\n",
    "print(f'toxicity [mean, std] after detox: [{mean_after_detoxification}, {std_after_detoxification}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f4b42-5a32-41bf-ac07-84b842c53f77",
   "metadata": {
    "tags": []
   },
   "source": [
    "And compare the toxicity scores of the reference model (before detoxification) and fine-tuned model (after detoxification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a0239e8-9e37-4405-8083-a0dd1b2f05d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage improvement of toxicity score after detoxification:\n",
      "mean: 3.18%\n",
      "std: -11.71%\n"
     ]
    }
   ],
   "source": [
    "mean_improvement = (mean_before_detoxification - mean_after_detoxification) / mean_before_detoxification\n",
    "std_improvement = (std_before_detoxification - std_after_detoxification) / std_before_detoxification\n",
    "\n",
    "print(f'Percentage improvement of toxicity score after detoxification:')\n",
    "print(f'mean: {mean_improvement*100:.2f}%')\n",
    "print(f'std: {std_improvement*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d92d5fe-7bd5-44f7-a671-dbb17284f66f",
   "metadata": {},
   "source": [
    "<a name='3.4'></a>\n",
    "### 3.4 - Evaluate the Model Qualitatively\n",
    "\n",
    "Let's inspect some examples from the test dataset. You can compare the original `ref_model` to the fine-tuned/detoxified `ppo_model` using the toxicity evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c18b1-449e-4e85-9548-8628343b29a1",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSAyLTMgbWludXRlcyB0byBydW4uPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f49d852-2d35-4c1f-8299-626e6dbcadcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [12:12<00:00,  7.32s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "compare_results = {}\n",
    "\n",
    "df_batch = test_dataset[0:batch_size]\n",
    "\n",
    "compare_results[\"query\"] = df_batch[\"query\"]\n",
    "prompt_tensors = df_batch[\"input_ids\"]\n",
    "\n",
    "summary_tensors_ref = []\n",
    "summary_tensors = []\n",
    "\n",
    "# Get response from ppo and base model.\n",
    "for i in tqdm(range(batch_size)):\n",
    "    gen_len = output_length_sampler()\n",
    "    generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "    \n",
    "    summary = ref_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors_ref.append(summary)\n",
    "\n",
    "    summary = ppo_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors.append(summary)\n",
    "\n",
    "# Decode responses.\n",
    "compare_results[\"response_before\"] = [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\n",
    "compare_results[\"response_after\"] = [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n",
    "\n",
    "# Sentiment analysis of query/response pairs before/after.\n",
    "texts_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\n",
    "rewards_before = sentiment_pipe(texts_before, **sent_kwargs)\n",
    "compare_results[\"reward_before\"] = [reward[not_hate_index][\"score\"] for reward in rewards_before]\n",
    "\n",
    "texts_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\n",
    "rewards_after = sentiment_pipe(texts_after, **sent_kwargs)\n",
    "compare_results[\"reward_after\"] = [reward[not_hate_index][\"score\"] for reward in rewards_after]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78cfaac-e7da-4179-86c5-5ac1c85744a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e27ad2e-51d5-41e6-878d-da9652c1aad8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Store and review the results in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "11001c60-b877-4ba7-b530-9aa7fdeb46f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response_before</th>\n",
       "      <th>response_after</th>\n",
       "      <th>reward_before</th>\n",
       "      <th>reward_after</th>\n",
       "      <th>reward_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question: I was studying for icing and a tailplane stall. I have looked up some internet pages and instrument flying handbook, and found the procedure below.\\n\\n1. raise flaps to the previous setting. (To reduce down wash from the main wing so that reducing negative angle of attack of the tail and break the stall)\\n2. apply nose up elevator pressure (I don't get it. The nose up pressure will make the elevator to go up and wouldn't this increase the negative angle of attack and worsen the stall?)\\n3. do not increase airspeed unless it is necessary to avoid a wing stall. (Why shouldn't we increase airspeed?)\\n\\nSo now I'm trying to understand the reason why should a pilot do such actions. Can you help me out?\\n\\nAnswer:</td>\n",
       "      <td>1) I don't know what it is, but assume it is wrong. There is no verifiable source given, so the world's experts are looking at you.\\n\\n2)</td>\n",
       "      <td>605.3. Pilot Actions at Stall Entry\\n\\n(a) When the aircraft has a natural or induced stall, the pilot applies up elevator, neutralize ailer</td>\n",
       "      <td>2.287549</td>\n",
       "      <td>3.497573</td>\n",
       "      <td>1.210024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question: In a C# enumeration, are there any negative side effects of using a negative number?\\n\\nI am modelling response codes and one of the codes in negative. This compiles but I want to know if there are any negative side effects to this.\\n\\n```\\npublic enum ResponseCodes\\n{\\n    InvalidServerUserPasswordCombo = -1,\\n\\n    // etc.\\n}\\n\\n```\\n\\nAnswer:</td>\n",
       "      <td>1) Positive number is always useful.\\nPositive numbers increase, negative numbers decrease.\\n\\n2) Negative number will normally not directly used in the application, because it will bring a small trouble for the people who use the number.\\n\\n3) If you use it, you must also declare it.\\n\\n\\begin{code</td>\n",
       "      <td>0 would be a more appropriate value, but AFAIK, there's no difference between any negative-numbered enumeration.&lt;/s&gt;&lt;s&gt; Tags: c#, .net, caching, ninject\\n\\nQuestion: Ninject .NET in C# .NET cache dependencies\\n\\nI would like to store some data in cache (the data are fetch</td>\n",
       "      <td>2.192516</td>\n",
       "      <td>3.255872</td>\n",
       "      <td>1.063356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: Isn't that nicely recursive? \\n\\nI've got a portable command prompt on my external drive, and it has a nice .bat file to configure some initial settings, but I'd like more! \\n\\nHere's what I know how to set from .bat:\\n\\n* Colors = (color XY) where x and y are hex digits for the predefined colors\\n* Prompt = (prompt $p$g) sets the prompt to \"C:\\etc\\etc &gt;\" the default prompt\\n* Title = (title \"text\") sets the window title to \"text\"\\n* Screen Size = (mode con: cols=XX lines=YY) sets the columns and lines size of the window\\n* Path = (SET PATH=%~d0\\bin;%PATH%) sets up local path to my tools and appends the computer's path\\n\\nSo that's all great. But there are a few settings I can't seem to set from the bat. Like, how would I set these up wihtout using the Properties dialogue:\\n\\n* Buffer = not screen size, but the buffer\\n* Options like quick edit mode and autocomplete\\n* Popup colors\\n* Font. And can you use a font on the portable drive, or must it be installed to work?\\n* ...</td>\n",
       "      <td>6000A.COM is a notable fixed-document JScript notepad,[2] started by Jamie Zawinski in 1997. It is a rewrite of some bits of the original and continues 6000B   . 6000A.COM is notable because it replaced Notepad in Windows as the default platform documentation\\n\\nComment: Oh aprend esto en la Hamilton High School en Phoenix!\\n\\nComment: To be fair, John Siassor's hoax was pretty good.</td>\n",
       "      <td>1. OS environment variables, such as path, color, prompt, title and window size can be set as normally as we did in Windows commandline.\\n2. Options can be set by command prompt by `set` command, but it will are run command prompt loader, so you must gained administrator privileges . For example, `set transient` will set Windows command prompt(default) \\ Transient mode.\\n\\n3. Font can be set by prompt too, but the limitation that we can not simploy change the output like colorful prompt etc. More information of prompt font,</td>\n",
       "      <td>1.826687</td>\n",
       "      <td>2.877825</td>\n",
       "      <td>1.051139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question: ![http://farm4.static.flickr.com/3338/4564960137_0d3c09192d_o.png](https://i.stack.imgur.com/Y2gpf.png)\\n\\ncan we change text in this lightbox\\n\\nwant to change \"This site requires that you Connect with Facebook.\" and \"Connect with Facebook to Continue\"\\n\\nAnswer:</td>\n",
       "      <td>1-Get a HTML file or something to have the option\\n\\n2-When the lightbox appear and if onClick is run, just change the text by looads of strings...\\n\\nComment: i want to change the text for whole lightbox\\njust like it is mentioned in the image\\n\\nComment: there must be a \"This</td>\n",
       "      <td>1) File : \\strong{wwwvars.php} (located here 'C:\\xampp\\htdocs\\YOURSITENAME\\img\\wwwvars.php')\\nmay be you will find this string is at line 58 as follow;\\n\\n\\begin{code}\\n$exInstance = ES(\"facebook</td>\n",
       "      <td>1.869028</td>\n",
       "      <td>2.913131</td>\n",
       "      <td>1.044103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Question: I have an array of objects:\\n\\n```\\nthis.array = [{name: null}, {name: null}, {name: null}]\\n\\n```\\n\\nand array of reservend names:\\n\\n```\\nthis.reserved = [\"name2\", \"name3\"]\\n\\n```\\n\\nI loop through array and try to set uniques name (not included inside `reserved` array)\\n\\n```\\n  for (let i = 0; i &lt; array.length; i++) {\\n    this.setDefaultName(array[i], 1);\\n  }\\n\\n  private setDefaultName(obj, index){\\n    if (!this.reserved.includes(`name${index}`)) {\\n      obj.name = `name${index}`;\\n      this.reserved.push(`name${index}`);\\n    } else {\\n      return this.setDefaultName(obj, index + 1);\\n    }\\n  }\\n\\n```\\n\\nAfter that all objects from array have name \"name3\". The expected result is to have sequence unique name: \"name1\", \"name4\", \"name5\".\\nCould anyone help me?\\n\\nAnswer:</td>\n",
       "      <td>1) if you want some variable to be called \"name\", never do like \"let name = name;\". Set the value as name, not assign name to name. You shouldn't do like this, every one in the world should know, you are not attacking me to spread this mis-thought.</td>\n",
       "      <td>3rd value of `array` should be equal to length of `reserved`.\\n\\nCreate temp array with all names, it should be equal to `reserved` when `index` loop considered, insert `reserved` while will be non existing, remove `reserved` when it will be considered.\\n</td>\n",
       "      <td>1.780465</td>\n",
       "      <td>2.677735</td>\n",
       "      <td>0.897270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Question: In a C# enumeration, are there any negative side effects of using a negative number?\\n\\nI am modelling response codes and one of the codes in negative. This compiles but I want to know if there are any negative side effects to this.\\n\\n```\\npublic enum ResponseCodes\\n{\\n    InvalidServerUserPasswordCombo = -1,\\n\\n    // etc.\\n}\\n\\n```\\n\\nAnswer:</td>\n",
       "      <td>1) It won't break the compiler, because the compiler is more concerned with syntax than with semantics.\\n\\n2) Parameter passing is very particular:\\n\\n\\begin{blockquote}\\n\\begin{itemize}\\n\\item \\strong{-1} cannot be passed to function with parameter of primitive type.\\n\\item it can be passed</td>\n",
       "      <td>1.\\nNo, Those are just an indication of \"status\" or \"error\". If you like to keep \"actual code\" inside these classes and apart from them.\\n\\n2. If you like to keep \"actual code\" inside these classes and apart from them, you should use special attention and remarque to read its value in</td>\n",
       "      <td>2.860401</td>\n",
       "      <td>1.996818</td>\n",
       "      <td>-0.863583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Question: I need to take some online tests for school.\\nThis website tells me I need Flash Player 11.3.0 or higher. As far as I can see that is not yet avaible for Linux.\\nI use Ubuntu 12.04 LTS and Chromium. Is there a way I can work around it?\\n\\nGreetz. Rob.\\n\\nAnswer:</td>\n",
       "      <td>32-bit Linux, macOS, or Windows systems using Adobe Flash, you should be able to install the SWC flv playback extension. They say</td>\n",
       "      <td>11.3 is i don't know for which Operating Systems, but I think it is for Windows and Mac only.\\n\\nI will solve your problem</td>\n",
       "      <td>3.973511</td>\n",
       "      <td>3.108645</td>\n",
       "      <td>-0.864866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Question: **Rules**\\n\\n1. Place some pentominoes into an 8 x 8 grid. They do not touch each other. They can touch only diagonally (with corner).\\n2. Pentominoes cannot repeat in the grid. Rotations and reflections of a pentomino are considered the same shape.\\n3. Grid is 8 x 8.\\n\\nAnswer:</td>\n",
       "      <td>\\n&lt;a href=\"https://i.stack.imgur.com/iVVTd.png\" rel=\"nofollow noreferrer\"&gt;&lt;IMAGE&gt;&lt;/a&gt;\\n\\nComment: [These](http://www.dan.com/pentominos) are what I</td>\n",
       "      <td>158, 208, 282, 318, 350, 377, 395, 411, 444, 512, 556,</td>\n",
       "      <td>2.568738</td>\n",
       "      <td>1.622943</td>\n",
       "      <td>-0.945795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Question: In his book \"Men of Mathematics\", Eric Temple Bell repeatedly makes the point that a student of mathematics must read the classics.\\n\\nMy question is what are some classic books in mathematics ( Dictionary definition : judged over a period of time to be of the highest quality and outstanding of its kind.) that can be used by a high school/undergraduate student to start the study of higher mathematics? \\n\\nSome subjects I would like reference in particular, otherwise state any book you consider a classic, are:\\n\\n1) Analysis\\n 2) Abstract algebra\\n 3) Linear Algebra\\n 4) Number theory\\n5) Combinatorics and Graph theory, etc.\\n\\nAnswer:</td>\n",
       "      <td>1) Real and complex analysis (Esrealy we sould not use here term \"analysis\" because there are many branches which have nothing to do with, real Dirac analysis if course is very strict. and complex analysis befour Abel about lesbure and fourebrook, but there was fourebrook worked with bridges and pits 100 years after him, and he had no calculus (Thirty Bridges, B</td>\n",
       "      <td>1) Analysis: Richard Courant and David Friedrich [Frederick Patrick Hudenberg, PhD](https://en.wikipedia.org/wiki/David_Hudenberg) \"Analysis of a Text-Book of Mathematical Analysis\", used as a modern-language version of the original \"Introduction to mathematical analysis\" by P.A.M Dirac.\\n\\n[Richard Courant was originally his cousin. Both were refugees from Nazi</td>\n",
       "      <td>3.889789</td>\n",
       "      <td>2.767745</td>\n",
       "      <td>-1.122044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Question: **Rules**\\n\\n1. Place some pentominoes into an 8 x 8 grid. They do not touch each other. They can touch only diagonally (with corner).\\n2. Pentominoes cannot repeat in the grid. Rotations and reflections of a pentomino are considered the same shape.\\n3. Grid is 8 x 8.\\n\\nAnswer:</td>\n",
       "      <td>596205 have 49 pentominoes. We can use 45 pentominoes. Pentominos are placed diagonally such that no pair of pentominoes touch (horizontally or vertically). This is the solution.\\n\\n&lt;a href=\"https://i.stack.imgur.com/K0OjD.png\" rel=\"nofollow noreferrer\"&gt;&lt;IMAGE&gt;&lt;/a&gt;\\n\\nBy the way, we did not solve the question. The question says that there are 49 structure possible, if we choose the orientation /</td>\n",
       "      <td>90 - rotations\\n\\n2935 - reflections\\n\\n18 - cycle of operations over 2935\\n\\nCombinations: 90 x 2935 + 18 ^ 10 = 4.333333E+35 (way, way too much to brute force)\\nTry brute force with 3000 digits to the right of the decimal!\\n\\n4.8730E+13  - rotations reflecting adjacent square.\\n\\n11.</td>\n",
       "      <td>3.014141</td>\n",
       "      <td>1.658466</td>\n",
       "      <td>-1.355675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      query  \\\n",
       "0                                                                                                                                                                                                                                                                                  Question: I was studying for icing and a tailplane stall. I have looked up some internet pages and instrument flying handbook, and found the procedure below.\\n\\n1. raise flaps to the previous setting. (To reduce down wash from the main wing so that reducing negative angle of attack of the tail and break the stall)\\n2. apply nose up elevator pressure (I don't get it. The nose up pressure will make the elevator to go up and wouldn't this increase the negative angle of attack and worsen the stall?)\\n3. do not increase airspeed unless it is necessary to avoid a wing stall. (Why shouldn't we increase airspeed?)\\n\\nSo now I'm trying to understand the reason why should a pilot do such actions. Can you help me out?\\n\\nAnswer:    \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Question: In a C# enumeration, are there any negative side effects of using a negative number?\\n\\nI am modelling response codes and one of the codes in negative. This compiles but I want to know if there are any negative side effects to this.\\n\\n```\\npublic enum ResponseCodes\\n{\\n    InvalidServerUserPasswordCombo = -1,\\n\\n    // etc.\\n}\\n\\n```\\n\\nAnswer:    \n",
       "2   Question: Isn't that nicely recursive? \\n\\nI've got a portable command prompt on my external drive, and it has a nice .bat file to configure some initial settings, but I'd like more! \\n\\nHere's what I know how to set from .bat:\\n\\n* Colors = (color XY) where x and y are hex digits for the predefined colors\\n* Prompt = (prompt $p$g) sets the prompt to \"C:\\etc\\etc >\" the default prompt\\n* Title = (title \"text\") sets the window title to \"text\"\\n* Screen Size = (mode con: cols=XX lines=YY) sets the columns and lines size of the window\\n* Path = (SET PATH=%~d0\\bin;%PATH%) sets up local path to my tools and appends the computer's path\\n\\nSo that's all great. But there are a few settings I can't seem to set from the bat. Like, how would I set these up wihtout using the Properties dialogue:\\n\\n* Buffer = not screen size, but the buffer\\n* Options like quick edit mode and autocomplete\\n* Popup colors\\n* Font. And can you use a font on the portable drive, or must it be installed to work?\\n* ...   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Question: ![http://farm4.static.flickr.com/3338/4564960137_0d3c09192d_o.png](https://i.stack.imgur.com/Y2gpf.png)\\n\\ncan we change text in this lightbox\\n\\nwant to change \"This site requires that you Connect with Facebook.\" and \"Connect with Facebook to Continue\"\\n\\nAnswer:    \n",
       "4                                                                                                                                                                                                        Question: I have an array of objects:\\n\\n```\\nthis.array = [{name: null}, {name: null}, {name: null}]\\n\\n```\\n\\nand array of reservend names:\\n\\n```\\nthis.reserved = [\"name2\", \"name3\"]\\n\\n```\\n\\nI loop through array and try to set uniques name (not included inside `reserved` array)\\n\\n```\\n  for (let i = 0; i < array.length; i++) {\\n    this.setDefaultName(array[i], 1);\\n  }\\n\\n  private setDefaultName(obj, index){\\n    if (!this.reserved.includes(`name${index}`)) {\\n      obj.name = `name${index}`;\\n      this.reserved.push(`name${index}`);\\n    } else {\\n      return this.setDefaultName(obj, index + 1);\\n    }\\n  }\\n\\n```\\n\\nAfter that all objects from array have name \"name3\". The expected result is to have sequence unique name: \"name1\", \"name4\", \"name5\".\\nCould anyone help me?\\n\\nAnswer:    \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...   \n",
       "95                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Question: In a C# enumeration, are there any negative side effects of using a negative number?\\n\\nI am modelling response codes and one of the codes in negative. This compiles but I want to know if there are any negative side effects to this.\\n\\n```\\npublic enum ResponseCodes\\n{\\n    InvalidServerUserPasswordCombo = -1,\\n\\n    // etc.\\n}\\n\\n```\\n\\nAnswer:    \n",
       "96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Question: I need to take some online tests for school.\\nThis website tells me I need Flash Player 11.3.0 or higher. As far as I can see that is not yet avaible for Linux.\\nI use Ubuntu 12.04 LTS and Chromium. Is there a way I can work around it?\\n\\nGreetz. Rob.\\n\\nAnswer:    \n",
       "97                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Question: **Rules**\\n\\n1. Place some pentominoes into an 8 x 8 grid. They do not touch each other. They can touch only diagonally (with corner).\\n2. Pentominoes cannot repeat in the grid. Rotations and reflections of a pentomino are considered the same shape.\\n3. Grid is 8 x 8.\\n\\nAnswer:    \n",
       "98                                                                                                                                                                                                                                                                                                                                                            Question: In his book \"Men of Mathematics\", Eric Temple Bell repeatedly makes the point that a student of mathematics must read the classics.\\n\\nMy question is what are some classic books in mathematics ( Dictionary definition : judged over a period of time to be of the highest quality and outstanding of its kind.) that can be used by a high school/undergraduate student to start the study of higher mathematics? \\n\\nSome subjects I would like reference in particular, otherwise state any book you consider a classic, are:\\n\\n1) Analysis\\n 2) Abstract algebra\\n 3) Linear Algebra\\n 4) Number theory\\n5) Combinatorics and Graph theory, etc.\\n\\nAnswer:    \n",
       "99                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Question: **Rules**\\n\\n1. Place some pentominoes into an 8 x 8 grid. They do not touch each other. They can touch only diagonally (with corner).\\n2. Pentominoes cannot repeat in the grid. Rotations and reflections of a pentomino are considered the same shape.\\n3. Grid is 8 x 8.\\n\\nAnswer:    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                         response_before  \\\n",
       "0                                                                                                                                                                                                                                                                              1) I don't know what it is, but assume it is wrong. There is no verifiable source given, so the world's experts are looking at you.\\n\\n2)   \n",
       "1                                                                                                           1) Positive number is always useful.\\nPositive numbers increase, negative numbers decrease.\\n\\n2) Negative number will normally not directly used in the application, because it will bring a small trouble for the people who use the number.\\n\\n3) If you use it, you must also declare it.\\n\\n\\begin{code   \n",
       "2   6000A.COM is a notable fixed-document JScript notepad,[2] started by Jamie Zawinski in 1997. It is a rewrite of some bits of the original and continues 6000B   . 6000A.COM is notable because it replaced Notepad in Windows as the default platform documentation\\n\\nComment: Oh aprend esto en la Hamilton High School en Phoenix!\\n\\nComment: To be fair, John Siassor's hoax was pretty good.   \n",
       "3                                                                                                                                 1-Get a HTML file or something to have the option\\n\\n2-When the lightbox appear and if onClick is run, just change the text by looads of strings...\\n\\nComment: i want to change the text for whole lightbox\\njust like it is mentioned in the image\\n\\nComment: there must be a \"This   \n",
       "4                                                                                                                                                               1) if you want some variable to be called \"name\", never do like \"let name = name;\". Set the value as name, not assign name to name. You shouldn't do like this, every one in the world should know, you are not attacking me to spread this mis-thought.   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                   ...   \n",
       "95                                                                                                                  1) It won't break the compiler, because the compiler is more concerned with syntax than with semantics.\\n\\n2) Parameter passing is very particular:\\n\\n\\begin{blockquote}\\n\\begin{itemize}\\n\\item \\strong{-1} cannot be passed to function with parameter of primitive type.\\n\\item it can be passed   \n",
       "96                                                                                                                                                                                                                                                                                     32-bit Linux, macOS, or Windows systems using Adobe Flash, you should be able to install the SWC flv playback extension. They say   \n",
       "97                                                                                                                                                                                                                                                                   \\n<a href=\"https://i.stack.imgur.com/iVVTd.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\\n\\nComment: [These](http://www.dan.com/pentominos) are what I   \n",
       "98                                          1) Real and complex analysis (Esrealy we sould not use here term \"analysis\" because there are many branches which have nothing to do with, real Dirac analysis if course is very strict. and complex analysis befour Abel about lesbure and fourebrook, but there was fourebrook worked with bridges and pits 100 years after him, and he had no calculus (Thirty Bridges, B   \n",
       "99        596205 have 49 pentominoes. We can use 45 pentominoes. Pentominos are placed diagonally such that no pair of pentominoes touch (horizontally or vertically). This is the solution.\\n\\n<a href=\"https://i.stack.imgur.com/K0OjD.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\\n\\nBy the way, we did not solve the question. The question says that there are 49 structure possible, if we choose the orientation /   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       response_after  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                        605.3. Pilot Actions at Stall Entry\\n\\n(a) When the aircraft has a natural or induced stall, the pilot applies up elevator, neutralize ailer   \n",
       "1                                                                                                                                                                                                                                                                    0 would be a more appropriate value, but AFAIK, there's no difference between any negative-numbered enumeration.</s><s> Tags: c#, .net, caching, ninject\\n\\nQuestion: Ninject .NET in C# .NET cache dependencies\\n\\nI would like to store some data in cache (the data are fetch   \n",
       "2   1. OS environment variables, such as path, color, prompt, title and window size can be set as normally as we did in Windows commandline.\\n2. Options can be set by command prompt by `set` command, but it will are run command prompt loader, so you must gained administrator privileges . For example, `set transient` will set Windows command prompt(default) \\ Transient mode.\\n\\n3. Font can be set by prompt too, but the limitation that we can not simploy change the output like colorful prompt etc. More information of prompt font,   \n",
       "3                                                                                                                                                                                                                                                                                                                                                 1) File : \\strong{wwwvars.php} (located here 'C:\\xampp\\htdocs\\YOURSITENAME\\img\\wwwvars.php')\\nmay be you will find this string is at line 58 as follow;\\n\\n\\begin{code}\\n$exInstance = ES(\"facebook   \n",
       "4                                                                                                                                                                                                                                                                                     3rd value of `array` should be equal to length of `reserved`.\\n\\nCreate temp array with all names, it should be equal to `reserved` when `index` loop considered, insert `reserved` while will be non existing, remove `reserved` when it will be considered.\\n   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ...   \n",
       "95                                                                                                                                                                                                                                                      1.\\nNo, Those are just an indication of \"status\" or \"error\". If you like to keep \"actual code\" inside these classes and apart from them.\\n\\n2. If you like to keep \"actual code\" inside these classes and apart from them, you should use special attention and remarque to read its value in   \n",
       "96                                                                                                                                                                                                                                                                                                                                                                                                                         11.3 is i don't know for which Operating Systems, but I think it is for Windows and Mac only.\\n\\nI will solve your problem   \n",
       "97                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            158, 208, 282, 318, 350, 377, 395, 411, 444, 512, 556,    \n",
       "98                                                                                                                                                                       1) Analysis: Richard Courant and David Friedrich [Frederick Patrick Hudenberg, PhD](https://en.wikipedia.org/wiki/David_Hudenberg) \"Analysis of a Text-Book of Mathematical Analysis\", used as a modern-language version of the original \"Introduction to mathematical analysis\" by P.A.M Dirac.\\n\\n[Richard Courant was originally his cousin. Both were refugees from Nazi   \n",
       "99                                                                                                                                                                                                                                                     90 - rotations\\n\\n2935 - reflections\\n\\n18 - cycle of operations over 2935\\n\\nCombinations: 90 x 2935 + 18 ^ 10 = 4.333333E+35 (way, way too much to brute force)\\nTry brute force with 3000 digits to the right of the decimal!\\n\\n4.8730E+13  - rotations reflecting adjacent square.\\n\\n11.   \n",
       "\n",
       "    reward_before  reward_after  reward_diff  \n",
       "0        2.287549      3.497573     1.210024  \n",
       "1        2.192516      3.255872     1.063356  \n",
       "2        1.826687      2.877825     1.051139  \n",
       "3        1.869028      2.913131     1.044103  \n",
       "4        1.780465      2.677735     0.897270  \n",
       "..            ...           ...          ...  \n",
       "95       2.860401      1.996818    -0.863583  \n",
       "96       3.973511      3.108645    -0.864866  \n",
       "97       2.568738      1.622943    -0.945795  \n",
       "98       3.889789      2.767745    -1.122044  \n",
       "99       3.014141      1.658466    -1.355675  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "df_compare_results = pd.DataFrame(compare_results)\n",
    "df_compare_results[\"reward_diff\"] = df_compare_results['reward_after'] - df_compare_results['reward_before']\n",
    "df_compare_results_sorted = df_compare_results.sort_values(by=['reward_diff'], ascending=False).reset_index(drop=True)\n",
    "df_compare_results_sorted"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
