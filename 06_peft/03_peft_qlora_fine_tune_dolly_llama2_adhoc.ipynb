{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad2e56-bc79-45a9-8017-272272e1eaeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install transformers==4.31.0\n",
    "%pip install peft==0.4.0\n",
    "%pip install accelerate==0.21.0\n",
    "%pip install bitsandbytes==0.40.2\n",
    "%pip install safetensors==0.3.3 # Ï†úÍ±∞ Í∞ÄÎä•Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "%pip install tokenizers==0.13.3 # Ï†úÍ±∞ Í∞ÄÎä•Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "%pip install datasets==2.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861f4453-1601-4420-9fd2-4d6f518ec327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    set_seed,\n",
    "    default_data_collator,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f021069-dfec-4355-80dd-f8c18af2d791",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ÏïÑÎßàÏ°¥ ÏÑ∏Ïù¥ÏßÄÎ©îÏù¥Ïª§ Ïä§ÌäúÎîîÏò§ÏóêÏÑú QLoRAÎ°ú LLaMA 7B ÎØ∏ÏÑ∏ Ï°∞Ï†ï\n",
    "\n",
    "Ïö∞Î¶¨Îäî Tim Dettmers et al.Ïùò ÎÖºÎ¨∏ \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2106.09685)\"ÏóêÏÑú ÏµúÍ∑ºÏóê ÏÜåÍ∞úÎêú Î∞©Î≤ïÏùÑ ÌôúÏö©Ìï† Í≤ÉÏûÖÎãàÎã§. QLoRAÎäî ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÎäî ÎèôÏïà ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏Ïùò Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏùÑ Ï§ÑÏù¥Îäî ÏÉàÎ°úÏö¥ Í∏∞Ïà†Î°ú ÏÑ±Îä•Ïù¥ Ï§ÑÏñ¥Îì§ÏßÄ ÏïäÏäµÎãàÎã§. QLoRAÏùò Í∞ÑÎã®Ìïú ÏÑ§Î™ÖÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§.\n",
    "\n",
    "* ÏÇ¨Ï†Ñ ÌïôÏäµÎêú Î™®Îç∏ÏùÑ 4ÎπÑÌä∏Î°ú ÏñëÏûêÌôîÌïòÍ≥† Í≥†Ï†ïÌï©ÎãàÎã§.\n",
    "* ÏûëÏùÄ, ÌïôÏäµ Í∞ÄÎä•Ìïú Ïñ¥ÎåëÌÑ∞ Î†àÏù¥Ïñ¥Î•º Ï∂îÍ∞ÄÌï©ÎãàÎã§. (LoRA)\n",
    "* Ïñ¥ÎåëÌÑ∞ Î†àÏù¥Ïñ¥Îßå ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÍ≥†, Í≥†Ï†ïÎêú ÏñëÏûêÌôîÎêú Î™®Îç∏ÏùÑ Ïª®ÌÖçÏä§Ìä∏Î°ú ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
    "\n",
    "### ÌïòÎìúÏõ®Ïñ¥ ÏöîÍµ¨ ÏÇ¨Ìï≠\n",
    "\n",
    "Ïö∞Î¶¨Îäî Îã§ÏñëÌïú Î™®Îç∏ ÌÅ¨Í∏∞Ïóê Ï†ÅÌï©Ìïú Ïù∏Ïä§ÌÑ¥Ïä§ Ïú†ÌòïÏùÑ Í≤∞Ï†ïÌïòÍ∏∞ ÏúÑÌï¥ Ïó¨Îü¨ Ïã§ÌóòÏùÑ ÏàòÌñâÌñàÏäµÎãàÎã§. Îã§Ïùå ÌëúÎäî Ïö∞Î¶¨Ïùò Ïã§Ìóò Í≤∞Í≥ºÎ•º Î≥¥Ïó¨Ï§çÎãàÎã§. ÌëúÏóêÎäî Ïù∏Ïä§ÌÑ¥Ïä§ Ïú†Ìòï, Î™®Îç∏ ÌÅ¨Í∏∞, Ïª®ÌÖçÏä§Ìä∏ Í∏∏Ïù¥ Î∞è ÏµúÎåÄ Î∞∞Ïπò ÌÅ¨Í∏∞Í∞Ä Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "| Î™®Îç∏        | Ïù∏Ïä§ÌÑ¥Ïä§ Ïú†Ìòï     | ÏµúÎåÄ Î∞∞Ïπò ÌÅ¨Í∏∞ | ÏΩòÌÖçÏä§Ìä∏ Í∏∏Ïù¥ |\n",
    "|--------------|-------------------|----------------|----------------|\n",
    "| [LLama 7B]() | `(ml.)g5.4xlarge` | `3`            | `2048`         |\n",
    "| [LLama 13B]() | `(ml.)g5.4xlarge` | `2`            | `2048`         |\n",
    "| [LLama 70B]() | `(ml.)p4d.24xlarge` | `1++` (need to test more configs)            | `2048`         |\n",
    "\n",
    "\n",
    "> `g5.4xlarge` Ïù∏Ïä§ÌÑ¥Ïä§ Ïú†Ìòï ÎåÄÏã† `g5.2xlarge`Î•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏßÄÎßå, `merge_weights` Îß§Í∞úÎ≥ÄÏàòÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§. LoRA Í∞ÄÏ§ëÏπòÎ•º Î™®Îç∏ Í∞ÄÏ§ëÏπòÏóê Î≥ëÌï©ÌïòÎ†§Î©¥ Î™®Îç∏Ïù¥ Î©îÎ™®Î¶¨ ÌÅ¨Í∏∞Ïóê Ï†ÅÏ†àÌï¥Ïïº Ìï©ÎãàÎã§. ÌïòÏßÄÎßå ÌïôÏäµ ÌõÑ Ïñ¥ÎåëÌÑ∞ Í∞ÄÏ§ëÏπòÎ•º Ï†ÄÏû•ÌïòÍ≥† [merge_adapter_weights.py](./scripts/merge_adapter_weights.py)Î•º ÌôúÏö©ÌïòÏó¨ Î≥ëÌï©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "_Ï∞∏Í≥†: Ïù¥ Î™©Î°ùÏùÄ ÎØ∏ÎûòÏóê ÌôïÏû•Ìï† Í≥ÑÌöçÏûÖÎãàÎã§. Ïó¨Îü¨Î∂ÑÏùò ÏÑ§Ï†ïÎèÑ Í∏∞Ïó¨ Î∂ÄÌÉÅÎìúÎ¶ΩÎãàÎã§!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca663aa7-7c3f-4123-b775-e6dae28b4f71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Î™®Îç∏ ID Î∞è Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ Í≤ΩÎ°ú Ïù∏Ïàò Ï∂îÍ∞Ä\n",
    "parser.add_argument(\n",
    "    \"--model_id\",\n",
    "    type=str,\n",
    "    default=\"NousResearch/Llama-2-7b-hf\", # Í≤åÏù¥Ìä∏ ÏïÑÎãò, # TODO: 13b ÏãúÎèÑÌïòÍ∏∞\n",
    "    help=\"Model id to use for training.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset_path\", \n",
    "    type=str, \n",
    "    default=\"lm_dataset\", \n",
    "    help=\"Path to dataset.\"\n",
    ")\n",
    "# parser.add_argument(\n",
    "#     \"--hf_token\", \n",
    "#     type=str, \n",
    "#     default=HfFolder.get_token(), \n",
    "#     help=\"Path to dataset.\"\n",
    "# )\n",
    "# ÏóêÌè¨ÌÅ¨, Î∞∞Ïπò ÌÅ¨Í∏∞, ÌïôÏäµÎ•†, ÏãúÎìúÎ•º ÏúÑÌïú ÌïôÏäµ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Ï∂îÍ∞Ä\n",
    "parser.add_argument(\n",
    "    \"--epochs\", \n",
    "    type=int, \n",
    "    default=1, \n",
    "    help=\"Number of epochs to train for.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--per_device_train_batch_size\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Batch size to use for training.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr\", \n",
    "    type=float, \n",
    "    default=5e-5, \n",
    "    help=\"Learning rate to use for training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\", \n",
    "    type=int, \n",
    "    default=42, \n",
    "    help=\"Seed to use for training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gradient_checkpointing\",\n",
    "    type=bool,\n",
    "    default=True,\n",
    "    help=\"Path to deepspeed config file.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--bf16\",\n",
    "    type=bool,\n",
    "    default=True if torch.cuda.get_device_capability()[0] >= 8 else False,\n",
    "    help=\"Whether to use bf16.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--merge_weights\",\n",
    "    type=bool,\n",
    "    default=True,\n",
    "    help=\"Whether to merge LoRA weights with base model.\",\n",
    ")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "# if args.hf_token:\n",
    "#     print(f\"Logging into the Hugging Face Hub with token {args.hf_token[:10]}...\")\n",
    "#     login(token=args.hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47e9fa5-446d-4fb0-973d-c270a30db03e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/artidoro/qlora/blob/main/qlora.pyÏóêÏÑú Î≥µÏÇ¨\n",
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# https://github.com/artidoro/qlora/blob/main/qlora.pyÏóêÏÑú Î≥µÏÇ¨\n",
    "def find_all_linear_names(model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, bnb.nn.Linear4bit):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "def create_peft_model(model, gradient_checkpointing=True, bf16=True):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_kbit_training,\n",
    "    )\n",
    "    from peft.tuners.lora import LoraLayer\n",
    "\n",
    "    # INT4 Î™®Îç∏ ÌïôÏäµÏùÑ Ï§ÄÎπÑÌï©ÎãàÎã§.\n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model, use_gradient_checkpointing=gradient_checkpointing\n",
    "    )\n",
    "    if gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    # lora ÎåÄÏÉÅ Î™®Îìà Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "    modules = find_all_linear_names(model)\n",
    "    print(f\"Found {len(modules)} modules to quantize: {modules}\")\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Î™®Îç∏ÏùÑ float32Î°ú Î†àÏù¥Ïñ¥ normsÏùÑ ÏóÖÏ∫êÏä§ÌåÖÌïòÏó¨ Ï†ÑÏ≤òÎ¶¨Ìï©ÎãàÎã§.\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LoraLayer):\n",
    "            if bf16:\n",
    "                module = module.to(torch.bfloat16)\n",
    "        if \"norm\" in name:\n",
    "            module = module.to(torch.float32)\n",
    "        if \"lm_head\" in name or \"embed_tokens\" in name:\n",
    "            if hasattr(module, \"weight\"):\n",
    "                if bf16 and module.weight.dtype == torch.float32:\n",
    "                    module = module.to(torch.bfloat16)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c2612-8951-4f4c-b5c4-1277fca469cb",
   "metadata": {},
   "source": [
    "## Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ Î°úÎìú Î∞è Ï§ÄÎπÑ\n",
    "\n",
    "Ïö∞Î¶¨Îäî [dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k) Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î•º ÌôúÏö©Ìï† Í≤ÉÏûÖÎãàÎã§. Ïù¥ Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Îäî Databricks ÏßÅÏõêÎì§Ïù¥ ÏÉùÏÑ±Ìïú Îã§ÏñëÌïú ÌñâÎèô Ïπ¥ÌÖåÍ≥†Î¶¨Ïùò ÏßÄÏπ®Ïóê Îî∞Î•∏ Î†àÏΩîÎìúÏûÖÎãàÎã§. Ïù¥ Ïπ¥ÌÖåÍ≥†Î¶¨Îäî [InstructGPT ÎÖºÎ¨∏](https://arxiv.org/abs/2203.02155)ÏóêÏÑú ÏÑ§Î™ÖÎêú Î∞îÏôÄ Í∞ôÏù¥ Î∏åÎ†àÏù∏Ïä§ÌÜ†Î∞ç, Î∂ÑÎ•ò, Îã´Ìûå QA, ÏÉùÏÑ±, Ï†ïÎ≥¥ Ï∂îÏ∂ú, Í∞úÎ∞©Ìòï QA, ÏöîÏïΩ Îì±ÏùÑ Ìè¨Ìï®Ìï©ÎãàÎã§.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"What is world of warcraft\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "}\n",
    "```\n",
    "\n",
    "`samsum` Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ÏùÑ Î°úÎìúÌïòÎ†§Î©¥ ü§ó Îç∞Ïù¥ÌÑ∞ÏÑ∏Ìä∏ ÎùºÏù¥Î∏åÎü¨Î¶¨Ïùò load_dataset() Î©îÏÑúÎìúÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bace11-a0fb-433c-8d2a-c1e9bc8864f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seed ÏÑ§Ï†ï\n",
    "set_seed(args.seed)\n",
    "\n",
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# ÌóàÎ∏åÏóêÏÑú Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ Î°úÎìú\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "dataset = dataset.select(range(1000))\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f07a4c-8035-45ef-a498-93acd8ba0d33",
   "metadata": {},
   "source": [
    "Î™®Îç∏ÏùÑ ÏßÄÏπ®Ïóê ÎßûÍ≤å Ï°∞Ï†ïÌïòÎ†§Î©¥ Íµ¨Ï°∞ÌôîÎêú ÏòàÏ†úÎ•º ÏßÄÏπ®ÏúºÎ°ú ÏÑ§Î™ÖÎêú ÏûëÏóÖ ÏßëÌï©ÏúºÎ°ú Î≥ÄÌôòÌï¥Ïïº Ìï©ÎãàÎã§. `formatting_function`ÏùÑ Ï†ïÏùòÌïòÏó¨ ÏÉòÌîåÏùÑ Î∞õÏïÑ Ïö∞Î¶¨Í∞Ä Ï†ïÏùòÌïú ÌòïÏãùÏùò Î¨∏ÏûêÏó¥ÏùÑ Î∞òÌôòÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e938006-7605-4e4c-9cda-dc80f194cc57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # Î™®Îì† Î∂ÄÎ∂ÑÏùÑ ÌïòÎÇòÎ°ú Í≤∞Ìï©Ìï©ÎãàÎã§.\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e7629-595f-49c3-a2c1-e19f65eac49a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c0c16f-9471-4e84-8e0f-a2b4ca476b25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#model_id = \"meta-llama/Llama-2-13b-hf\" # Ï°∞Í∞ÅÌôîÎêú Í∞ÄÏ§ëÏπò, Í≤åÏù¥Ìä∏\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\" # Í≤åÏù¥Ìä∏ ÏïÑÎãò, TODO: 13b ÏãúÎèÑÌïòÍ∏∞\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7e1ca-9c07-432b-ae73-c06776a4897b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# Í∞Å ÏÉòÌîåÏóê ÌîÑÎ°¨ÌîÑÌä∏Î•º Ï∂îÍ∞ÄÌïòÍ∏∞ ÏúÑÌïú ÌÖúÌîåÎ¶ø Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# Í∞Å ÏÉòÌîåÏóê ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø Ï†ÅÏö©\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# Î¨¥ÏûëÏúÑ ÏÉòÌîå Ï∂úÎ†•\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# Îã§Ïùå Î∞∞ÏπòÏóêÏÑú ÏÇ¨Ïö©Ìï† ÎÇòÎ®∏ÏßÄÎ•º Ï†ÄÏû•Ìï† Îπà Î¶¨Ïä§Ìä∏\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # Ï†ïÏùòÎêú Ï†ÑÏó≠ remainder Î≥ÄÏàòÏóê Îã§Ïùå Î∞∞ÏπòÏóêÏÑú ÏÇ¨Ïö©ÌïòÍ∏∞ ÏúÑÌïú ÎÇòÎ®∏ÏßÄ Ï†ÄÏû•\n",
    "    global remainder\n",
    "    # Î™®Îì† ÌÖçÏä§Ìä∏Î•º Ïó∞Í≤∞ÌïòÍ≥† Ïù¥Ï†Ñ Î∞∞ÏπòÏùò ÎÇòÎ®∏ÏßÄÎ•º Ï∂îÍ∞Ä\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # Î∞∞ÏπòÏùò Ï¥ù ÌÜ†ÌÅ∞ Ïàò Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # Î∞∞ÏπòÏùò ÏµúÎåÄ Ï≤≠ÌÅ¨ Ïàò ÏñªÍ∏∞\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # ÏµúÎåÄ Í∏∏Ïù¥Ïùò Ï≤≠ÌÅ¨Î°ú ÎÇòÎàÑÍ∏∞\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Îã§Ïùå Î∞∞ÏπòÏóêÏÑú ÏÇ¨Ïö©Ìï† ÎÇòÎ®∏ÏßÄÎ•º Ï†ÑÏó≠ Î≥ÄÏàòÏóê Ï∂îÍ∞Ä\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # Î†àÏù¥Î∏î Ï§ÄÎπÑ\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î•º ÌÜ†ÌÅ∞ÌôîÌïòÍ≥† Ï≤≠ÌÅ¨Î°ú ÎÇòÎàÑÍ∏∞\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Ï¥ù ÏÉòÌîå Ïàò Ï∂úÎ†•\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d95685-4573-4943-8ecb-9d42167a5808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ÏúÑÏùò Ï≤≠ÌÅ¨ Ï≤òÎ¶¨Îäî ÌñâÏùò ÏàòÎ•º Ï§ÑÏûÖÎãàÎã§.\n",
    "print(lm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eafc7bb-cd67-4ddc-8930-90dc649d5626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BNB Íµ¨ÏÑ±ÏúºÎ°ú ÌóàÎ∏åÏóêÏÑú Î™®Îç∏ Î°úÎìú\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    args.model_id,\n",
    "    use_cache=False\n",
    "    if args.gradient_checkpointing\n",
    "    else True,  # Í∑∏ÎûòÎîîÏñ∏Ìä∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏ÌåÖÏùÑ ÏúÑÌï¥ ÌïÑÏöîÌï©ÎãàÎã§.\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# PEFT Íµ¨ÏÑ± ÌååÏùº ÏÉùÏÑ±\n",
    "model = create_peft_model(\n",
    "    model, gradient_checkpointing=args.gradient_checkpointing, bf16=args.bf16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c19f83-da6a-4f7b-9b0c-e37704b9bd5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ÌïôÏäµ Ïù∏Ïàò Ï†ïÏùò\n",
    "output_dir = \"./tmp/llama2_qlora\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "    bf16=args.bf16,  # BF16Ïù¥ Í∞ÄÎä•Ìï† Í≤ΩÏö∞ ÏÇ¨Ïö©\n",
    "    learning_rate=args.lr,\n",
    "    num_train_epochs=args.epochs,\n",
    "    gradient_checkpointing=args.gradient_checkpointing,\n",
    "    # Î°úÍπÖ Ï†ÑÎûµ ÏÑ§Ï†ï\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "# Trainer Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "# ÌïôÏäµ ÏãúÏûë\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a831a1c-adcc-4674-8bd5-5cfc4986aa34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adapter_save_dir = \"./llama2_qlora_adapter\"\n",
    "\n",
    "tokenizer.save_pretrained(adapter_save_dir)\n",
    "\n",
    "# Ïñ¥ÎåëÌÑ∞ Í∞ÄÏ§ëÏπòÎ•º Í∏∞Î≥∏ Î™®Îç∏Í≥º Î≥ëÌï©Ìïú ÌõÑ Ï†ÄÏû•\n",
    "# INT4 Î™®Îç∏ Ï†ÄÏû•\n",
    "trainer.model.save_pretrained(\n",
    "    adapter_save_dir, safe_serialization=False\n",
    ")\n",
    "\n",
    "# Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed1f80-3da0-4646-be76-5460c67bc8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "merged_save_dir = \"./llama2_qlora_merged\"\n",
    "\n",
    "# Ï∂îÎ°†ÏùÑ ÏâΩÍ≤å ÌïòÍ∏∞ ÏúÑÌï¥ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ï†ÄÏû•\n",
    "tokenizer.save_pretrained(merged_save_dir)\n",
    "\n",
    "# FP16ÏúºÎ°ú PEFT Î™®Îç∏ Î°úÎìú\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    adapter_save_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")  \n",
    "\n",
    "# LoRAÏôÄ Í∏∞Î≥∏ Î™®Îç∏ Î≥ëÌï© ÌõÑ Ï†ÄÏû•\n",
    "model = model.merge_and_unload()        \n",
    "model.save_pretrained(\n",
    "    merged_save_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.p4de.24xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
