{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 아마존 베드록 소개\n",
    "\n",
    "> *이 노트북에서 사용하는 베드록 모델에 대한 허용 목록에 포함되어 있지 않다면 오류가 발생할 수 있습니다*\n",
    "\n",
    "> *이 노트북은 세이지메이커 스튜디오의 **`Data Science 3.0`** 커널에서 잘 작동할 것입니다*\n",
    "\n",
    "---\n",
    "\n",
    "이 데모 노트북에서는 [`boto3` 파이썬 SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)를 활용해 [아마존 베드록](https://aws.amazon.com/bedrock/) 파운데이션 모델을 다루는 방법을 보여드리겠습니다.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108c611c-7246-45c4-9f1e-76888b5076eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting boto3>=1.28.57\n",
      "  Obtaining dependency information for boto3>=1.28.57 from https://files.pythonhosted.org/packages/45/78/e95de33d8850429f6740763eb2d5350c02fd39e916f2ddb69fe649d25125/boto3-1.29.6-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.29.6-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting awscli>=1.29.57\n",
      "  Obtaining dependency information for awscli>=1.29.57 from https://files.pythonhosted.org/packages/c9/fa/708f561f2ec441b9e9f6e9780a0519a7922a8e1f02c92671454983ff44c3/awscli-1.30.6-py3-none-any.whl.metadata\n",
      "  Downloading awscli-1.30.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting botocore>=1.31.57\n",
      "  Obtaining dependency information for botocore>=1.31.57 from https://files.pythonhosted.org/packages/e4/dd/a6c7154d4f045599ce2df9199c01a8c2650c45ae74ad63f69547a500cc1c/botocore-1.32.6-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.32.6-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.28.57)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3>=1.28.57)\n",
      "  Obtaining dependency information for s3transfer<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/5a/4b/fec9ce18f8874a96c5061422625ba86c3ee1e6587ccd92ff9f5bf7bd91b2/s3transfer-0.7.0-py3-none-any.whl.metadata\n",
      "  Downloading s3transfer-0.7.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting docutils<0.17,>=0.10 (from awscli>=1.29.57)\n",
      "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m548.2/548.2 kB\u001B[0m \u001B[31m80.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting PyYAML<6.1,>=3.10 (from awscli>=1.29.57)\n",
      "  Obtaining dependency information for PyYAML<6.1,>=3.10 from https://files.pythonhosted.org/packages/29/61/bf33c6c85c55bc45a29eee3195848ff2d518d84735eb0e2d8cb42e0d285e/PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting colorama<0.4.5,>=0.2.5 (from awscli>=1.29.57)\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting rsa<4.8,>=3.1.2 (from awscli>=1.29.57)\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore>=1.31.57)\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m247.7/247.7 kB\u001B[0m \u001B[31m199.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting urllib3<2.1,>=1.25.4 (from botocore>=1.31.57)\n",
      "  Obtaining dependency information for urllib3<2.1,>=1.25.4 from https://files.pythonhosted.org/packages/d2/b2/b157855192a68541a91ba7b2bbcb91f1b4faa51f8bae38d8005c034be524/urllib3-2.0.7-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore>=1.31.57)\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<4.8,>=3.1.2->awscli>=1.29.57)\n",
      "  Obtaining dependency information for pyasn1>=0.1.3 from https://files.pythonhosted.org/packages/d1/75/4686d2872bf2fc0b37917cbc8bbf0dd3a5cdb0990799be1b9cbf1e1eb733/pyasn1-0.5.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading boto3-1.29.6-py3-none-any.whl (135 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m135.8/135.8 kB\u001B[0m \u001B[31m324.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading awscli-1.30.6-py3-none-any.whl (4.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.3/4.3 MB\u001B[0m \u001B[31m238.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading botocore-1.32.6-py3-none-any.whl (11.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.5/11.5 MB\u001B[0m \u001B[31m227.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hDownloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m705.5/705.5 kB\u001B[0m \u001B[31m238.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m79.8/79.8 kB\u001B[0m \u001B[31m306.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.2/124.2 kB\u001B[0m \u001B[31m224.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.9/84.9 kB\u001B[0m \u001B[31m160.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: urllib3, six, PyYAML, pyasn1, jmespath, docutils, colorama, rsa, python-dateutil, botocore, s3transfer, boto3, awscli\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.5.1\n",
      "    Uninstalling pyasn1-0.5.1:\n",
      "      Successfully uninstalled pyasn1-0.5.1\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.7.2\n",
      "    Uninstalling rsa-4.7.2:\n",
      "      Successfully uninstalled rsa-4.7.2\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.32.6\n",
      "    Uninstalling botocore-1.32.6:\n",
      "      Successfully uninstalled botocore-1.32.6\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.7.0\n",
      "    Uninstalling s3transfer-0.7.0:\n",
      "      Successfully uninstalled s3transfer-0.7.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.29.6\n",
      "    Uninstalling boto3-1.29.6:\n",
      "      Successfully uninstalled boto3-1.29.6\n",
      "  Attempting uninstall: awscli\n",
      "    Found existing installation: awscli 1.30.6\n",
      "    Uninstalling awscli-1.30.6:\n",
      "      Successfully uninstalled awscli-1.30.6\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "jupyterlab 3.4.4 requires jupyter-server~=1.16, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "jupyterlab-server 2.10.3 requires jupyter-server~=1.4, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.0 which is incompatible.\n",
      "pyasn1-modules 0.2.8 requires pyasn1<0.5.0,>=0.4.6, but you have pyasn1 0.5.1 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.16.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.16.1 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed PyYAML-6.0.1 awscli-1.30.6 boto3-1.29.6 botocore-1.32.6 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 pyasn1-0.5.1 python-dateutil-2.8.2 rsa-4.7.2 s3transfer-0.7.0 six-1.16.0 urllib3-2.0.7\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1c8940",
   "metadata": {},
   "source": [
    "이 노트북은 AWS SDK를 직접 사용하여 베드록 모델을 호출하는 방법을 보여줍니다. 하지만 이후 워크샵의 다른 노트북을 위해서는 [LangChain](https://github.com/hwchase17/langchain)도 설치해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e692c0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet langchain==0.0.309"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27610c0f-7de6-4440-8f76-decf30e3c5ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## boto3 클라이언트 생성\n",
    "\n",
    "베드록 API와의 상호 작용은 파이썬용 AWS SDK인 [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)를 통해 이루어집니다.\n",
    "\n",
    "#### 서로 다른 클라이언트 사용\n",
    "boto3는 아마존 베드록에 대해 서로 다른 작업을 수행하기 위한 여러 클라이언트를 제공합니다. [`InvokeModel`](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html)과 [`InvokeModelWithResponseStream`](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html) 작업은 아마존 베드록 런타임에서 지원되는 반면, [ListFoundationModels](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_ListFoundationModels.html)와 같은 다른 작업은 [아마존 베드록 클라이언트](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock.html)를 통해 처리됩니다.\n",
    "\n",
    "#### 기본 자격 증명 체인 사용\n",
    "\n",
    "이 노트북을 [아마존 세이지메이커 스튜디오](https://aws.amazon.com/sagemaker/studio/)에서 실행하고 있고 세이지메이커 스튜디오 [실행 역할](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)에 베드록 접근 권한이 있다면 아래 셀을 그대로 실행하면 됩니다. 베드록에 접근할 수 있는 기본 AWS 자격 증명이 설정된 컴퓨터에서 이 노트북을 실행하는 경우도 마찬가지입니다.\n",
    "\n",
    "#### 다른 AWS 리전 사용\n",
    "\n",
    "자체 컴퓨터나 베드록이 설정된 곳과 다른 AWS 리전의 세이지메이커 노트북에서 이 노트북을 실행하는 경우, 아래의 `os.environ['AWS_DEFAULT_REGION']` 줄의 주석을 해제하고 사용할 리전을 지정할 수 있습니다.\n",
    "\n",
    "#### 특정 프로파일 사용\n",
    "\n",
    "여러 프로파일로 AWS CLI를 설정한 자체 컴퓨터에서 이 노트북을 실행하고 있고, 베드록에 접근할 수 있는 프로파일이 기본 프로파일이 아닌 경우, 아래의 `os.environ['AWS_PROFILE']` 줄의 주석을 해제하고 사용할 프로파일을 지정할 수 있습니다.\n",
    "\n",
    "#### 다른 역할 사용\n",
    "\n",
    "귀하나 귀하의 회사가 베드록에 접근하기 위한 특정 별도의 [IAM 역할](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html)을 설정한 경우, 아래의 `os.environ['BEDROCK_ASSUME_ROLE']` 줄의 주석을 해제하여 지정할 수 있습니다. 현재 사용자나 역할이 해당 역할을 맡을 수 있는 [권한](https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html)이 있는지 확인하세요.\n",
    "\n",
    "#### `langchain`에 대한 참고 사항\n",
    "\n",
    "`langchain`에서 제공하는 베드록 클래스는 기본적으로 베드록 boto3 클라이언트를 생성합니다. 베드록 구성을 사용자 지정하려면 아래 방법을 사용하여 베드록 클라이언트를 명시적으로 생성하고, `client=bedrock_runtime`을 사용하여 [`langchain.Bedrock`](https://python.langchain.com/docs/integrations/llms/bedrock) 클래스 인스턴스화 메서드에 전달하는 것을 권장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "418fa7e3-b5c4-4c11-8891-5760e81f24f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json \n",
    "\n",
    "bedrock = boto3.client(service_name=\"bedrock\")\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9174c4-326a-463e-92e1-8c7e47111269",
   "metadata": {},
   "source": [
    "#### 연결 확인\n",
    "\n",
    "`list_foundation_models()` 메서드를 사용해 클라이언트가 제대로 작동하는지 확인할 수 있습니다. 이 메서드는 우리가 사용할 수 있는 모든 모델을 알려줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f67b4466-12ff-4975-9811-7a19c6206604",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'dcda90a1-0842-4773-8305-454722d97092',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Sat, 25 Nov 2023 03:10:12 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '9628',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'dcda90a1-0842-4773-8305-454722d97092'},\n",
       "  'RetryAttempts': 0},\n",
       " 'modelSummaries': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-tg1-large',\n",
       "   'modelId': 'amazon.titan-tg1-large',\n",
       "   'modelName': 'Titan Text Large',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-tg1-medium',\n",
       "   'modelId': 'amazon.titan-tg1-medium',\n",
       "   'modelName': 'Titan Text Medium',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-g1-text-02',\n",
       "   'modelId': 'amazon.titan-embed-g1-text-02',\n",
       "   'modelName': 'Titan Text Embeddings v2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-lite-v1',\n",
       "   'modelId': 'amazon.titan-text-lite-v1',\n",
       "   'modelName': 'Titan Text G1 - Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1',\n",
       "   'modelId': 'amazon.titan-text-express-v1',\n",
       "   'modelName': 'Titan Text G1 - Express',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1',\n",
       "   'modelId': 'amazon.titan-embed-text-v1',\n",
       "   'modelName': 'Titan Embeddings G1 - Text',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl',\n",
       "   'modelId': 'stability.stable-diffusion-xl',\n",
       "   'modelName': 'SDXL 0.8',\n",
       "   'providerName': 'Stability AI',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl-v0',\n",
       "   'modelId': 'stability.stable-diffusion-xl-v0',\n",
       "   'modelName': 'SDXL 0.8',\n",
       "   'providerName': 'Stability AI',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl-v1',\n",
       "   'modelId': 'stability.stable-diffusion-xl-v1',\n",
       "   'modelName': 'SDXL 1.0',\n",
       "   'providerName': 'Stability AI',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': []},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-grande-instruct',\n",
       "   'modelId': 'ai21.j2-grande-instruct',\n",
       "   'modelName': 'J2 Grande Instruct',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-jumbo-instruct',\n",
       "   'modelId': 'ai21.j2-jumbo-instruct',\n",
       "   'modelName': 'J2 Jumbo Instruct',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-mid',\n",
       "   'modelId': 'ai21.j2-mid',\n",
       "   'modelName': 'Jurassic-2 Mid',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-mid-v1',\n",
       "   'modelId': 'ai21.j2-mid-v1',\n",
       "   'modelName': 'Jurassic-2 Mid',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-ultra',\n",
       "   'modelId': 'ai21.j2-ultra',\n",
       "   'modelName': 'Jurassic-2 Ultra',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-ultra-v1',\n",
       "   'modelId': 'ai21.j2-ultra-v1',\n",
       "   'modelName': 'Jurassic-2 Ultra',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1',\n",
       "   'modelId': 'anthropic.claude-instant-v1',\n",
       "   'modelName': 'Claude Instant',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v1',\n",
       "   'modelId': 'anthropic.claude-v1',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2',\n",
       "   'modelId': 'anthropic.claude-v2',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-text-v14',\n",
       "   'modelId': 'cohere.command-text-v14',\n",
       "   'modelName': 'Command',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-light-text-v14',\n",
       "   'modelId': 'cohere.command-light-text-v14',\n",
       "   'modelName': 'Command Light',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-english-v3',\n",
       "   'modelId': 'cohere.embed-english-v3',\n",
       "   'modelName': 'Embed English',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-multilingual-v3',\n",
       "   'modelId': 'cohere.embed-multilingual-v3',\n",
       "   'modelName': 'Embed Multilingual',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1',\n",
       "   'modelId': 'meta.llama2-13b-chat-v1',\n",
       "   'modelName': 'Llama 2 Chat 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-chat-v1',\n",
       "   'modelId': 'meta.llama2-70b-chat-v1',\n",
       "   'modelName': 'Llama 2 Chat 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND']},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-v1',\n",
       "   'modelId': 'meta.llama2-13b-v1',\n",
       "   'modelName': 'Llama 2 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': []},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-v1',\n",
       "   'modelId': 'meta.llama2-70b-v1',\n",
       "   'modelName': 'Llama 2 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': []}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock.list_foundation_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## `InvokeModel` 본문과 출력\n",
    "\n",
    "아마존 베드록 런타임 클라이언트의 `invoke_model()` 메서드(`InvokeModel` API)는 어떤 모델을 사용하든 대부분의 텍스트 생성 및 처리 작업에 주로 사용하는 메서드가 될 것입니다.\n",
    "\n",
    "메서드는 공유되지만, 입력과 출력의 형식은 사용하는 파운데이션 모델에 따라 다릅니다. 이는 아래에 설명된 바와 같습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "a4650fa3-a831-4039-9fd6-749926d35979",
   "metadata": {},
   "source": [
    "### Amazon Titan Large\n",
    "\n",
    "#### Input\n",
    "```json\n",
    "{   \n",
    "    \"inputText\": \"<prompt>\",\n",
    "    \"textGenerationConfig\" : { \n",
    "        \"maxTokenCount\": 512,\n",
    "        \"stopSequences\": [],\n",
    "        \"temperature\": 0.1,  \n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"inputTextTokenCount\": 613,\n",
    "    \"results\": [{\n",
    "        \"tokenCount\": 219,\n",
    "        \"outputText\": \"<output>\"\n",
    "    }]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0c329-ff58-483e-8abd-63867950c14b",
   "metadata": {},
   "source": [
    "### AI21 Jurassic (Grande and Jumbo) \n",
    "\n",
    "#### Input\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"<prompt>\",\n",
    "    \"maxTokens\": 200,\n",
    "    \"temperature\": 0.5,\n",
    "    \"topP\": 0.5,\n",
    "    \"stopSequences\": [],\n",
    "    \"countPenalty\": {\"scale\": 0},\n",
    "    \"presencePenalty\": {\"scale\": 0},\n",
    "    \"frequencyPenalty\": {\"scale\": 0}\n",
    "}\n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": 1234,\n",
    "    \"prompt\": {\n",
    "        \"text\": \"<prompt>\",\n",
    "        \"tokens\": [\n",
    "            {\n",
    "                \"generatedToken\": {\n",
    "                    \"token\": \"\\u2581who\\u2581is\",\n",
    "                    \"logprob\": -12.980147361755371,\n",
    "                    \"raw_logprob\": -12.980147361755371\n",
    "                },\n",
    "                \"topTokens\": null,\n",
    "                \"textRange\": {\"start\": 0, \"end\": 6}\n",
    "            },\n",
    "            //...\n",
    "        ]\n",
    "    },\n",
    "    \"completions\": [\n",
    "        {\n",
    "            \"data\": {\n",
    "                \"text\": \"<output>\",\n",
    "                \"tokens\": [\n",
    "                    {\n",
    "                        \"generatedToken\": {\n",
    "                            \"token\": \"<|newline|>\",\n",
    "                            \"logprob\": 0.0,\n",
    "                            \"raw_logprob\": -0.01293118204921484\n",
    "                        },\n",
    "                        \"topTokens\": null,\n",
    "                        \"textRange\": {\"start\": 0, \"end\": 1}\n",
    "                    },\n",
    "                    //...\n",
    "                ]\n",
    "            },\n",
    "            \"finishReason\": {\"reason\": \"endoftext\"}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb6bee-7654-4269-9127-9afa4e823454",
   "metadata": {},
   "source": [
    "### Anthropic Claude\n",
    "\n",
    "#### Input\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"\\n\\nHuman:<prompt>\\n\\nAnswer:\",\n",
    "    \"max_tokens_to_sample\": 300,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"completion\": \"<output>\",\n",
    "    \"stop_reason\": \"stop_sequence\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb94aa2f-30da-499b-b3f5-02f102dbb1ea",
   "metadata": {},
   "source": [
    "### Stability AI Stable Diffusion XL\n",
    "\n",
    "#### Input\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"text_prompts\": [\n",
    "        {\"text\": \"this is where you place your input text\"}\n",
    "    ],\n",
    "    \"cfg_scale\": 10,\n",
    "    \"seed\": 0,\n",
    "    \"steps\": 50\n",
    "}\n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "```json\n",
    "{ \n",
    "    \"result\": \"success\", \n",
    "    \"artifacts\": [\n",
    "        {\n",
    "            \"seed\": 123, \n",
    "            \"base64\": \"<image in base64>\",\n",
    "            \"finishReason\": \"SUCCESS\"\n",
    "        },\n",
    "        //...\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f4adca-cfc4-439b-84b7-e528398684e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Common inference parameter definitions\n",
    "\n",
    "### Randomness and Diversity\n",
    "\n",
    "Foundation models support the following parameters to control randomness and diversity in the \n",
    "response.\n",
    "\n",
    "**Temperature** – Large language models use probability to construct the words in a sequence. For any \n",
    "given next word, there is a probability distribution of options for the next word in the sequence. When \n",
    "you set the temperature closer to zero, the model tends to select the higher-probability words. When \n",
    "you set the temperature further away from zero, the model may select a lower-probability word.\n",
    "\n",
    "In technical terms, the temperature modulates the probability density function for the next tokens, \n",
    "implementing the temperature sampling technique. This parameter can deepen or flatten the density \n",
    "function curve. A lower value results in a steeper curve with more deterministic responses, and a higher \n",
    "value results in a flatter curve with more random responses.\n",
    "\n",
    "**Top K** – Temperature defines the probability distribution of potential words, and Top K defines the cut \n",
    "off where the model no longer selects the words. For example, if K=50, the model selects from 50 of the \n",
    "most probable words that could be next in a given sequence. This reduces the probability that an unusual \n",
    "word gets selected next in a sequence.\n",
    "In technical terms, Top K is the number of the highest-probability vocabulary tokens to keep for Top-\n",
    "K-filtering - This limits the distribution of probable tokens, so the model chooses one of the highest-\n",
    "probability tokens.\n",
    "\n",
    "**Top P** – Top P defines a cut off based on the sum of probabilities of the potential choices. If you set Top \n",
    "P below 1.0, the model considers the most probable options and ignores less probable ones. Top P is \n",
    "similar to Top K, but instead of capping the number of choices, it caps choices based on the sum of their \n",
    "probabilities.\n",
    "For the example prompt \"I hear the hoof beats of ,\" you may want the model to provide \"horses,\" \n",
    "\"zebras\" or \"unicorns\" as the next word. If you set the temperature to its maximum, without capping \n",
    "Top K or Top P, you increase the probability of getting unusual results such as \"unicorns.\" If you set the \n",
    "temperature to 0, you increase the probability of \"horses.\" If you set a high temperature and set Top K or \n",
    "Top P to the maximum, you increase the probability of \"horses\" or \"zebras,\" and decrease the probability \n",
    "of \"unicorns.\"\n",
    "\n",
    "### Length\n",
    "\n",
    "The following parameters control the length of the generated response.\n",
    "\n",
    "**Response length** – Configures the minimum and maximum number of tokens to use in the generated \n",
    "response.\n",
    "\n",
    "**Length penalty** – Length penalty optimizes the model to be more concise in its output by penalizing \n",
    "longer responses. Length penalty differs from response length as the response length is a hard cut off for \n",
    "the minimum or maximum response length.\n",
    "\n",
    "In technical terms, the length penalty penalizes the model exponentially for lengthy responses. 0.0 \n",
    "means no penalty. Set a value less than 0.0 for the model to generate longer sequences, or set a value \n",
    "greater than 0.0 for the model to produce shorter sequences.\n",
    "\n",
    "### Repetitions\n",
    "\n",
    "The following parameters help control repetition in the generated response.\n",
    "\n",
    "**Repetition penalty (presence penalty)** – Prevents repetitions of the same words (tokens) in responses. \n",
    "1.0 means no penalty. Greater than 1.0 decreases repetition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce22c308-ebbf-4ef5-a823-832b7c236e31",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try out the models\n",
    "\n",
    "With some theory out of the way, let's see the models in action! Run the cells below to see basic, synchronous example invocations for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a0a79b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json \n",
    "\n",
    "bedrock = boto3.client(service_name=\"bedrock\")\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893872fe-04fa-4f09-9736-6c6173ec1fc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Amazon Titan Express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7df55eed-a3cf-426c-95ea-ec60dade6477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "prompt_data = \"\"\"Command: Write me a blog about making strong business decisions as a leader.\n",
    "\n",
    "Blog:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd2bb671-6b10-4948-9e5e-95d6ced3b86f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a leader, making strong business decisions is crucial for the success of your organization. Here are some key principles to guide you in making informed and effective decisions:\n",
      "\n",
      "Define Your Vision: Before making any decision, it's important to have a clear vision of what you want to achieve. This vision should be shared with your team and should guide all of your decision-making processes.\n",
      "\n",
      "Gather Information: Gather as much information as possible about the decision you need to make. This includes data, market trends, customer feedback, and internal metrics. Use this information to inform your decision-making process.\n",
      "\n",
      "Consider Alternatives:\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "    body = json.dumps({\"inputText\": prompt_data})\n",
    "    modelId = \"amazon.titan-text-express-v1\"\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    print(response_body.get(\"results\")[0].get(\"outputText\"))\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    \n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c0fe6-576a-4380-89aa-726bab5d65ff",
   "metadata": {},
   "source": [
    "### Anthropic Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a725de2-bdea-4d86-b12d-d1d7cdda010b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "prompt_data = \"\"\"Human: Write me a blog about making strong business decisions as a leader.\n",
    "\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ba33ac0-fa16-4c4f-b882-e838d0cb5830",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a draft blog post on making strong business decisions as a leader:\n",
      "\n",
      "Making Strong Business Decisions as a Leader  \n",
      "\n",
      "As a leader in business, one of the most important skills is being able to make strong, thoughtful decisions that will positively impact the company and employees. Here are some tips for making effective business decisions:\n",
      "\n",
      "- Gather all relevant information. Don't make a decision without first doing thorough research and gathering facts, data, and perspectives from different sources. Make sure you understand the full context before deciding.\n",
      "\n",
      "- Weigh pros and cons. Look at the potential benefits and risks of each option. Think critically about all possible outcomes, both positive and negative. Write out pros and cons lists if it helps visualize. \n",
      "\n",
      "- Consult experts and team members. Get input from people with expertise in the area as well as those who will be impacted by the decision. Different perspectives lead to stronger decisions.\n",
      "\n",
      "- Take your time. Don't feel rushed into making a quick decision unless absolutely necessary. Give yourself time to reflect and carefully consider. Sleep on it if you're stuck.\n",
      "\n",
      "- Trust your intuition. After gathering information and input, go with your gut instinct. Your subconscious often knows best, especially as a seasoned leader.\n",
      "\n",
      "- Decide and commit. Once you've made your decision, commit to it fully. Second-guessing yourself will only lead to confusion and delayed action.\n",
      "\n",
      "- Learn from results. Study the outcome of your decision, both positive and negative. Reflect on what went well or poorly to make better choices next time.\n",
      "\n",
      "- Adjust when necessary. If results show your decision is failing, be willing to acknowledge that and course-correct quickly. Adapting shows strength as a leader.\n",
      "\n",
      "Making business decisions is challenging, but being thoughtful and strategic will lead to positive outcomes. Trust in your ability to analyze information and choose wisely. The more practice you gain in decision-making, the stronger your judgement will become over time.\n"
     ]
    }
   ],
   "source": [
    "body = json.dumps({\"prompt\": prompt_data, \"max_tokens_to_sample\": 500})\n",
    "modelId = \"anthropic.claude-v2\"  # change this to use a different version from the model provider\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    print(response_body.get(\"completion\"))\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    \n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e3144-c6df-400d-aab1-1540614dbbde",
   "metadata": {},
   "source": [
    "### AI21 Jurassic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c02d1585-945e-45d1-99d2-171e956138f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making strong business decisions as a leader is an essential skill for any professional in a position of authority. These decisions can have far-reaching consequences, affecting not only the immediate outcomes but also the overall trajectory and success of the organization. Here are some key tips to keep in mind when making business decisions as a leader:\n",
      "\n",
      "1. Understand the problem: Before making a decision, it's important to thoroughly understand the problem or opportunity at hand. This means taking the time to gather and analyze all available information, including market trends, customer insights, and financial data. Only by understanding the situation can you accurately identify the best solution and judge the potential consequences of different choices.\n",
      "2. Consider multiple options: Instead of jumping to conclusions or making impulsive decisions, consider multiple options and take a balanced view of the situation. This can be accomplished by conducting a cost-benefit analysis or by running simulations of potential scenarios. By exploring a range of options, you can gain a better understanding of the pros and cons of each option and make an informed decision.\n",
      "3. Seek input from others: Sometimes, a fresh perspective can provide the insight you need to make better decisions. Seek out input from colleagues, team members, and other stakeholders who may be able to provide valuable insights and alternative solutions. By involving others in the process, you can foster a sense of shared ownership and commitment to the decisions that are made.\n",
      "4. Be open to compromise: Leadership means making difficult trade-offs and being willing to compromise when necessary. Instead of\n"
     ]
    }
   ],
   "source": [
    "body = json.dumps({\"prompt\": prompt_data, \"maxTokens\": 200})\n",
    "modelId = \"ai21.j2-mid-v1\"  # change this to use a different version from the model provider\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    print(response_body.get(\"completions\")[0].get(\"data\").get(\"text\"))\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    \n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc498bea",
   "metadata": {},
   "source": [
    "### Stability Stable Diffusion XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "173e51a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "iVBORw0KGgoAAAANSUhEUgAAAgAAAAIACAIAAAB7GkOtAAABrGVYSWZNTQAqAAAACAAGAQAABAAAAAEA...\n"
     ]
    }
   ],
   "source": [
    "prompt_data = \"a fine image of an astronaut riding a horse on Mars\"\n",
    "body = json.dumps({\n",
    "    \"text_prompts\": [{\"text\": prompt_data}],\n",
    "    \"cfg_scale\": 10,\n",
    "    \"seed\": 20,\n",
    "    \"steps\": 50\n",
    "})\n",
    "modelId = \"stability.stable-diffusion-xl\"\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    print(response_body[\"result\"])\n",
    "    print(f'{response_body.get(\"artifacts\")[0].get(\"base64\")[0:80]}...')\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    \n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a271fa6-13fd-480a-87a5-3702d29a5c43",
   "metadata": {},
   "source": [
    "**Note:** The output is a [base64 encoded](https://docs.python.org/3/library/base64.html) string of the image data. You can use any image processing library (such as [Pillow](https://pillow.readthedocs.io/en/stable/)) to decode the image as in the example below:\n",
    "\n",
    "```python\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "base_64_img_str = response_body.get(\"artifacts\")[0].get(\"base64\")\n",
    "image = Image.open(io.BytesIO(base64.decodebytes(bytes(base_64_img_str, \"utf-8\"))))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4621a301-53e4-4182-9fce-8ee422813e9d",
   "metadata": {},
   "source": [
    "## Generate streaming output\n",
    "\n",
    "For large language models, it can take noticeable time to generate long output sequences. Rather than waiting for the entire response to be available, latency-sensitive applications may like to **stream** the response to users.\n",
    "\n",
    "Run the code below to see how you can achieve this with Bedrock's `invoke_model_with_response_stream()` method - returning the response body in separate chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c69627e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "This LLM does not have access to the Internet and cannot access any images. It has been trained to engage in conversation and generate responses to your questions."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output, display, display_markdown, Markdown\n",
    "\n",
    "body = json.dumps({\"inputText\": prompt_data})\n",
    "modelId = \"amazon.titan-text-express-v1\"  # (Change this, and the request body, to try different models)\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    stream = response.get('body')\n",
    "    output = []\n",
    "\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if chunk:\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                text = chunk_obj['outputText']\n",
    "                clear_output(wait=True)\n",
    "                output.append(text)\n",
    "                display_markdown(Markdown(''.join(output)))\n",
    "            \n",
    "except botocore.exceptions.ClientError as error:\n",
    "    \n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf097eb-1bfa-41f1-8135-cdd4ad6e9983",
   "metadata": {},
   "source": [
    "### Anthropic Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd800ef5-78f7-43d4-b73e-c37c7609cb40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "prompt_data = \"\"\"Human: Write me a blog about making strong business decisions as a leader.\n",
    "\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be7fbbe8-78b8-4071-b80c-3f954185fad8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Here is a draft blog post on making strong business decisions as a leader:\n",
       "\n",
       "## How to Make Strong Business Decisions as a Leader\n",
       "\n",
       "Being a leader means you will constantly be faced with important business decisions that could impact your company's future. While making choices is an inevitable part of any job, it's important as a leader to make the right decisions. Here are some tips for making wise, impactful business decisions:\n",
       "\n",
       "**Look at the big picture.** When considering options, think about how each decision will affect your long-term goals and overall company strategy. Avoid getting bogged down in short-term wins if they compromise where you want the business to be in 5 or 10 years. \n",
       "\n",
       "**Gather all relevant data.** Do thorough research before committing to a path. Look at past performance metrics, industry trends, customer feedback, competitive analyses - anything that could provide useful context. Don't rely on assumptions or gut feelings alone.\n",
       "\n",
       "**Involve the right people.** Bring in others who have valuable experience or stake in the outcome. Get input from employees, partners, advisors - groups impacted by or knowledgeable about the decision topic. Diverse perspectives help uncover aspects you may have overlooked.\n",
       "\n",
       "**Consider risks and downsides.** Every choice has potential pros and cons. Carefully weigh what could go wrong as well as what could go right. Think through contingency plans in case things don't progress as hoped. Being aware of vulnerabilities allows for smarter risk mitigation.\n",
       "\n",
       "**Trust your instincts - to an extent.** Valuable intuition comes from accumulated knowledge and experience. If initial analyses point one way but your gut says otherwise, reexamine assumptions. However, truly major decisions still require solid rationale; intuition alone isn't enough. \n",
       "\n",
       "**Be willing to change course if needed.** Committing to a path doesn't mean staying rigid if new information arises. Decisive leadership requires acting, then assessing outcomes and adapting as situations evolve. Don't be afraid to course-correct if monitoring suggests the initial decision isn't working out as planned.\n",
       "\n",
       "Making strong choices is a key job duty of any business leader. By thoroughly researching options, considering various perspectives, properly managing risks, and maintaining flexibility, you can make the best possible decisions to propel your company forward."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import botocore\n",
    "from IPython.display import clear_output, display, display_markdown, Markdown\n",
    "\n",
    "body = json.dumps({\"prompt\": prompt_data, \"max_tokens_to_sample\": 500})\n",
    "modelId = \"anthropic.claude-instant-v1\"  # (Change this to try different model versions)\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    stream = response.get('body')\n",
    "    output = []\n",
    "\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if chunk:\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                text = chunk_obj['completion']\n",
    "                clear_output(wait=True)\n",
    "                output.append(text)\n",
    "                display_markdown(Markdown(''.join(output)))\n",
    "            \n",
    "except botocore.exceptions.ClientError as error:\n",
    "    \n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef3451d-b66a-4b11-a1ed-734bf9e7bbec",
   "metadata": {},
   "source": [
    "## Generate embeddings\n",
    "\n",
    "Use text embeddings to convert text into meaningful vector representations. You input a body of text \n",
    "and the output is a (1 x n) vector. You can use embedding vectors for a wide variety of applications. \n",
    "Bedrock currently offers Titan Embeddings for text embedding that supports text similarity (finding the \n",
    "semantic similarity between bodies of text) and text retrieval (such as search).\n",
    "\n",
    "At the time of writing you can use `amazon.titan-embed-g1-text-02` as embedding model via the API. The input text size is 8192 tokens and the output vector length is 1536.\n",
    "\n",
    "To use a text embeddings model, use the InvokeModel API operation or the Python SDK.\n",
    "Use InvokeModel to retrieve the vector representation of the input text from the specified model.\n",
    "\n",
    "\n",
    "\n",
    "#### Input\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"inputText\": \"<text>\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"embedding\": []\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645dbd8",
   "metadata": {},
   "source": [
    "Let's see how to generate embeddings of some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1085cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = \"Amazon Bedrock supports foundation models from industry-leading providers such as \\\n",
    "AI21 Labs, Anthropic, Stability AI, and Amazon. Choose the model that is best suited to achieving \\\n",
    "your unique goals.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c54b424",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding vector has 1536 values\n",
      "[0.16601562, 0.23632812, 0.703125, '...', 0.26953125, -0.609375, -0.55078125]\n"
     ]
    }
   ],
   "source": [
    "body = json.dumps({\"inputText\": prompt_data})\n",
    "modelId = \"amazon.titan-embed-text-v1\"  # (Change this to try different embedding models)\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    print(f\"The embedding vector has {len(embedding)} values\\n{embedding[0:3]+['...']+embedding[-3:]}\")\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    \n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48a0e8-147d-4525-a6b2-68a09af1b2c4",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In this notebook we showed some basic examples of invoking Amazon Bedrock models using the AWS Python SDK. You're now ready to explore the other labs to dive deeper on different use-cases and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb76df-4e99-4ebe-a954-53992ad317dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
