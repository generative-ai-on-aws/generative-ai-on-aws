{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 세이지메이커 모델 병렬성 라이브러리에서 Sharded Data Parallelism 기법을 활용해 Falcon 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 노트북에서는 허깅 페이스 트랜스포머 [Falcon](https://huggingface.co/docs/transformers/main/model_doc/falcon) 모델을 파이토치 2.0과 [세이지메이커의 모델 병렬성 라이브러리 (SMP)](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html)에서 지원하는 [샤딩 데이터 병렬성(Sharded Data Parallelism)](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-sharded-data-parallelism.html) 기법을 활용해 [GLUE/SST2 데이터 세트](https://huggingface.co/datasets/glue/viewer/sst2/train)로 학습하는 방법을 배울 수 있습니다.\n",
    "\n",
    "샤딩 데이터 병렬성은 모델 파라미터, 기울기, 옵티마이저 상태를 데이터 병렬 그룹의 GPU 간에 분산시키는 분산 학습 기법입니다. 이 기법은 극단적으로 큰 모델을 위해 설계되었으며, 아마존의 자체 기술인 [MiCS](https://arxiv.org/pdf/2205.00119.pdf)를 활용해 거의 선형적인 확장 효율성을 달성합니다. 단일 GPU에 맞지 않는 대형 모델의 경우, 먼저 SMP안에서 샤딩 데이터 병렬성 기법과 함께 [Activation Checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-checkpointing.html) 및 [Activation Offloading](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-offloading.html)을 활용하는 것을 추천합니다. 이후에는 텐서 병렬성 또는 파이프라인 병렬성과 같은 다른 기법을 고려할 수 있습니다.\n",
    "\n",
    "이 기능은 [텐서 병렬성](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-tensor-parallelism.html)과도 호환됩니다.\n",
    "\n",
    "이 노트북에는 다음 파일이 포함되어 있습니다.\n",
    "\n",
    "- `train.py`: 세이지메이커 파이토치 에스티메이터에 전달될 엔트리 포인트 스크립트입니다. 이 스크립트는 SMP와 함께 Falcon 모델의 종단 간 학습을 실행하며, 샤딩 데이터 병렬성 설정이 적용된 코드로 모델을 저장, 로드 및 미세 조정하는 기능이 구현되어 있습니다.\n",
    "- `data_pipeline.py`: 학습 데이터 세트를 준비하기 위한 데이터 파이프라인 함수가 포함되어 있습니다.\n",
    "- `learining_rate.py`: 학습률 스케줄을 위한 함수가 포함되어 있습니다.\n",
    "- `requirements.txt`: 허깅 페이스 트랜스포머를 포함한 의존성을 설치합니다.\n",
    "- `memory_tracker.py`: 메모리 사용량을 추적하는 함수가 포함되어 있습니다.\n",
    "- `model_config.py`: 모델 구성 정보를 가져오는 함수가 포함되어 있습니다.\n",
    "- `sdp_utils.py`: 샤딩 데이터 병렬성을 위한 유틸리티 함수가 포함되어 있습니다.\n",
    "\n",
    "### 추가 자료\n",
    "- 세이지메이커 모델 병렬성 라이브러리에 대한 더 자세한 내용은 [세이지메이커 분산을 활용한 모델 병렬 분산 학습](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html) 문서에서 확인할 수 있습니다.\n",
    "\n",
    "- 세이지메이커 파이썬 SDK를 파이토치와 함께 활용하는 방법에 대한 더 자세한 내용은 [세이지메이커 파이썬 SDK와 파이토치 활용](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html) 문서에서 확인할 수 있습니다.\n",
    "\n",
    "- 자게 학습 이미지를 활용해 아마존 세이지메이커에서 학습 작업을 시작하는 방법에 대한 더 자세한 내용은 [자체 학습 알고리즘 활용](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html) 문서에서 확인할 수 있습니다.\n",
    "\n",
    "- 샤딩 데이터 병렬성에 대한 더 자세한 내용은 [샤딩 데이터 병렬성](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-sharded-data-parallelism.html) 문서나 블로그 [AWS에서 거대 모델 학습의 선형적 확장](https://www.amazon.science/blog/near-linear-scaling-of-gigantic-model-training-on-aws)에서 확인할 수 있습니다.\n",
    "\n",
    "### 사전 요구 사항\n",
    "학습을 위한 입력 데이터를 저장할 S3 버킷을 생성해야 합니다. 이 버킷은 학습 작업을 시작할 AWS 리전과 동일한 리전에 있어야 합니다. S3 버킷을 생성하는 방법에 대한 더 자세한 내용은 *아마존 S3 문서*의 [첫 번째 S3 버킷 생성](https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html)에서 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아마존 세이지메이커 초기화\n",
    "\n",
    "다음 셀을 실행하여 세이지메이커 모듈을 가져오고 현재 세이지메이커 작업 환경에 대한 정보를 조회합니다. 여기에는 AWS 계정 ID, AWS 리전, 아마존 세이지메이커 실행 역할의 ARN이 포함됩니다. 세이지메이커 SDK를 최신 버전으로 업그레이드합니다.\n",
    "\n",
    "**참고:** 이 단계에서는 커널 재시작이 필요할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade sagemaker\n",
    "%pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # 새 역할을 생성하는 대신 기존 역할 ARN 제공\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account: {account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region: {region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "# 기본 버킷 가져오기\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print()\n",
    "print(\"Default bucket for this session: \", default_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLUE/SST2 데이터 다운로드 및 준비\n",
    "여기에서는 GLUE/SST2 데이터 세트를 다운로드하고 준비한 후, 파일을 S3에 복사합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 허깅 페이스 트랜스포머와 데이터 세트 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q datasets transformers==4.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, load_from_disk, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import transformers\n",
    "import logging\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from transformers.testing_utils import CaptureLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로드\n",
    "이 섹션에서는 [GLUE/SST2](https://huggingface.co/datasets/glue/viewer/sst2/train) 데이터 세트를 로드하고 학습 및 검증 데이터 세트로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"dataset_name\": \"glue\",\n",
    "    \"dataset_config_name\": \"sst2\",\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"cache_dir\": \"tmp\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\n",
    "    hyperparameters[\"dataset_name\"],\n",
    "    hyperparameters[\"dataset_config_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"validation\" not in raw_datasets.keys():\n",
    "    raw_datasets[\"validation\"] = load_dataset(\n",
    "        hyperparameters[\"dataset_name\"],\n",
    "        hyperparameters[\"dataset_config_name\"],\n",
    "        split=\"train[:5%]\",\n",
    "        cache_dir=hyperparameters[\"cache_dir\"],\n",
    "    )\n",
    "\n",
    "    raw_datasets[\"train\"] = load_dataset(\n",
    "        hyperparameters[\"dataset_name\"],\n",
    "        hyperparameters[\"dataset_config_name\"],\n",
    "        split=\"train[5%:]\",\n",
    "        cache_dir=hyperparameters[\"cache_dir\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토크나이저 로드\n",
    "\n",
    "거의 모든 NLP 작업은 토크나이저로 시작합니다. 토크나이저는 텍스트 데이터를 NLP 모델이 처리할 수 있는 형식(토큰)으로 변환합니다.\n",
    "다음 셀에서는 [AutoTokenizer.from_pretrained()](https://huggingface.co/docs/transformers/v4.19.4/en/autoclass_tutorial#autotokenizer)를 활용해 Falcon 모델의 토크나이저를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": hyperparameters[\"cache_dir\"],\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"tiiuae/falcon-40b\", trust_remote_code=True, **tokenizer_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "\n",
    "다음 두 셀에서는 토크나이저를 실행하고 텍스트를 블록 크기보다 작은 청크로 그룹화하는 함수를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
    "\n",
    "    with CaptureLogger(tok_logger) as cl:\n",
    "        output = tokenizer(examples[text_column_name])\n",
    "        # clm 입력은 블록 크기보다 훨씬 더 길 수 있습니다.\n",
    "        if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "            tok_logger.warning(\n",
    "                \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\"\n",
    "            )\n",
    "    return output\n",
    "\n",
    "\n",
    "# 데이터 세트의 모든 텍스트를 연결하고 블록 크기의 청크를 생성하는 주요 데이터 처리 함수입니다.\n",
    "def group_texts(examples):\n",
    "    # 모든 텍스트를 연결합니다.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # 작은 나머지는 삭제합니다. 모델이 패딩을 지원하면 이 삭제 대신 패딩을 추가할 수 있습니다.\n",
    "    # 필요에 따라 이 부분을 사용자 맞춤 설정할 수 있습니다.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        # max_len으로 청크로 분할합니다.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "# _LazyModule 오류를 피하기 위해 이 함수를 피클링할 때 토크나이즈 함수 전에 로거 로딩 강제\n",
    "tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "\n",
    "block_size = tokenizer.model_max_length\n",
    "if block_size > 1024:\n",
    "    logger.warning(\n",
    "        f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "        \"Picking 1024 instead. You can change that default value by passing --block_size xxx.\"\n",
    "    )\n",
    "    block_size = 1024\n",
    "else:\n",
    "    if block_size > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The block_size passed ({block_size}) is larger than the maximum length for the model\"\n",
    "            f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    block_size = min(block_size, tokenizer.model_max_length)\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    #     num_proc=args.preprocessing_num_workers,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 작업의 각 에포크에서 학습 및 검증 데이터 세트를 적절히 매핑할 수 있도록 추가 하이퍼파라미터 및 S3 경로를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparameters[\"do_train\"]:\n",
    "    if \"train\" not in tokenized_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = lm_datasets[\"train\"]\n",
    "\n",
    "\n",
    "if hyperparameters[\"do_eval\"]:\n",
    "    if \"validation\" not in tokenized_datasets:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    eval_dataset = lm_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_location = None\n",
    "validation_dataset_location = None\n",
    "\n",
    "\n",
    "if hyperparameters[\"do_train\"]:\n",
    "    train_dataset.to_json(\"./training.json\")\n",
    "    training_dataset_location = \"s3://{}/dataset/train/\".format(default_bucket)\n",
    "\n",
    "if hyperparameters[\"do_eval\"]:\n",
    "    eval_dataset.to_json(\"./validation.json\")\n",
    "    validation_dataset_location = \"s3://{}/dataset/validation/\".format(default_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_dataset_location is not None:\n",
    "    command = \"aws s3 cp ./training.json {}\".format(training_dataset_location)\n",
    "    os.system(command)\n",
    "\n",
    "if validation_dataset_location is not None:\n",
    "    command = \"aws s3 cp ./validation.json {}\".format(validation_dataset_location)\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparameters[\"do_train\"]:\n",
    "    command = \"rm ./training.json\"\n",
    "    os.system(command)\n",
    "\n",
    "if hyperparameters[\"do_eval\"]:\n",
    "    command = \"rm ./validation.json\"\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store training_dataset_location\n",
    "%store validation_dataset_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아마존 S3 버킷 경로 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서는 작업에 활용할 학습 데이터 경로를 지정해야 합니다. 사용되는 버킷은 학습이 실행될 리전과 동일해야 합니다. 위의 셀에서 GLUE/SST2 학습 및 검증 분할 데이터 세트를 다운로드하고 JSON 파일을 S3 버킷에 업로드했습니다. 이 예제는 이 JSON 파일에서 학습을 진행합니다.\n",
    "\n",
    "예제 텐서 병렬 학습 작업이 성공적으로 실행된 후, 자신만의 데이터 세트가 저장된 S3 버킷으로 수정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r training_dataset_location\n",
    "%store -r validation_dataset_location\n",
    "\n",
    "# 자체 데이터를 사용하는 경우, 아래 줄의 주석을 제거하고 위치를 지정할 수 있습니다.\n",
    "# training_dataset_location = YOUR_S3_BUCKET/training\n",
    "# validation_dataset_location = YOUR_S3_BUCKET/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_bucket = training_dataset_location\n",
    "s3_test_bucket = validation_dataset_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 S3 버킷은 학습 작업의 출력 아티팩트를 저장합니다. 필요에 따라 수정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_bucket = f\"s3://sagemaker-{region}-{account}/smp-tensorparallel-outputdir/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아마존 S3로 세이지메이커 학습 데이터 채널 정의\n",
    "\n",
    "이 단계에서는 세이지메이커 학습 데이터 채널을 S3 버킷에 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_fsx 기본값은 False로 설정\n",
    "# fsx를 사용하려는 경우, 아래 변수를 True로 설정 (다음 셀 참조)\n",
    "use_fsx = False\n",
    "if not use_fsx:\n",
    "    if s3_train_bucket != None:\n",
    "        train = sagemaker.inputs.TrainingInput(\n",
    "            s3_train_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "        )\n",
    "        data_channels = {\"train\": train}\n",
    "    else:\n",
    "        data_channels = {\"train\": mock_data}\n",
    "    if s3_test_bucket != None:\n",
    "        test = sagemaker.inputs.TrainingInput(\n",
    "            s3_test_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "        )\n",
    "        data_channels[\"test\"] = test\n",
    "    else:\n",
    "        data_channels[\"test\"] = mock_data\n",
    "    print(data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (선택 사항) 아마존 FSx를 데이터 채널 및 체크포인트 사용 설정\n",
    "\n",
    "이전 옵션인 아마존 S3 사용은 설정이 더 쉬우나, 대용량 입력 및 모델 사이즈를 처리할 때 FSx를 사용하는 것이 성능에 유리할 수 있습니다. 13B 이상의 모델을 사용하는 경우 체크포인팅은 FSx를 사용해야 합니다.\n",
    "\n",
    "FSx Lustre 파일 시스템을 생성하고 S3 버킷에서 FSx 파일 시스템으로 데이터 세트를 가져오는 방법은 [아마존 세이지메이커에서 FSx를 사용한 Mask-RCNN의 분산 학습](https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/distributed_tensorflow_mask_rcnn/mask-rcnn-scriptmode-fsx.ipynb)에서 확인할 수 있습니다. FSx 파일 시스템은 학습 작업이 인터넷에 접근할 수 있도록 인터넷 게이트웨이가 있는 프라이빗 서브넷에서 생성돼야 합니다. FSx Lustre 파일 시스템을 데이터 입력 채널로 설정하는 방법에 대한 일반적인 안내는 [아마존 FSx Lustre를 사용하는 데이터 입력 채널 구성 문서](https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html#model-access-training-data-fsx)에서 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음에서 얻은 지침:\n",
    "# https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/distributed_tensorflow_mask_rcnn/mask-rcnn-scriptmode-fsx.ipynb\n",
    "\n",
    "if use_fsx:\n",
    "    from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "    # FSx Lustre 파일 시스템 ID를 지정합니다.\n",
    "    file_system_id = \"<your-file-system-id>\"\n",
    "\n",
    "    # FSx에 활용된 보안 그룹(SG)과 서브넷을 지정합니다. 이러한 정보는 세이지메이커 에스티메이터에 전달돼 작업에서 활용됩니다.\n",
    "    fsx_security_group_id = \"<your-security-group-id>\"\n",
    "    fsx_subnet = \"<your-subnet>\"\n",
    "\n",
    "    # 파일 시스템의 입력 데이터 디렉터리 경로를 지정합니다.\n",
    "    # 아래에 정규화된 절대 경로를 제공해야 합니다.\n",
    "    # fsx 생성 시 또는 자동으로 생성된 마운트 이름을 사용할 수 있습니다.\n",
    "    # 이 마운트 이름은 콘솔의 FSX 페이지에서 확인할 수 있습니다.\n",
    "    # fsx에서 생성된 마운트 이름 예: \"3x5lhbmv\"\n",
    "    base_path = \"<your-mount-name>\"\n",
    "\n",
    "    # 파일 시스템 유형을 지정합니다.\n",
    "    file_system_type = \"FSxLustre\"\n",
    "\n",
    "    train = FileSystemInput(\n",
    "        file_system_id=file_system_id,\n",
    "        file_system_type=file_system_type,\n",
    "        directory_path=base_path,\n",
    "        file_system_access_mode=\"rw\",\n",
    "    )\n",
    "\n",
    "    data_channels = {\"train\": train, \"test\": train}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터, 메트릭 정의 및 MPI 옵션 설정\n",
    "\n",
    "다음 `hyperparameters` 딕셔너리는 학습 스크립트(`train.py`)에 인수를 전달하고 학습 작업을 생성할 때 모델 병렬 구성을 설정합니다.\n",
    "\n",
    "커스텀 `mpi` 플래그를 추가할 수도 있습니다. 기본적으로 `--mca btl_vader_single_copy_mechanism none`을 활용해 불필요한 로그를 제거합니다.\n",
    "\n",
    "다음으로, 세이지메이커에서 메트릭 업로드를 활성화하기 위해 기본 메트릭 정의를 추가합니다. 추가적인 메트릭 정의를 추가할 수 있습니다.\n",
    "\n",
    "`sharded_data_parallel_degree` 매개변수를 `hyperparameter` 딕셔너리에 추가하는 점을 주의깊게 확인해야 합니다. 이 매개변수는 세이지메이커 파이토치 에스티메이터를 구성할 때 샤딩 데이터 병렬성을 활성화하는 데 활용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_steps\": 100,\n",
    "    \"seed\": 12345,\n",
    "    \"fp16\": 0,\n",
    "    \"bf16\": 1,\n",
    "    \"lr\": 2.0e-4,\n",
    "    \"lr_decay_iters\": 125000,\n",
    "    \"min_lr\": 0.00001,\n",
    "    \"lr-decay-style\": \"linear\",\n",
    "    \"warmup\": 0.01,\n",
    "    \"num_kept_checkpoints\": 5,\n",
    "    \"checkpoint_freq\": 200,\n",
    "    \"logging_freq\": 1,\n",
    "    \"save_final_full_model\": 0,\n",
    "    \"delayed_param\": 1,\n",
    "    \"offload_activations\": 0,\n",
    "    \"activation_loading_horizon\": 4,\n",
    "    \"gradient_accumulation\": 1,\n",
    "    \"validation_freq\": 200,\n",
    "    \"train_batch_size\": 4,\n",
    "    \"val_batch_size\": 4,\n",
    "    \"zipped_data\": 0,\n",
    "    \"epochs\": 100,\n",
    "    \"use_distributed_transformer\": 0,\n",
    "    \"model_type\": \"falcon\",\n",
    "    # 샤딩 데이터 병렬성을 위한 매개변수\n",
    "    \"sharded_data_parallel_degree\": 16,\n",
    "}\n",
    "\n",
    "if use_fsx:\n",
    "    # fsx 데이터 세트 경로에 따라 training-dir 및 test-dir 경로를 업데이트해야 합니다. \n",
    "    # 학습을 다시 시작하려는 경우 checkpoint-dir를 이전 작업과 동일한 경로로 설정합니다.\n",
    "    SM_TRAIN_DIR = \"/opt/ml/input/data/train\"\n",
    "    hyperparameters[\"checkpoint-dir\"] = f\"{SM_TRAIN_DIR}/checkpointdir-job2\"\n",
    "    hyperparameters[\"model-dir\"] = f\"{SM_TRAIN_DIR}/modeldir-job2\"\n",
    "    hyperparameters[\"training-dir\"] = f\"{SM_TRAIN_DIR}/datasets/pytorch_gpt/train_synthetic\"\n",
    "    hyperparameters[\"test-dir\"] = f\"{SM_TRAIN_DIR}/datasets/pytorch_gpt/val_synthetic\"\n",
    "\n",
    "# 체크포인트 경로 (hyperparameters['checkpoint-dir'] 또는 checkpoint_s3_uri)는 작업마다 고유하지 않습니다.\n",
    "# 서로 다른 실행을 위해 필요에 따라 수정해야 합니다.\n",
    "# 동일한 경로를 관련 없는 실행에 사용할 경우, 불필요한 체크포인트를 다운로드하는 시간이 늘어나고,\n",
    "# 체크포인트를 로드할 때 충돌이 발생할 수 있습니다.\n",
    "\n",
    "mpioptions = \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR \"\n",
    "mpioptions += (\n",
    "    \"-x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 \"\n",
    ")\n",
    "mpioptions += \"-x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\"\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"base_metric\", \"Regex\": \"<><><><><><>\"}\n",
    "]  # 사용자 정의 메트릭 정의를 추가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 구성을 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = \"falcon-7b\"\n",
    "\n",
    "if model_config == \"falcon-7b\":\n",
    "    model_params = {\n",
    "        \"max_context_width\": 2048,\n",
    "        \"hidden_width\": 4544,\n",
    "        \"num_layers\": 32,\n",
    "        \"num_heads\": 71,\n",
    "        \"num_heads_kv\": 71,\n",
    "    }\n",
    "else:\n",
    "    raise RuntimeError(\"Unknown model config\")\n",
    "\n",
    "for k, v in model_params.items():\n",
    "    hyperparameters[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세이지메이커 학습 작업에 대한 필수 매개변수 지정\n",
    "\n",
    "다음으로, [세이지메이커 에스티메이터 클래스](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)를 활용하여 세이지메이커 학습 작업을 정의하며, 학습 작업 이름, EC2 인스턴스 수, 인스턴스 유형 및 인스턴스에 연결된 볼륨 크기와 같은 값을 다음 매개변수를 통해 전달합니다.\n",
    "\n",
    "* `instance_count`\n",
    "* `instance_type`\n",
    "* `volume_size`\n",
    "* `base_job_name`\n",
    "\n",
    "### EC2 인스턴스 유형 및 갯수 업데이트\n",
    "\n",
    "`instance_type` 및 `instance_count` 매개변수에 지정하는 인스턴스 유형과 인스턴스 수는 총 GPU 수(world size)를 결정합니다.\n",
    "\n",
    "$$ \\text{(world size) = (단일 인스턴스의 GPU 수)}\\times\\text{(인스턴스 수)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "\n",
    "instance_count = 2\n",
    "\n",
    "# 해당 인스턴스의 GPU 수를 설정합니다.\n",
    "processes_per_host = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 인스턴스 유형의 GPU 수를 확인하려면 [아마존 EC2 인스턴스 유형](https://aws.amazon.com/ec2/instance-types/)을 확인합니다. **Accelerated Computing** 섹션에서 일반 목적 GPU 인스턴스를 확인할 수 있습니다. 예를 들어, `p4d.24xlarge`라는 인스턴스 유형은 세이지메이커에서 `ml.p4d.24xlarge`에 해당합니다.\n",
    "세이지메이커에서 지원하는 `ml` 인스턴스와 비용 정보는 [아마존 세이지메이커 비용](https://aws.amazon.com/sagemaker/pricing/)에서 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 작업 이름 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "sharding_degree = hyperparameters[\"sharded_data_parallel_degree\"]\n",
    "base_job_name = (\n",
    "    f'smp-{model_config}-{machine_str}-sdp{sharding_degree}-bs{hyperparameters[\"train_batch_size\"]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_fsx:\n",
    "    # 학습을 다시 시작하려면 checkpoint_s3_uri를 이전 작업과 동일한 경로로 설정합니다.\n",
    "    # 로드할 이전 체크포인트는 동일한 모델 구성이어야 합니다.\n",
    "    checkpoint_bucket = f\"s3://sagemaker-{region}-{account}/\"\n",
    "    checkpoint_s3_uri = (\n",
    "        f\"{checkpoint_bucket}/experiments/gpt_synthetic_simpletrainer_checkpoints/{base_job_name}/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"base_job_name: {base_job_name} checkpoint_s3_uri: {checkpoint_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 세이지메이커 파이토치 에스티메이터 생성\n",
    "\n",
    "다음 셀에서는 위에서 정의한 매개변수로 파이토치 에스티메이터를 생성합니다. 세이지메이커 API와 함수가 스크립트에 어떻게 적용되는지 보려면 `train.py` 파일을 참조하면 좋습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "if use_fsx:\n",
    "    # FSx 파일 시스템을 생성할 때 활용한 보안 그룹과 서브넷 설정\n",
    "    kwargs[\"security_group_ids\"] = [fsx_security_group_id]\n",
    "    kwargs[\"subnets\"] = [fsx_subnet]\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=os.getcwd(),\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": processes_per_host,\n",
    "            \"custom_mpi_options\": mpioptions,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"ddp\": True,\n",
    "                    \"skip_tracing\": True,\n",
    "                    \"delayed_parameter_initialization\": hyperparameters[\"delayed_param\"] > 0,\n",
    "                    \"offload_activations\": hyperparameters[\"offload_activations\"] > 0,\n",
    "                    \"activation_loading_horizon\": hyperparameters[\"activation_loading_horizon\"],\n",
    "                    \"sharded_data_parallel_degree\": hyperparameters[\"sharded_data_parallel_degree\"],\n",
    "                    \"fp16\": hyperparameters[\"fp16\"] > 0,\n",
    "                    \"bf16\": hyperparameters[\"bf16\"] > 0,\n",
    "                    # partitions는 현재 SM SDK에서 필수 매개변수이므로 반드시 전달해야 합니다.\n",
    "                    \"partitions\": 1,\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    framework_version=\"2.0\",\n",
    "    py_version=\"py310\",\n",
    "    output_path=s3_output_bucket,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri if not use_fsx else None,\n",
    "    checkpoint_local_path=hyperparameters[\"checkpoint-dir\"] if use_fsx else None,\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameters=hyperparameters,\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,\n",
    "    base_job_name=base_job_name,\n",
    "    **kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the `estimator.fit` method to launch the SageMaker training job of the Falcon model with sharded data parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "smp_estimator.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 로그 접근\n",
    "\n",
    "학습 로그는 [아마존 클라우드워치](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html)에서 접근할 수 있습니다. 학습 작업 로그가 포함된 출력 스트림을 가진 **algo-1** 노드의 로그를 확인해야 합니다.\n",
    "\n",
    "클라우드워치를 활용해 학습 및 추론 동안 세이지메이커 GPU 및 메모리 사용량을 추적할 수 있습니다. 세이지메이커가 클라우드워치에 기록하는 메트릭 및 로그에 대한 자세한 내용은 아마존 세이지메이커 개발자 가이드의 [세이지메이커 작업 및 엔드포인트 매트릭](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-jobs) 섹션에서 확인할 수 있습니다.\n",
    "\n",
    "클라우드워치를 처음 사용하는 경우, [아마존 클라우드워치 시작하기](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/GettingStarted.html) 문서를 확인하면 좋습니다.\n",
    "\n",
    "아마존 세이지메이커 학습 작업을 모니터링하고 분석하는 추가 정보는 [메트릭을 활용해 학습 작업 모니터링 및 분석하기](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html) 문서에서 확인할 수 있습니다.\n",
    "\n",
    "\n",
    "## 추론을 위한 학습 모델 배포\n",
    "\n",
    "대부분의 경우, 학습된 모델은 추론을 위해 단일 장치에 배포할 수 있습니다. 추론은 적은 메모리만 필요하기 때문입니다. 학습 후에는 SMP API를 활용해 단일 통합 모델을 생성할 수 있습니다: 텐서플로우의 경우 [smp.DistributedModel.save_model()](https://sagemaker.readthedocs.io/en/stable/api/training/smp_versions/latest/smd_model_parallel_tensorflow.html#smp.DistributedModel.save_model) 함수, 파이토치의 경우 [smp.save()](https://sagemaker.readthedocs.io/en/stable/api/training/smp_versions/latest/smd_model_parallel_pytorch.html#apis-for-saving-and-loading) 함수를 사용하면 됩니다.\n",
    "\n",
    "모델을 빌드하고 학습한 후, 다음 두 가지 방법 중 하나로 배포하여 예측 결과를 수신할 수 있습니다.\n",
    "\n",
    "* 모델의 예측 결과를 수신하기 위한 엔드포인트를 설정하려면 세이지메이커 호스팅 서비스를 사용하면 됩니다. 세이지메이커 호스팅 서비스를 활용해 단일 모델 또는 여러 모델을 배포하는 개요는 [세이지메이커 호스팅 서비스에서 모델 배포하기](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html#how-it-works-hosting)에서 확인할 수 있습니다.\n",
    "* 전체 데이터 세트에 대한 예측 결과를 수신하려면 세이지메이커 배치 트랜스폼을 사용하면 됩니다. 세이지메이커 배치 트랜스폼을 활용해 모델을 배포하는 개요는 [배치 트랜스폼으로 전체 데이터 세트에 대한 추론 받기](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)에서 확인할 수 있습니다.\n",
    "\n",
    "세이지메이커를 활용해 추론을 위해 모델을 배포하는 방법에 대해 더 알고 싶다면, [추론을 위한 모델 배포](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html) 문서에서 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "hide_input": false,
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
