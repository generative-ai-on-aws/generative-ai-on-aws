{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e7ec338-4468-4f59-ba9a-16ac395d9945",
   "metadata": {},
   "source": [
    "# Get Started Training Llama 2 with PyTorch FSDP in 5 Minutes\n",
    "\n",
    "_Based on this repo: https://github.com/aws-samples/awsome-distributed-training/_\n",
    "\n",
    "These scripts provide an easy way to get started with multinode [FSDP](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html) training on Slurm. It is designed to be as simple as possible, requires no data preparation, and uses a simple Conda environment. \n",
    "\n",
    "## 0. Prerequisites\n",
    "\n",
    "Before running this training, you'll need to create a Slurm cluster with an FSx for Lustre file system. Instructions can be found in [1.architectures](../../1.architectures).\n",
    "\n",
    "## 1. Create Environment\n",
    "\n",
    "On your cluster head node, \n",
    "1. Navigate to your shared FSx for Lustre file system.\n",
    "* If you followed the tutorial linked above, it will be location at `/fsx`.   \n",
    "2. Clone this repo. \n",
    "\n",
    "```\n",
    "cd /fsx\n",
    "git clone https://github.com/aws-samples/awsome-distributed-training/\n",
    "cd awsome-distributed-training/3.test_cases/10.FSDP\n",
    "```\n",
    "\n",
    "3. Run the `0.create_conda_env.sh` script. \n",
    "* This script will first download and install [Miniconda](https://docs.conda.io/projects/miniconda/en/latest/), then create a Conda env called `pt_fsdp`.\n",
    "\n",
    "```\n",
    "bash 0.create_conda_env.sh\n",
    "```\n",
    "\n",
    "* By creating this environment on the shared FSx for Lustre volume, all compute nodes in our cluster will have access to it.\n",
    "\n",
    "## 2. Data\n",
    "\n",
    "For this example, we'll be using the [C4 dataset](https://huggingface.co/datasets/allenai/c4), which is several hundred gigabytes. Instead of downloading the whole thing, the `create_streaming_dataloaders` function will stream the dataset from [HuggingFace](https://huggingface.co/datasets), so there's no data prep required for running this training. \n",
    "\n",
    "If you'd like to instead use your own dataset, you can do so by [formatting it as a HuggingFace dataset](https://huggingface.co/docs/datasets/create_dataset), and passing its location to the `--dataset_path` argument.\n",
    "\n",
    "## 3. Launch Training\n",
    "\n",
    "The script to launch a Slurm batch training job can be found in `1.distributed_training.sbatch`. You can adjust the number of training nodes by modifying `#SBATCH --nodes=4`. You can also adjust the training parameters in `TRAINING_ARGS`. Additional parameters can be found in `model/arguments.py`. Note that we use the same directory for both `--checkpoint_dir` and `--resume_from_checkpoint`. If there are multiple checkpoints, `--resume_from_checkpoint` will automatically select the most recent one. This way if our training is interupted for any reason, it will automatically pick up the most recent checkpoint.\n",
    "\n",
    "```\n",
    "declare -a TRAINING_ARGS=(\n",
    "    --num_key_value_heads=32 \\\n",
    "    --llama_intermediate_size=11008 \\\n",
    "    --max_context_width=4096 \\\n",
    "    --hidden_width=4096 \\\n",
    "    --num_layers=32 \\\n",
    "    --num_heads=32 \\\n",
    "    --model_type=llama_v2 \\\n",
    "    --checkpoint_freq=1000 \\\n",
    "    --validation_freq=500 \\\n",
    "    --checkpoint_dir=./checkpoints \\\n",
    "    --resume_from_checkpoint=./checkpoints\n",
    ")\n",
    "```\n",
    "\n",
    "To launch your training, run\n",
    "\n",
    "```\n",
    "sbatch 1.distributed_training.sbatch\n",
    "```\n",
    "\n",
    "You'll find a new file in the FSDP directory of the form `slurm-[job-number].out`. This will be continuously updated with your training logs. Don't be worried if you see a long stream of NCCL logs (we prefer to use `NCCL_DEBUG=INFO` for verbose logging). After about a minute, you should see your model training, with an output similar to below.\n",
    "\n",
    "```\n",
    "+ TORCHRUN=./pt_fsdp/bin/torchrun\n",
    "+ export TRAIN_SCRIPT=./train.py\n",
    "+ TRAIN_SCRIPT=./train.py\n",
    "+ TRAINING_ARGS=(--max_context_width=4096 --num_key_value_heads=32 \\ # 7b: 32 13b: 40 70b: 8 --llama_intermediate_size=11008 \\ # 7b: 11008 13b: 13824 70b: 28672 --hidden_width=4096 \\ # 7b: 4096 13b: 5120 70b: 8192 --num_layers=32 \\ # 7b: 32 13b: 40 70b: 80 --num_heads=32 \\ # 7b: 32 13b: 40 70b: 64 --model_type=llama_v2 --checkpoint_freq=50 --validation_freq=500 --checkpoint_dir=./checkpoints --resume_from_checkpoint=./checkpoints)\n",
    "...\n",
    "0: 2023-11-29 04:17:52 I [train.py:175] Creating Model\n",
    "0: 2023-11-29 04:19:17 I [train.py:182] Created model with total parameters: 6889410560 (6.89 B)\n",
    "0: 2023-11-29 04:19:28 I [train.py:209] Wrapped model with FSDP\n",
    "0: 2023-11-29 04:19:28 I [train.py:226] Created optimizer\n",
    "...\n",
    "2: ip-10-1-41-139:6171:8092 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.7.3-aws\n",
    "3: ip-10-1-44-54:6168:6168 [7] NCCL INFO cudaDriverVersion 12020\n",
    "0: ip-10-1-14-81:6158:9214 [2] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\n",
    "...\n",
    "0: ip-10-1-14-81:6158:9214 [2] NCCL INFO comm 0x8b6b550 rank 2 nranks 32 cudaDev 2 busId 201c0 - Init COMPLETE\n",
    "0: ip-10-1-14-81:6157:9213 [1] NCCL INFO comm 0x8494480 rank 1 nranks 32 cudaDev 1 busId 101d0 - Init COMPLETE\n",
    "0: 2023-11-29 04:19:48 I [train.py:122] Batch 0 Loss: 11.6533041, Speed: 3.98 samples/sec, lr: 0.000006\n",
    "0: 2023-11-29 04:19:54 I [train.py:122] Batch 1 Loss: 11.620493, Speed: 10.72 samples/sec, lr: 0.000013\n",
    "0: 2023-11-29 04:20:00 I [train.py:122] Batch 2 Loss: 11.3152923, Speed: 10.71 samples/sec, lr: 0.000019\n",
    "0: 2023-11-29 04:20:06 I [train.py:122] Batch 3 Loss: 10.461415, Speed: 10.11 samples/sec, lr: 0.000025\n",
    "0: 2023-11-29 04:20:12 I [train.py:122] Batch 4 Loss: 11.8934202, Speed: 10.71 samples/sec, lr: 0.000031\n",
    "0: 2023-11-29 04:20:18 I [train.py:122] Batch 5 Loss: 13.9545879, Speed: 10.70 samples/sec, lr: 0.000038\n",
    "```\n",
    "\n",
    "To modify training for a 13 or 70B Llama 2 model, just change the corresponding parameters based on the values in the [Llama 2 paper](https://arxiv.org/abs/2307.09288).\n",
    "\n",
    "| Param                    |     7B      |     13B     |     70B     |\n",
    "| ------------------------ | ----------- | ----------- | ----------- |\n",
    "| llama_intermediate_size  | 11008       | 13824       | 28672       |\n",
    "| num_key_value_heads      | 32          | 40          | 8           |\n",
    "| hidden_width             | 4096        | 5120        | 8192        |\n",
    "| num_layers               | 32          | 40          | 80          |\n",
    "| num_heads                | 32          | 40          | 64          |\n",
    "\n",
    "If you need to cancel or modify your job, see the Slurm commands available in the [Slurm documentation](https://slurm.schedmd.com/quickstart.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f4b66-b5ad-46b4-a184-ad94726c1a34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "This guide assumes that you have the following:\n",
    "* A functional Slurm cluster on AWS. We also assume that Ubuntu AMI is used.\n",
    "* Neuron SDK is installed on the cluster (see [AWS Neuron SDK documentation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx) for the steps).\n",
    "* An FSx for Lustre filesystem mounted on `/fsx`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd579e5-05ca-47bf-9202-47341ccbe94e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Setup Environment\n",
    "First of all, you need to have a Python virtual environment for `torch-neuronx` under `APPS_PATH`.\n",
    "\n",
    "```bash\n",
    "bash 1.setup-venv.sh ${APPS_PATH} # The argument specifies APPS_PATH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d3b38c-d0b9-49f2-b828-29a206ed5777",
   "metadata": {},
   "source": [
    "## 2. `neuronx-nemo-megatron` library need to be installed (and initialized) in the environment.\n",
    "\n",
    "\n",
    "```bash\n",
    "bash 2.setup-neuronx-nemo-megatron.sh ${APPS_PATH} #\n",
    "```\n",
    "You will see the following ERROR line during the script execution. This is safe to ignore.\n",
    "\n",
    "```console\n",
    "+ python3 -c 'from nemo.collections.nlp.data.language_modeling.megatron.dataset_utils import compile_helper; compile_helper()'\n",
    "2023-Nov-18 09:17:45.728072 175272:175272 ERROR  TDRV:tdrv_get_dev_info                       No neuron device available\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e428098-b662-48d8-b8bb-306235b4e26f",
   "metadata": {},
   "source": [
    "## 1. Prepare Llama2 model\n",
    "\n",
    "We recommend that you setup a Slurm cluster using the template in the architectures directory. Before creating the Slurm cluster, you need to setup the following environment variables:\n",
    "\n",
    "```bash\n",
    "export APPS_PATH=/fsx\n",
    "export FSX_PATH=/fsx\n",
    "export MODEL_PATH=/fsx\n",
    "export DATA_PATH=$FSX_PATH/data/books\n",
    "export TEST_CASE_PATH=${APPS_PATH}/awsome-distributed-training/3.test_cases/8.neuronx-nemo-megatron  # where you copy the test case or set to your test case path\n",
    "```\n",
    "\n",
    "This test case requires Llama2 model, which governed by the Meta license and must be downloaded and converted to the standard [Hugging Face](https://huggingface.co/) format prior to running this sample.\n",
    "You can submit access request from [here](https://ai.meta.com/resources/models-and-libraries/llama-downloads/), we need \"Llama 2 & Llama Chat\" to be checked. Use the [download.sh](https://github.com/facebookresearch/llama/blob/main/download.sh) in the official repository. You will be asked to input an URL from the email you recieve from meta.  \n",
    "\n",
    "We will assume that you had placed the model and tokenizer as follows on cluster:\n",
    "\n",
    "```\n",
    "${MODEL_PATH}/Llama2-meta/\n",
    "├── 7B/\n",
    "│   ├── checklist.chk\n",
    "│   ├── consolidated.00.pth\n",
    "│   └── params.json\n",
    "├── tokenizer.model\n",
    "└── tokenizer_checklist.chk\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1190313a-b7f9-4b6e-b39d-79779dc6a5b3",
   "metadata": {},
   "source": [
    "## 3. Convert weights to HuggingFace format\n",
    "To convert the model to the standard Hugging Face format, the following script in transformers can be called with the following (example) command:\n",
    "\n",
    "```\n",
    "sbatch 3.convert-weight.sbatch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e893096-b3b3-4ea1-83d9-4540e30aeb8c",
   "metadata": {},
   "source": [
    "You can check progress of with `tail` command.\n",
    "\n",
    "```\n",
    "$ tail -f slurm-3.convert-weight.sbatch-xxx.out \n",
    "```\n",
    "\n",
    "```console\n",
    "Fetching all parameters from the checkpoint at /fsx/Llama2-meta/7B.\n",
    "Loading the checkpoint in a Llama model.\n",
    "Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.65it/s]\n",
    "...\n",
    "```\n",
    "\n",
    "Once the job completed, you will have the Llama-2-7b model weights and tokenizer in a huggingface format under a directory called `Llama2-7b-hf` with the following format:\n",
    "\n",
    "```console\n",
    "${DATAPATH}/Llama2-7b-hf/\n",
    "├── config.json\n",
    "├── generation_config.json\n",
    "├── pytorch_model-00001-of-00002.bin\n",
    "├── pytorch_model-00002-of-00002.bin\n",
    "├── pytorch_model.bin.index.json\n",
    "├── special_tokens_map.json\n",
    "├── tokenizer.json\n",
    "├── tokenizer.model\n",
    "└── tokenizer_config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0dc8f4-6b55-4e66-acc9-a18b2a084e56",
   "metadata": {},
   "source": [
    "## 4. Download and Tokenize dataset\n",
    "This tutorial makes use of a [Red pyjama dataset](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T). The dataset can be downloaded to your cluster by running the following commands on the head node:\n",
    "\n",
    "```bash\n",
    "mkdir -p ${DATA_PATH} \n",
    "wget https://data.together.xyz/redpajama-data-1T/v1.0.0/book/book.jsonl -O ${DATA_PATH}/book.jsonl # Note: Dataset download is 50G and will take approximately 3-4 hours to download. You can also use https://aria2.github.io/ for faster download\n",
    "# or\n",
    "# wget https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample/resolve/main/book_sample.jsonl -O ${DATA_PATH}/book.jsonl # Smaller sample dataset for quick testing\n",
    "```\n",
    "\n",
    "Once you have the Tokenizer and the dataset. You can tokenize the dataset following the below command: \n",
    "```bash\n",
    "sbatch 4.tokenize.sbatch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb2ce9-e7da-4624-a298-c56b9003158d",
   "metadata": {},
   "source": [
    "Post tokenizing the dataset, you will have a path to the tokenizer and the dataset which will be used for pretraining. \n",
    "\n",
    "## Llama2 training configurations\n",
    "We tested with the following model sizes: 7B\n",
    "### Llama2 7B\n",
    "\n",
    "- Model configuration\n",
    "    - Attention heads: 32\n",
    "    - Layers: 32\n",
    "    - Sequence length: 4096\n",
    "    - Hidden size: 4096\n",
    "    - Hidden FFN size: 11008\n",
    "    - Microbatch size: 1\n",
    "    - Global batch size: 256\n",
    "\n",
    "- Distributed training configuration\n",
    "    - Number of nodes: 4\n",
    "    - Tensor parallel degree: 8\n",
    "    - Pipeline parallel degree: 1\n",
    "    - Data parallel degree: 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002981a-4275-448e-920c-220900140e0c",
   "metadata": {},
   "source": [
    "## 5. Pre-compile the model\n",
    "By default, PyTorch Neuron uses a just in time (JIT) compilation flow that sequentially compiles all of the neural network compute graphs as they are encountered during a training job. The compiled graphs are cached in a local compiler cache so that subsequent training jobs can leverage the compiled graphs and avoid compilation (so long as the graph signatures and Neuron version have not changed).\n",
    "\n",
    "An alternative to the JIT flow is to use the included [neuron_parallel_compile](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/training/pytorch-neuron-parallel-compile.html?highlight=neuron_parallel_compile) command to perform ahead of time (AOT) compilation. In the AOT compilation flow, the compute graphs are first identified and extracted during a short simulated training run, and the extracted graphs are then compiled and cached using parallel compilation, which is considerably faster than the JIT flow.\n",
    "\n",
    "Before starting the compilation you need to update your path to the dataset and tokenizer in the llama_7b script as below : \n",
    "\n",
    "```bash\n",
    "cd ${APPS_PATH}/neuronx-nemo-megatron/nemo/examples/nlp/language_modeling\n",
    "vi test_llama.sh\n",
    "```\n",
    "\n",
    "Update the below lines\n",
    "\n",
    "```bash\n",
    ": ${TOKENIZER_PATH=$HOME/llamav2_weights/7b-hf}\n",
    ": ${DATASET_PATH=$HOME/examples_datasets/llama_7b/book.jsonl-processed_text_document}\n",
    "```\n",
    "\n",
    "to\n",
    "```bash\n",
    ": ${TOKENIZER_PATH=${MODEL_PATH}/Llama2-7b-hf}\n",
    ": ${DATASET_PATH=${DATA_PATH}/book-tokenized_text_document}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03638422-afe1-45f9-a731-398d9ff27e59",
   "metadata": {},
   "source": [
    "Then, run the following command to launch an AOT pre-compilation job on your ParallelCluster:\n",
    "\n",
    "```bash\n",
    "bash 5.precompile-model.sh\n",
    "```\n",
    "\n",
    "Once you have launched the precompilation job, run the `squeue` command to view the SLURM job queue on your cluster. If you have not recently run a job on your cluster, it may take 4-5 minutes for the requested trn1.32xlarge nodes to be launched and initialized. Once the job is running, `squeue` should show output similar to the following:\n",
    "\n",
    "```console\n",
    "    JOBID  PARTITION  NAME           USER    ST  TIME  NODES NODELIST(REASON)\n",
    "    10     compute1   compile.slurm  ubuntu  R   5:11  4     compute1-dy-queue1-i1-[1-4]\n",
    "```\n",
    "\n",
    "You can view the output of the precompilation job by examining the file named `slurm-compile.slurm-ZZ.out` where ZZ represents the JOBID of your job in the `squeue` output, above. Ex:\n",
    "```\n",
    "tail -f slurm-compile.slurm-10.out\n",
    "```\n",
    "\n",
    "Once the precompilation job is complete, you should see a message similar to the following in the logs:\n",
    "\n",
    "```console\n",
    "2023-06-11 23:04:08.000738: INFO ||PARALLEL_COMPILE||: Total graphs: 22\n",
    "2023-06-11 23:04:08.000738: INFO ||PARALLEL_COMPILE||: Total successful compilations: 22\n",
    "2023-06-11 23:04:08.000738: INFO ||PARALLEL_COMPILE||: Total failed compilations: 0\n",
    "```\n",
    "\n",
    "At this point, you can press `CTRL-C` to exit the tail command.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c350131-e69d-4a93-86a2-fc4f6b867e06",
   "metadata": {},
   "source": [
    "## 6. Launch a pretraining job\n",
    "\n",
    "Submit the training job\n",
    "\n",
    "```\n",
    "bash 6.pretrain-model.sh\n",
    "```\n",
    "\n",
    "\n",
    "As outlined above, you can again use the `squeue` command to view the job queue. Once you see that your pretraining job is running, you can view the output of the training job by examining the file named `slurm-run.slurm-ZZ.out` where ZZ represents the JOBID of your job:\n",
    "\n",
    "```bash\n",
    "tail -f slurm-run.slurm-11.out\n",
    "```\n",
    "\n",
    "Once the model is loaded onto the Trainium accelerators and training has commenced, you will begin to see output indicating the job progress:\n",
    "\n",
    "```console\n",
    "Epoch 0:  22%|██▏       | 4499/20101 [22:26:14<77:48:37, 17.95s/it, loss=2.43, v_num=5563, reduced_train_loss=2.470, gradient_norm=0.121, parameter_norm=1864.0, global_step=4512.0, consumed_samples=1.16e+6, iteration_time=16.40]\n",
    "Epoch 0:  22%|██▏       | 4500/20101 [22:26:32<77:48:18, 17.95s/it, loss=2.43, v_num=5563, reduced_train_loss=2.470, gradient_norm=0.121, parameter_norm=1864.0, global_step=4512.0, consumed_samples=1.16e+6, iteration_time=16.40]\n",
    "Epoch 0:  22%|██▏       | 4500/20101 [22:26:32<77:48:18, 17.95s/it, loss=2.44, v_num=5563, reduced_train_loss=2.450, gradient_norm=0.120, parameter_norm=1864.0, global_step=4512.0, consumed_samples=1.16e+6, iteration_time=16.50]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaee5a9-02e2-4f3e-8598-982b7b96f915",
   "metadata": {},
   "source": [
    "## Contributors\n",
    "\n",
    "* [A] Keita Watanabe - mlkeita@\n",
    "* [R] Verdi March - marcverd@\n",
    "* [R] Brad Doran \n",
    "* [R] Justin Pirtle \n",
    "* [R] Pierre-Yves Aquilanti - pierreya@\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
